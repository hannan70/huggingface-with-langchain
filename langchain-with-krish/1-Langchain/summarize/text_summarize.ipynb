{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2318b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# load env variable\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "os.environ['HUGGINGFACE_TOKEN'] = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import (HumanMessage, AIMessage, SystemMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "920d3d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000197B9DE6710>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000197B9E9C850>, model_name='llama3-70b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(model=\"llama3-70b-8192\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf27402",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = \"\"\"\n",
    "Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think, learn, and make decisions. It is a broad field of computer science that includes subfields such as machine learning, deep learning, natural language processing (NLP), robotics, and computer vision. AI is increasingly becoming an integral part of modern life, powering technologies like voice assistants (e.g., Siri, Alexa), recommendation systems (e.g., Netflix, Amazon), self-driving cars, chatbots, and healthcare diagnostics.\n",
    "\n",
    "Machine learning, a key branch of AI, involves algorithms that allow computers to learn from data without being explicitly programmed. Deep learning, a subset of machine learning, uses neural networks to solve complex problems such as image and speech recognition. NLP enables machines to understand and generate human language, which is crucial for translation services, sentiment analysis, and intelligent chatbots.\n",
    "\n",
    "AI has significant applications across various industries. In healthcare, AI assists in early disease detection and personalized treatment. In finance, it’s used for fraud detection and algorithmic trading. In manufacturing, AI improves efficiency through predictive maintenance and automation. The education sector benefits from personalized learning experiences, while agriculture uses AI for crop monitoring and yield prediction.\n",
    "\n",
    "Despite its benefits, AI also presents challenges. Ethical concerns such as data privacy, algorithmic bias, and job displacement must be addressed. The development of responsible and transparent AI systems is essential to ensure fairness and trust.\n",
    "\n",
    "As AI continues to evolve, it holds the potential to solve some of the world’s most pressing problems. However, the focus must remain on using AI for the betterment of humanity, with regulations and guidelines to ensure its safe and ethical deployment. Education and public awareness will play a key role in shaping an AI-driven future that is inclusive, equitable, and beneficial for all.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f8fb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\machine-learning\\huggingface-langchain\\langchain-with-krish\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee461aa2",
   "metadata": {},
   "source": [
    "### Way One for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ca71c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_message = [\n",
    "    SystemMessage(content=\"You are an expert at summarizing speeches.\"),\n",
    "    HumanMessage(content=f\"Provide a short and concise summary of the following speech:\\n\\n{speech}\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8999dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HANNAN\\AppData\\Local\\Temp\\ipykernel_8812\\2694584740.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm(chat_message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is a short and concise summary of the speech:\\n\\nArtificial Intelligence (AI) is a broad field that simulates human intelligence in machines, with applications in healthcare, manufacturing, finance, education, and more. While it presents challenges like data privacy and job displacement, AI also has the potential to solve pressing global problems. To ensure its safe and ethical deployment, regulations, education, and public awareness are crucial in shaping an AI-driven future that benefits all.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 397, 'total_tokens': 490, 'completion_time': 0.303817878, 'prompt_time': 0.012674057, 'queue_time': 0.20686162800000002, 'total_time': 0.316491935}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--c5569ad2-d10d-4adb-b552-f7a51e51bbf2-0', usage_metadata={'input_tokens': 397, 'output_tokens': 93, 'total_tokens': 490})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(chat_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53f79b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here is a concise summary of the speech:\\n\\nArtificial Intelligence (AI) is a rapidly growing field that simulates human intelligence in machines, with applications in healthcare, manufacturing, finance, education. While AI has the potential to solve pressing global problems, it also raises ethical concerns such as data privacy, bias, and job loss. To harness AI's benefits, it's essential to develop responsible and transparent systems, ensure regulations, and promote public awareness to create a safe, equitable, and inclusive AI-driven future.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the summary content\n",
    "llm(chat_message).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c62e48",
   "metadata": {},
   "source": [
    "### Way two for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f8e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_two = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Your task has two steps:\n",
    "    1. Write a concise summary of the following speech: {speech}.\n",
    "    2. Translate that summary into {language}.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46994a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HANNAN\\AppData\\Local\\Temp\\ipykernel_8812\\3631414846.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt_two)\n",
      "C:\\Users\\HANNAN\\AppData\\Local\\Temp\\ipykernel_8812\\3631414846.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  bangla_summary = llm_chain.run({\"speech\": speech, \"language\": \"bangla\"})\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt_two)\n",
    "bangla_summary = llm_chain.run({\"speech\": speech, \"language\": \"bangla\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68b836e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Summary:**\\n\\nArtificial Intelligence (AI) is the simulation of human intelligence in machines that can think, learn, and make decisions. AI is a broad field that includes machine learning, deep learning, natural language processing, robotics, and computer vision. It has various applications across industries, including disease detection, personalized treatment, fraud detection, and personalized learning experiences. However, AI also raises ethical concerns such as data bias, job displacement, and the need for responsible and transparent AI systems. As AI continues to evolve, it's essential to focus on using it for the betterment of humanity with regulations and public awareness.\\n\\n**Bangla Translation:**\\n\\nকৃত্রিম বুদ্ধিমত্তা (AI) হল মানব বুদ্ধিমত্তার সিমুলেশন যা মেশিন চিন্তা, শিখা এবং সিদ্ধান্ত নেওয়ার ক্ষমতা রাখে। AI একটি ব্যাপক কম্পিউটার বিজ্ঞান ক্ষেত্র যা অন্তর্ভুকরে মেশিন লার্নিং, ডিপ লার্নিং, ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং, রোবোটিক্স এবং কম্পিউটার ভিশন। এটি বিভিন্ন শিল্পের মধ্যে অন্তর্ভুক্ত হয়, যেমন রোগ শনাক্তকরণ, ব্যক্তিগত চিকিৎসা, জালিয়াতি সনাক্তকরণ এবং ব্যক্তিগত শিক্ষা অভিজ্ঞতা। তবে AI এও নৈতিক উদ্বেগ উত্থাপন করে, যেমন ডাটা গোপনীয়তা, অ্যালগরিদমিক পক্ষপাতিত্ব এবং চাকরি সরবরাহীতা। যখন AI বিকাশ লাভ করে, তবে এটি মানবতার উন্নয়নের জন্য ব্যবহার করা আবশক। নিয়ম এবং জনসাধারণা একটি গুরুত্বপূর্ণ ভূমিকা পালন করবে একটি AI-চালিত ভবিষ্যত যা সমান, ন্যায়সঙ্গত এবং সর্বজনীন হবে।\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bangla_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d22c1493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are the two steps:\\n\\n**Step 1: Concise Summary**\\n\\nArtificial Intelligence (AI) simulates human intelligence in machines, enabling them to think, learn, and decide. AI is a broad field encompassing machine learning, deep learning, natural language processing, robotics, and computer vision. It powers technologies like voice assistants, self-driving cars, chatbots, and healthcare diagnostics. While AI has significant applications across industries, it also raises ethical concerns like data privacy, algorithmic bias, and job displacement. Responsible AI development is crucial for ensuring fairness and trust.\\n\\n**Step 2: French Translation**\\n\\nL'Intelligence Artificielle (IA) simule l'intelligence humaine dans les machines, leur permettant de penser, d'apprendre et de décider. L'IA est un vaste domaine qui englobe l'apprentissage automatique, l'apprentissage profond, le traitement du langage naturel, la robotique et la vision par ordinateur. Elle alimente les technologies telles que les assistants vocaux, les voitures autonomes, les chatbots et les diagnostics de santé. Bien que l'IA ait des applications significatives dans de nombreux secteurs, elle soulève également des préoccupations éthiques telles que la confidentialité des données, les biais algorithmiques et la perte d'emplois. Le développement d'IA responsable est crucial pour assurer l'équité et la fiabilité.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_summary = llm_chain.run({\"speech\": speech, \"language\": \"french\"})\n",
    "french_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281cb55b",
   "metadata": {},
   "source": [
    "#### StuffDocumentsChain \n",
    "If you have a small pdf then you can use StuffDocumentsChain. because it All the information can be given to the LLM at once.\n",
    "Problem: token limit problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56dccf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'GPL Ghostscript 8.15', 'creator': 'PScript5.dll Version 5.2', 'creationdate': 'D:20070730160943', 'moddate': 'D:20070730160943', 'title': 'Microsoft Word - Document1', 'author': 'Shri', 'source': '../pdf/apjspeech.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='A P J Abdul Kalam Departing speech \\n \\n \\nFriends, I am delighted to address you all, in the country and those livi ng abroad, after \\nworking with you and completing five beautiful and eventful years in Rashtrapati \\nBhavan. Today, it is indeed a thanks giving occasion. I would like to narr ate, how I \\nenjoyed every minute of my tenure enriched by the wonderful assoc iation from each one \\nof you, hailing from different walks of life, be it politics, sci ence and technology, \\nacademics, arts, literature, business, judiciary, administration, local bodies, farming, \\nhome makers, special children, media and above all from the youth and st udent \\ncommunity who are the future wealth of our country. During my intera ction at \\nRashtrapati Bhavan in Delhi and at every state and union territor y as well as through my \\nonline interactions, I have many unique experiences to share with you, which signify the \\nfollowing important messages: \\n \\n1. Accelerate development : Aspiration of the youth, \\n \\n2. Empower villages, \\n \\n3. Mobilize rural core competence for competitiveness, \\n \\n4. Seed to Food: Backbone for agricultural growth \\n \\n5. Defeat problems and succeed, \\n \\n6. Overcome problems through partnership, \\n \\n7. Courage in combating calamities, \\n \\n8. Connectivity for societal transformation, \\n \\n9. Defending the nation: Our pride and \\n \\n10. Youth movement for Developed India 2020. \\n \\nNow let me share with you each of the messages. \\n \\nAccelerate Development: Aspiration of the youth \\nWhile there were many significant events during my tenure, a que stion from a little girl \\nAnukriti of Sri Sathya Sai Jagriti Vidya Mandir School, of Dar wa village from Haryana, \\nduring children’s visit to Rashtrapati Bhavan on May 22, 2006, rings in my  mind ever \\nafter. \\n \\nAnukriti asked me “why India cannot become a developed nation before the year 2020”. I \\nappreciated the question and said it was indeed a thoughtful question and as sured her that \\nthat her dream would be taken to the highest institution of the nation and we would work'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.15', 'creator': 'PScript5.dll Version 5.2', 'creationdate': 'D:20070730160943', 'moddate': 'D:20070730160943', 'title': 'Microsoft Word - Document1', 'author': 'Shri', 'source': '../pdf/apjspeech.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='for it to achieve before 2020. This question reflects how the desire t o live in developed \\nIndia has entered into the minds of the youth. The same feelings are  echoed by over \\nfifteen lakh youth, whom I have met so far and who represent the drea m of the 540 \\nmillion youth of the nation. The aspirations of the young to live in a prosperous, safe and \\nproud India should be the guiding factor in whatever profession we contribute. \\n \\nEmpower Villages \\nFriends, I recall my visit to Nagaland on 26th October 2002, soon after m y assuming \\noffice as President. It was a unique experience for me at Khuza ma village to meet tribal \\nvillage council members and discuss with them the village progress  and the dream of \\nvillage citizens. I was very happy to see the empowered village  council functioning with \\nfinancial powers and taking decisions. I saw a prosperous village with fruits and \\nvegetables production. However, there is a need for providing physical c onnectivity in \\nNagaland through quality roads for enabling faster movement of produ cts from villages \\nto the market. That meeting gave me a powerful message about the t ransformation which \\ncan take place to the 600,000 villages of India, if all the villages a re empowered to deal \\nwith their development and are well connected among themselves and wit h the urban \\nsocieties. \\n \\nMobilizing rural core competence for competitiveness \\nNow I would like to talk about the initiative of Periyar Maniammai  College of \\nTechnology for Women, Vallam, Tanjore of Providing Urban Amenities in Rural Areas \\n(PURA) complex involving 65 villages with a population of 3 lakhs. This inc ludes \\nprovision of three connectivities - physical, electronic and knowledge  - leading to \\neconomic connectivity. Periyar PURA has health care centers, pr imary to post graduate \\nlevel education and vocational training centers. This has resulted i n large-scale \\nemployment generation and creation of number of entrepreneurs with the  active support \\nof 1000 self-help groups. Two hundred acres of waste land has been develope d into a \\ncultivable land. The villagers are busy in cultivation, planting Jatropha, herbal and \\nmedicinal plants, power generation using bio-mass, food processing and above all \\nrunning marketing centers. It provides a sustainable economic development model for the \\nwhole region. \\n \\nDuring the last eight months, people of Periyar PURA villages technologically supported \\nby Periyar Maniammai College of Engineering for Women have w orked with experts \\nfrom Japan External Trade Organisation (JETRO) on various products, for  which core \\ncompetence and raw material are available in Thanjavur district. They developed \\ninternationally competitive prototypes for 55 life style products wi th support of JETRO \\nspecialists and feedback from exhibitions at Delhi and Tokyo. This co -operative venture \\nhas enhanced the innovative ability of the people of all the 65 villages  enabling them to \\ndevelop and produce internationally acceptable products. I have seen si milar type of \\nPURA being established in many states. The whole country needs 7000 PU RA to \\nbridging the rural – urban divide. \\n \\nSeed to Food: Backbone for agricultural growth'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.15', 'creator': 'PScript5.dll Version 5.2', 'creationdate': 'D:20070730160943', 'moddate': 'D:20070730160943', 'title': 'Microsoft Word - Document1', 'author': 'Shri', 'source': '../pdf/apjspeech.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='Let me now share with you, the enriching experience I had, whil e meeting more than \\n6000 farmers from different States and Union Territories visitin g Rashtrapati Bhavan. \\nThey evinced keen interest in the Mughal Gardens, the Herbal Gardens , the Spiritual \\nGarden, the Musical Garden, the Bio-diesel garden and the Nutrition Garden and interact \\nwith the Horticultural specialists. Recently, during my address to the agricultural \\nscientists while participating in a National Symposium on “Agri culture Cannot Wait”, I \\nsummarized the many practical suggestions given by farmers. W e have to double the \\nagricultural production with reduced land, reduced water resources and re duced \\nmanpower and improve the economic conditions of the nation through the principle  of \\n“Seed to Food” since agriculture is the backbone of the nation. We should em power the \\nfarmers to protect and nurture the fertile land for second green re volution. Meeting the \\nScientists and the Farmers has given me the confidence \\nthat the nation is poised to increase the agricultural GDP growth by atleast 4% per annum \\nthrough the partnership of farmers and agricultural scientists a nd industries particularly \\nfor value addition. \\n \\nDefeat the problems and succeed \\nOn the evening of February 24, 2007, at Coimbatore, I had a very beautif ul experience. \\nAs I got ready for meeting the first person out of twenty appoint ments, a wheel chair was \\nin sight with a smiling person probably in his late fifties; unfortunately he has no hands \\nand legs. His radiant face was revealing his happy state of mi nd. He introduced himself \\nas Vidwan Coimbatore SR Krishna Murthy. I greeted him and asked him  how this had \\nhappened. He smilingly said that it was from by birth. He thanked G od, his parents, \\nteachers and many others for giving him confidence, training and he lp. I asked him, what \\nI could do for him? He said, “I don’t need anything from you. I would like to sing in fr ont \\nof you”. I readily agreed. He sang melodiously the Saint Thyagraj a’s pancha ratna kriti \\nentharo mahanubavulu in Sriragam giving me a glimpse of his talent. I was quite touched. \\nWhat is the message? Despite being physically challenged, the latent talent of music \\ncould blossom in this \\nperson with his positive attitude and perseverance, encouraged by the pa rents, teachers, \\nacademics and rasikas. Now he wants to give, give and give his art to inspire others. Of \\ncourse, by his merit of music, in July 2007, he performed in the Rashtra pati Bhavan art \\ntheatre. \\n \\nOvercome the impact of disaster through partnership \\nI had the opportunity to experience the Indomitable Spirit of the people  and children of \\nJammu & Kashmir even as they were just recovering from the de vastating earthquake in \\n2005. I visited Urusa village on 26th November 2005 which has been adopted by the  \\nWestern Air Command, Air Force for providing relief and medical aid to the residents of \\nthat area. When I went there, I found that the school building had been severely damaged. \\nI met all the school children and the village citizen of Urusa. The villagers apprised me of \\ntheir losses and had all praise for Army and Air Force role in rescue and relief operations \\nalong with state government. I appreciate the courage of the people of Urusa in defeating \\ntheir problems. They have actually become the master of the probl em rather than \\nallowing problems to become their master. Despite the severe l oss due to the earthquake,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.15', 'creator': 'PScript5.dll Version 5.2', 'creationdate': 'D:20070730160943', 'moddate': 'D:20070730160943', 'title': 'Microsoft Word - Document1', 'author': 'Shri', 'source': '../pdf/apjspeech.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='the children and the members of the village participated in the reli ef operation with the \\nArmed Forces \\nbravely and were smiling when I went to meet them. They interact ed with me and said \\nthat the school was functional in the temporary tents. Here, I also witnessed the \\nparticipation of acting Chief Justice of Jammu & Kashmir along with State Government \\nauthorities in on-the-spot settlement of relief grants to be provide d to the victims whose \\nhouses had been damaged in the earth quake. I have experienced many s uch acts of \\ncourage from our citizens when faced with severe challenges. \\n \\nCourage in combating calamities \\nIn 2005, I met the Tribal Council Leaders, Students, Children of Chuckchucha  village \\nduring my visit to Car Nicobar Islands. While various reconstruction a nd rehabilitation \\nactivities were in progress, during the discussions with the members  of tribal council, I \\nrealized the unique trait among the Car-Nicobar islanders. Even thoug h there were many \\nhuman losses due to the Tsunami of 26 Dec 2004, the tribal islanders had  taken \\npossession of affected victims as their children and there is nothi ng like orphanage in \\nCar-Nicobar Islands. Touched by their courage, I composed few vers es called “Sea \\nWaves” which reads as follows: \\nSea Waves \\n \\nWe are the children of Sea waves, \\nSea waves are my friends. \\nWhen they become angry, \\nSea waves give the challenges. \\nGod has given the courage, \\nTo challenge the sea waves. \\nAnd we will succeed, \\nWe will succeed \\nWith Almighty’s grace. \\nAll the members who were gathered in the village sang the poem  with me and exhibited \\nlots of courage and enthusiasm even though they had gone through severe  suffering \\nduring the Tsunami. \\n \\nConnectivity for societal transformation \\nI addressed the Pan African Parliament on 16 September 2004, at Johanne sburg, South \\nAfrica. This was attended by 53 member countries of the Africa n Union, where I \\nproposed the concept of Pan African e-Network for providing seamless and integrated \\nsatellite, fiber optics and wireless network connecting 53 African countries at an \\nestimated cost of US $ 100 million. \\n \\nAs part of the project 12 universities (7 from India and 5 from Afri ca), 17 Super \\nSpecialty Hospitals (12 from India and 5 from Africa), 53 tele-me dicine centers and 53 \\ntele-education centres in Africa will be connected. The pilot project on tele-education and \\ntele-medicine in Ethiopia has already been commissioned. Indira Gandh i National Open \\nUniversity has taken up the MBA Course for 34 Ethiopian students of Addis  Ababa and \\nHarmaya Universities. As regards, tele-medicine, the special ists from CARE Hospital,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.15', 'creator': 'PScript5.dll Version 5.2', 'creationdate': 'D:20070730160943', 'moddate': 'D:20070730160943', 'title': 'Microsoft Word - Document1', 'author': 'Shri', 'source': '../pdf/apjspeech.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Hyderabad are providing one-hour live tele-consultation to doctors in B lack Lion \\nHospital, Addis Ababa in Cardiology and Radiology since November 2006. Using the \\nPan African network the Heads of the State in all the 53 countries  will be connected for \\ninstant communication. I am extremely happy that Indian experience  in bringing the \\nbenefits of technology to the people has enabled us to work with Afric a to bring societal \\ntransformation in the African continent. \\n \\nDefending the nation: Our pride \\nI visited KUMAR in Siachen Glacier located at 17,000 feet altitude  held by the Indian \\nArmy, had a memorable underwater journey in INS Sindhurakshak and flew  in a Sukhoi-\\n30 fighter experiencing 2.5 g. In these three experiences, I personal ly felt proud of our \\never vigilant Soldiers, Sailors and Air Warriors performing their  tasks beyond the call of \\ntheir duty even in the most adverse circumstances natural and man m ade. During the last \\nfive years, I had an opportunity to present colours to many regime nts, participate in \\nnumber of passing out parades, meet the troops who were going to undertake  peace \\nmissions and interact with the family members of our Defence F orces. Our Defence \\nForces are in a beautiful mission. When the nation sleeps during night , Members of our \\nDefence teams are awake to guard us and remain vigilant to counte r any threat. The \\nNation cherishes the valour, commitment and devotion to duty of our Defenc e Forces. \\nSimilarly, I had opportunities to interact \\nwith members of our para-military forces, central and state poli ce personnel including \\ninternal security forces who are making immense contribution in au gmenting the safety \\nand security of our citizens under difficult conditions. \\n \\nYouth movement for Developed India 2020 \\nRecently, in Hyderabad, I met a group of citizens who are putting i nto practice the motto \\nof transforming of our youth into enlightened citizen. The Lead India 2020 Found ation \\ncreated by Dr. N.B. Sudershan at Hyderabad is training thousands of s tudents in many \\ndistricts of Andhra Pradesh in partnership with the District Adm inistration. Particularly, I \\nhappened to know the transformation which has taken place among the students  of \\nMedak district. As per the district authorities the impact of the  training on the students is \\nvisible in terms of self-discipline, love for their parents and tea chers shedding of stage \\nfear and recognition of their duties towards the nation. I talke d to Ms. Padma, a student \\nleader from Andhra Pradesh Tribal Welfare School, Nalgonda who relat ed how she \\nweaned her father away from smoking after imbibing the spirit of  the 10 point oath from \\nthe Lead India Training Camp. This gives me an assurance that the  youth of our country \\nare on the right path \\nthrough this mission oriented programme. With the ignited minds of the  540 million \\nyouth below the age of 25, which I consider is the most powerful resour ce on the earth, \\nunder the earth and above the earth, we have to empower the youth through va lue based \\neducation and leadership. \\n \\nConclusion \\nI was touched by the variety of Indian panorama, emotional content of  the tune, cultural \\ndiversity and unity of minds in the vast land of ours. I have cited thes e examples just to \\ngive a glimpse of the richness of our tradition and effort being taken by different agencies'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.15', 'creator': 'PScript5.dll Version 5.2', 'creationdate': 'D:20070730160943', 'moddate': 'D:20070730160943', 'title': 'Microsoft Word - Document1', 'author': 'Shri', 'source': '../pdf/apjspeech.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='to preserve it. There are also many new adventures by instituti ons and individuals. I have \\nexperienced many of them and learnt a lot about my country and our peopl e. Even while \\npursuing our economic growth, we need to do a lot to preserve the rich and diverse \\ntreasures of our culture and civilization. It is our duty for our fut ure generations. This has \\nto be done in a much larger scale through countrywide participatio n of multiple \\ninstitutions. Our country is blessed with natural resources, has shown c onsiderable \\nprogress in the last sixty years, and above all we have hard wor king people particularly \\nthe power of the 540 million youth of the country. Every sector of our count ry has given \\nme the \\nconfidence that India can become a developed nation well before 2020. Whomsoeve r, I \\nmet they constantly ask what they can give to the nation. We should c onstantly strive to \\nempower such members of the society. With this spirit, I am extr emely happy that we are \\non the right path. Here I am reminded of a famous poem: \\n\"When you wish upon a star, \\nMakes no difference who you are, \\nAnything your heart desires, \\nWill come to you” \\nThis poem is true to all of us, and particularly for our youth and i f they aim great, I am \\nsure they will reach close to the target or the target. \\n \\nMy dear citizens, let us resolve to continue to work for realizing the missions of \\ndeveloped India 2020 with the following distinctive profile. \\n \\n1. A Nation where the rural and urban divide has reduced to a thin line. \\n \\n2. A Nation where there is an equitable distribution and adequate acce ss to energy and \\nquality water. \\n \\n3. A Nation where agriculture, industry and service sector work together in symphony. \\n \\n4. A Nation where education with value system is not denied to any meritorious \\ncandidates because of societal or economic discrimination. \\n \\n5. A Nation which is the best destination for the most talented s cholars, scientists, and \\ninvestors. \\n \\n6. A Nation where the best of health care is available to all. \\n \\n7. A Nation where the governance is responsive, transparent and corruption free. \\n \\n8. A Nation where poverty has been totally eradicated, illiterac y removed and crimes \\nagainst women and children are absent and none in the society feels alienated. \\n \\n9. A Nation that is prosperous, healthy, secure, peaceful and happy a nd continues with a \\nsustainable growth path.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 8.15', 'creator': 'PScript5.dll Version 5.2', 'creationdate': 'D:20070730160943', 'moddate': 'D:20070730160943', 'title': 'Microsoft Word - Document1', 'author': 'Shri', 'source': '../pdf/apjspeech.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='10. A Nation that is one of the best places to live in and is proud of its leadership. \\n \\nFinally let me thank each one of you for showering your love and aff ection on me \\nthroughout the last five years by your cooperation and support. \\n \\nDear Citizens, I conclude my address by sharing with you my mission in life which is to \\nbring connectivity between billion hearts and minds of the people of Indi a in our \\nmulticultural society and to embed the self confidence that \"we c an do it\". I will be \\nalways with you, dear citizens, in the great mission of making Indi a a developed nation \\nbefore 2020. \\n \\nMay God bless you. \\n \\nJai hind. \\nDr. A. P. J. Abdul Kalam \\nwww.presidentofindia.nic.in')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"../pdf/apjspeech.pdf\")\n",
    "docs = loader.load_and_split()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "099a9add",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "        write a short summary of the following speech\n",
    "        Speech : {text} \n",
    "        Translate that summary into {language}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8977db41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, template='\\n        write a short summary of the following speech\\n        Speech : {text} \\n        Translate that summary into {language}\\n    '), additional_kwargs={})])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d87e860d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here is a summary of Dr. A.P.J. Abdul Kalam's departing speech as the President of India:\\n\\n**Summary in English**\\n\\nIn his farewell address, Dr. A.P.J. Abdul Kalam reflected on his five-year tenure as President, highlighting the country's progress and the need for further development. He emphasized the importance of empowering villages, mobilizing rural resources, and accelerating development to achieve a developed India by 2020. He also stressed the need for connectivity, defeating problems, and overcoming calamities through partnership. He shared his experiences of meeting people from various walks of life, including farmers, scientists, and youth, who inspired him with their courage and determination. He expressed his confidence in India's ability to become a developed nation before 2020, with the power of its 540 million youth.\\n\\n**Summary in Bangla**\\n\\nড. এ.পি.জে. আব্দুল কালামের বিদায়ী ভাষণে, তিনি তাঁর পাঁচ বছরের রাষ্ট্রপতি হিসেবে কর্মজীবন সম্পর্কে আলোচনা করেন। তিনি দেশের প্রগতি এবং আরও উন্নয়নের প্রয়ন সম্পর্কে জোর দেন। তিনি গ্রামাঞ্চলক্ষী করণ, গ্রামীণ সমপদ সমবায় এবং উন্নয়ন ত্বরানবয় করার প্রয়োজন সম্পর্কে জোর দেন। তিনি সংযোগ, সমস্যাগুলি পরাজিত করা, এবং বিপদসমূহ পরাজিত করা সম্পর্কে জোর দেন। তিনি বিভিন্ন ক্ষেত্রের লোকদের সাথে সাক্ষাৎ করে, যারা তাঁকে তাঁদের সাহস এবং নির্ধারিত সঙ্কল্পে অনুপ্রাণিত করেন। তিনি ভারতের ৫৪০ কোটি যুবকদের ক্ষমতায় বিশ্বাস করেন যে ভারত ২০২০ সালের আগেই একটি উন্নত দেশ হতে পারে।\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm=llm, chain_type=\"stuff\", prompt=prompt)\n",
    "summary = chain.run({\"input_documents\": docs, \"language\": \"bangla\"})\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f124727",
   "metadata": {},
   "source": [
    "#### Map Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "feff7c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='sensors\\nArticle\\nEnergy Conservation for Internet of Things Tracking\\nApplications Using Deep Reinforcement Learning\\nSalman Md Sultan 1\\n , Muhammad Waleed 1\\n , Jae-Young Pyun 1,*\\n and Tai-Won Um2,*\\n/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045/gid00001\\n/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046\\nCitation: Sultan, S.M.; Waleed, M.;\\nPyun, J.-Y.; Um, T.-W. Energy\\nConservation for Internet of Things\\nTracking Applications Using Deep\\nReinforcement Learning. Sensors 2021,\\n21, 3261. https://doi.org/10.3390/\\ns21093261\\nAcademic Editor: Maria Gabriella\\nXibilia\\nReceived: 6 April 2021\\nAccepted: 6 May 2021\\nPublished: 8 May 2021\\nPublisher’s Note:MDPI stays neutral\\nwith regard to jurisdictional claims in\\npublished maps and institutional afﬁl-\\niations.\\nCopyright: © 2021 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\n1 Department of Information and Communication Engineering, Chosun University, Gwangju 61452, Korea;\\nsultanmohammadsalman@chosun.kr (S.M.S.); mwk.uet@gmail.com (M.W.)\\n2 Department of Cyber Security, College of Science and Technology, Duksung Women’s University,\\nSeoul 01369, Korea\\n* Correspondence: jypyun@chosun.ac.kr (J.-Y.P .); twum@duksung.ac.kr (T.-W.U.)\\nAbstract: The Internet of Things (IoT)-based target tracking system is required for applications such\\nas smart farm, smart factory, and smart city where many sensor devices are jointly connected to\\ncollect the moving target positions. Each sensor device continuously runs on battery-operated power,\\nconsuming energy while perceiving target information in a particular environment. To reduce sensor\\ndevice energy consumption in real-time IoT tracking applications, many traditional methods such\\nas clustering, information-driven, and other approaches have previously been utilized to select\\nthe best sensor. However, applying machine learning methods, particularly deep reinforcement\\nlearning (Deep RL), to address the problem of sensor selection in tracking applications is quite\\ndemanding because of the limited sensor node battery lifetime. In this study, we proposed a long\\nshort-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the\\nproblem of energy consumption in IoT target applications. The proposed method is utilized to select\\nthe energy-efﬁcient best sensor while tracking the target. The best sensor is deﬁned by the minimum\\ndistance function (i.e., derived as the state), which leads to lower energy consumption. The simulation\\nresults show favorable features in terms of the best sensor selection and energy consumption.\\nKeywords: deep reinforcement learning; internet of things; target tracking; best sensor selection;\\nenergy consumption\\n1. Introduction\\nIn a 5G sensor network, a massive amount of data are handled via sensor devices in a\\nlarge area. International Data Corporation (IDC) research states that 70% of companies will\\ndrive to use 1.2 billion devices for the connectivity management solution by 5G services\\nworldwide [1]. The Internet of Things (IoT) is the future of massive connectivity under\\n5G sensor networks. Currently, the IoT is performing a vital role in collecting a large\\namount of data via numerous sensors in real-time applications [2]. Kevin Ashton initially\\ncoined the IoT concept in 1999 [1,3]. Sensor-based IoT devices can provide various types of\\nservices, such as health, trafﬁc congestion control, robotics, and data analysis, which play\\na signiﬁcant role in daily life assistance [4]. Target tracking is another critical area where\\nthe sensors can be utilized to collect the target real-time position and report it to a server\\nwith its relevant information. The practice of tracking one or multiple targets has vast\\napplications in different research areas, such as object tracking (e.g., player, vehicle) [5–7],\\nborder monitoring to prevent illegal crossing, or battleﬁeld surveillance [8], infrared target\\nrecognition [9,10].\\nIn IoT target-tracking scenarios, tracking single or multiple targets can be realized\\nusing one or more sensors. However, it is impractical to utilize a single sensor for col-\\nlecting the target position information owing to an extended area and will take increased\\ncomputation with low tracking accuracy [ 11]. Therefore, it is pertinent to use multiple\\nsensors, particularly in tracking applications. Energy consumption in sensor applications is\\nSensors 2021, 21, 3261. https://doi.org/10.3390/s21093261 https://www.mdpi.com/journal/sensors'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2'}, page_content='Sensors 2021, 21, 3261 2 of 22\\na key task because of the sensor battery lifetime [11,12]. Moreover, it is unable to recharge\\nthe sensor battery in most cases. As a result, it is essential to efﬁciently reduce energy\\nconsumption because energy conservation leads to an increased battery lifespan. There\\nare various energy consumption reduction methods used in recent years (e.g., clustering,\\nsupport vector machine) [13,14]. However, large-scale functional implementation of these\\napproaches precludes more time and resources.\\nReinforcement learning (RL) is a machine learning subﬁeld that solves a problem\\nwithout any predeﬁned model. The RL agent learns the suboptimal policy by interacting\\nwith an unknown environment in real-time decision-based applications [ 15]. The use\\nof RL comprises two main elements: action and reward. In any dynamic interactive\\nenvironment, a precisely selected action will provide the best reward. Thus, providing the\\nbest outcome, based on current observations after acquiring a good reward in a real-time\\nenvironment. However, a massive number of autonomous IoT sensors are employed to\\nintelligently work with a dynamic environment to handle big data in next-generation\\n5G-based IoT applications (i.e., vehicle tracking, pedestrian tracking) [16]. Figure 1 shows\\nsome applications (e.g., smart transportation system, Intelligent Security System) including\\ndifferent types of sensors in the area of autonomous IoT. These autonomous IoT sensors\\ninteract and sense the environment to collect and send the relevant information to agent\\nfor taking the suboptimal action. The conventional RL algorithm (e.g., Tabular Q-learning)\\ntakes a higher time to handle this IoT environment because of large dimension sensor\\ndata [17].\\nAgentState\\nReward\\nAction\\nSmart Transportation System\\n▪ Traffic Signal Control\\n▪ Smart Parking System\\n▪ Autonomous Driving\\nIntelligent Security System\\n▪ Vehicle Tracking\\n▪ Border Monitoring\\n▪ Pedestrian Tracking\\nGPS Speed Sensor Camera RFID\\nApplication\\nAutonomous IoT Sensor\\nData Collection\\nFigure 1. Autonomous IoT applications.\\nDeep reinforcement learning (Deep RL) is an extended version of the conventional\\nRL algorithm to overcome iteration complexity in any large dynamic and interactive\\nenvironment [18]. Deep neural network (described as Q-approximator in this paper) is\\nthe main feature of Deep RL, predicting a suboptimal action from a speciﬁc state. In an\\nautonomous IoT target tracking system, Deep RL can be deployed to the sensor devices to\\nminimize the overall system computational complexity and energy consumption [17,19].\\nMoreover, there are different kinds of Q-approximators used in the Deep RL method to\\nsolve the energy consumption problem. Dense and long short-term memory (LSTM)-\\nbased Q-approximators are frequently utilized to increase energy efﬁciency in time-series\\nenvironments [20,21]. Note that the LSTM Q-approximator is more suitable than the dense\\nQ-approximator because of long-term dependencies in an IoT target tracking environment.\\nThe long-term memory features regulate the essential information sequentially (i.e., time-\\ndependent) to achieve better performance in the learning period [22–24].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3'}, page_content='Sensors 2021, 21, 3261 3 of 22\\nIn this study, we proposed a novel Deep RL framework that predicts the suboptimal\\nenergy-efﬁcient sensor to track the target in IoT tracking applications. Our proposed\\nsystem utilizes an LSTM deep Q-network (LSTM-DQN) as Q-approximator. Moreover,\\na data pre-processing approach is used for better state representation before applying\\nLSTM Q-approximator. The data pre-processing (e.g., normalization, feature selection)\\nis signiﬁcant for achieving stable LSTM Q-approximator [ 25,26]. In this paper, we use\\nmini-max normalization into our designed state space to improve LSTM Q-approximator\\nperformance. Furthermore, we also study epsilon-greedy and softmax action-selection\\nstrategies [27] in our proposed target tracking environment. However, the epsilon-greedy\\nmethod has faster improvement and convergence ability than the softmax method in our\\naction space. Therefore, we proposed an LSTM-DQN-epsilon-greedy method and com-\\npare it with LSTM-DQN-softmax, Dense-DQN-epsilon-greedy, and Dense-DQN-softmax\\napproaches in terms of average cumulative rewards, loss convergence, average sensor\\nselection accuracy, and average cumulative energy consumption.\\nThe remainder of this paper is organized as follows. A description of the related work\\nis provided in Section 2. Section 3 presents the system preliminaries. Sections 4 and 5 show\\nour proposed LSTM-DQN-epsilon-greedy algorithm and numerical results, respectively,\\nfor a detailed comparison. Finally, Section 6 presents the conclusion and future directions\\nof the research work.\\n2. Related Work\\nIn recent years, researchers have been working and investing much of their time to\\nsolve the problem of excessive energy consumption in tracking-based applications. Below,\\napplications based on the respective techniques from background studies are presented.\\n2.1. Tracking Application Based on Information-Driven Approaches\\nInformation-driven is a collaborative sensing technique for various target tracking\\napplications, where each deployed sensor is responsible for collaborating with other de-\\nployed sensors to collect moving target information [ 28]. Information-driven methods\\nwere ﬁrst proposed in terms of collaborative sensor selection via the information utility\\nfunction [29]. In this information-driven sensor selection method, the authors considered\\ndifferent Bayesian estimation problems (e.g., entropy and Mahalanobis distance-based util-\\nity measurements) to determine which sensor would track the moving target.Wei et al. [30]\\nproposed a dual-sensor control technique based on the information utility function in a\\nmulti-target tracking application. In this work, the authors used the posterior distance\\nbetween sensor and targets (PDST) function to minimize the distance between sensors\\nand targets, which helped the sensors directly drive the targets. Ping et el. in [ 31] used\\na partially observed Markov decision process (POMDP) to select suboptimal sensors for\\ntracking multiple targets. The POMDP sensor selection approach is implemented by maxi-\\nmizing the information gain via a probability hypothesis density (PHD)-based Bayesian\\nframework. Although the techniques proposed in [29–31] illustrated good tracking results,\\nthere is a limitation in choosing an energy-efﬁcient sensor to make their model work in an\\nintelligent manner to reduce the computational complexity.\\n2.2. Machine Learning-Based Techniques for Tracking Application\\nMachine learning is an excellent technique to overcome the computational complexity\\nissue in any complicated engineering problem because it is a self-learner, and it does not\\nneed to be reprogrammed [32–35]. Based on background studies, there are three types of\\nmachine learning approaches (i.e., supervised, unsupervised, and reinforcement learning),\\nwhich have been intelligently utilized for energy optimization. The study of supervised\\nlearning techniques is beyond the scope of this research.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}, page_content='Sensors 2021, 21, 3261 4 of 22\\n2.2.1. Unsupervised Learning-based Clustering Approaches\\nTo address the energy consumption problem, Hosseini and Mirvaziri in [ 36] intro-\\nduced a dynamic K-means clustering-based approach to minimize the target tracking error\\nand energy consumption in wireless sensor networks (WSNs). The proposed technique\\nuses a tube-shaped layering method for the sensor nodes to reduce energy dissipation\\nduring target tracking. In addition, Tengyue et al. [37] employed a clustering algorithm to\\ncontrol the sensor energy, which detected the target in a real-time mobile sensor network.\\nThey used the k-means++ algorithm to separate the sensor nodes into sub-groups. The k-\\nmeans++ separated the sensor nodes, which carried a higher weighted probability for\\ntarget detection, and the remaining unnecessary sensors remained in sleep mode to save\\nenergy consumption. Juan and Hongwei in [ 38] proposed another clustering approach\\nto balance energy in terms of multisensory distributed scheduling. Their work used the\\nenergy-balance technique to control the activation and deactivation modes of communi-\\ncation modules. They employed a multi-hop coordination strategy to decrease energy\\nconsumption. However, these types of unsupervised techniques are time-consuming to\\naddress because of the lack of available prior data labeling [34].\\n2.2.2. Reinforcement Learning Approaches\\nSensor scheduling is a promising approach for reducing energy consumption in many\\ntracking applications. Muhidul et al. in [39] proposed a cooperative RL to schedule the task\\nof each node based on the current tracking environment observation. The proposed method\\nhelped the deployed sensor nodes cooperate by sharing the adjacent node information\\nduring tracking. They applied a weighted reward function that combined both energy\\nconsumption and tracking quality matrices to improve the sensor node task scheduling at\\na particular time. Moreover, transmission scheduling is another necessary task in which\\nDeep RL can be applied. Jiang et al. in [ 40] proposed an approximation technique for\\ntransmitting packets in a scheduling manner for cognitive IoT networks. Their DQN model\\nutilized two parameters (i.e., the power for packet sending via multiple channels and\\npacket dropping) to enhance the system capacity in throughput terms. They used a stacked\\nauto-encoder as a Q-function approximator that mapped the policy to maximize system\\nperformance via a utility-based reward technique. However, they exploited the action\\nusing a comprehensive index evaluation method in a single relay to sync transmission.\\nTo reduce IoT device energy consumption, Mehdi et al. [ 41] employed a Deep RL\\ntechnique to learn an optimal policy for indoor localization problems in IoT-based smart\\ncity services. They deployed a semi-supervised technique to classify unlabeled data and\\nintegrated classiﬁed data with label data. They used iBeacons to provide a received\\nsignal strength indicator (RSSI) as an input for a semi-supervised Deep RL model, which\\nconsists of a variational autoencoder neural network Q-learning technique to enhance\\nindoor localization performance. In [ 27], the authors used two Deep RL methods (e.g.,\\nDQN and DDPG) to adjust the activation area radius so the system can minimize the\\naverage energy consumption in terms of vehicle-to-infrastructure (V2I) technology-based\\ntracking applications. They also used two action selection strategies (e.g., epsilon-greedy\\nand softmax) to determine the activation area radius.\\nThe Deep RL method has not been widely applied for energy saving in IoT target\\ntracking applications, particularly in energy-efﬁcient sensor selection approaches. Intel-\\nligently selecting the appropriate sensor to track the target is challenging because the\\ntarget position varies over time, creating tracking environment uncertainty. In this case,\\nthe DQN-based Deep RL is a sophisticated method because it has the best learning ca-\\npability when interacting with an uncertain dynamic environment. In DQN, selecting a\\nQ-approximator for the tracking environment is vital for obtaining improved learning\\nperformance. Therefore, we utilized the LSTM Q-approximator to predict the suboptimal\\ndecisions (i.e., sensor selection) based on sequential information (i.e., target position) with\\nthe assistance of different gate operations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5'}, page_content='Sensors 2021, 21, 3261 5 of 22\\nOur study is based on a discrete action space, which means that the proposed LSTM Q-\\napproximator selects the most energy-efﬁcient sensor among a ﬁnite set of sensors. Authors\\nin [27] showed epsilon-greedy and softmax-based action selection methods for the discrete\\naction space. The epsilon-greedy-based sensor-selection technique presented improved\\nefﬁciency compared to the softmax technique in the simulation results. Thus, we proposed\\nthe LSTM-DQN method with epsilon-greedy action selection (described as LSTM-DQN-\\nepsilon-greedy in this study) in a target tracking environment to select the best sensor for\\nmaximum energy conservation. Table 1 represents a comparison of different existed RL\\nmethods to reduce the energy consumption of the sensor.\\nTable 1. Related work that use RL-based methods to reduce the energy consumption of the sensor.\\nStudy RL-Based Methods Action-Selection Solution Evaluation Metrics\\n[39] SARSA ( λ) epsilon-greedy sensor scheduling energy consumption\\n[40]\\nQ-table with\\nstacked autoencoder epsilon-greedy transmission scheduling\\naverage power\\nconsumption and system utility\\n[27]\\nDQN, DDPG with\\nLSTM\\nepsilon-greedy\\nand softmax\\nradius adjustment\\nof the activated area\\naverage cumulative rewards\\nand energy consumption\\nProposed\\nmethod LSTM-DQN\\nepsilon-greedy\\nand softmax best sensor selection\\naverage cumulative rewards,\\nloss convergence,\\naverage best sensor selection\\naccuracy, and average\\naverage cumulative energy\\nconsumption\\n3. Preliminaries\\n3.1. System Overview\\nFigure 2 illustrates the tracking environment where multiple sensor devices repre-\\nsented as S = {S1, S2, .....,SD} are deployed at different positions to observe the moving\\ntargets, T = { T1, T2, .....,TL}, where L is the number of targets moving in the test area.\\nThe area consists of subareas X = {X1, X2, .....,XN}, where N is the number of subareas.\\nIn this study , our proposed LSTM-DQN-epsilon-greedy scheme allows one sensor to\\ntrack a single target at timet in a particular area, which eventually leads to trackingT targets\\nin N subareas. For instance, the selected sensors shown in green detect the targets, as shown\\nin Figure 2. The remaining sensors remained unselected to minimize energy consumption.\\nDeep RL \\nAgent\\nTarget State \\nInitialized\\nAction\\n(Select Best Sensor)\\nReward\\nTarget Next \\nState\\nEnvironment\\nTarget Detection Selected Sensor Unselected Sensor\\n𝟏) Area (𝑿𝟐)\\nArea (𝑿𝟑) Area (𝑿𝟒)\\nArea (𝑿\\nFigure 2. Deployed sensors for tracking target-based environment.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6'}, page_content='Sensors 2021, 21, 3261 6 of 22\\nFor suboptimal sensor selection, our proposed LSTM-DQN-epsilon-greedy-based IoT\\ntracking system tracks more than one target simultaneously in four subareas X1, X2, X3,\\nand X4, as shown in Figure 2, thus allowing the system to track all T targets in the ﬁrst\\nattempt. If we apply a single DQN algorithm for all N subareas, there is a possibility of not\\nachieving the required goal because when the system interacts with a large area, the sensor\\nselection space is more complicated to utilize the algorithm for effective simultaneous\\ntracking more than one target.\\nTo select the best sensor, it is imperative to estimate the distance between the moving\\ntarget and the sensors. A sensor with the minimum distance to the target location was\\nselected. However, in any practical scenario, the sensor has some noisy (i.e., Gaussian\\nnoise) measurements; thus, it can not collect the target position precisely. This study\\nconsiders that our target tracking environment is linear, including normally distributed or\\nGaussian process noise and some measurement errors. Kalman ﬁlter is suitable for any\\nlinear environment along with Gaussian noise to predict the target information with more\\nprecision [42–44]. Moreover, because of having linear features, the Kalman ﬁlter does not\\nrequire signiﬁcant memory except knowing only the prior state, which assists in predicting\\nthe target state over time [44]. Therefore, For the accurate measurement in a linear and\\nnoisy environment, the Kalman ﬁlter was used to localize the target position.\\n3.2. Kalman Filter\\nThe Kalman ﬁlter estimates the current system state from a series of noisy measure-\\nments, which is useful in tracking applications [42,45–47]. The Kalman ﬁlter is a recursive\\nestimator based on Bayesian ﬁlter theory that can compute the target state along with the\\nuncertainty [43,44]. The system has two signiﬁcant steps: prediction and updating. Various\\nessential Kalman ﬁlter parameters are listed in Table 2.\\nTable 2. Kalman ﬁlter parameters.\\nSymbols Description\\nα0 Initial state matrix\\nP0 Initial process covariance matrix\\nαk−1 Previous state matrix\\nMk Measurement input\\nG Kalman gain\\nAcck Control variable matrix\\nPk−1 Previous process covariance matrix\\nNkα Predicted noise matrix\\nNkp Process noise matrix\\nX, Y, Z Transition matrix\\nMe Measurement error covariance matrix\\nH, I Identity matrix\\nThe initial state matrix α0 indicates the early stage target observation and consists of\\nfour key information pieces such as the x- (x) and y-axis (y) positions, velocity along the x-\\n(vx) and y-axis (vy). In general, the covariance process measures the variation in random\\nvariables. The covariance for the four random variables is deﬁned as follows:\\nσ(x, y, vx, vy) = 1\\nn −1\\nn\\n∑\\ni=1\\n(xi −x)(yi −y)(x′\\ni −vx)(y′\\ni −vy), (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7'}, page_content='Sensors 2021, 21, 3261 7 of 22\\nwhere n is the number of samples, and the covariance matrix is deﬁned as σ(x, y, vx, vy)T .\\nThe initial state α0 and process covariance matrices P0 are expressed as,\\nα0 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx\\ny\\nvx\\nvy\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb (2)\\nP0 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nσ2x σxσy σxσvx σxσvy\\nσyσx σ2y σyσvx σyσvy\\nσvx σx σvx σy σ2vx σvx σvy\\nσvyσx σvyσy σvyσvx σ2vy\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (3)\\nIn the Kalman ﬁlter, the prediction step estimates the current predicted state αk and\\nthe process error covariance matrix Pk, which are expressed as,\\nαk = Xαk−1 + YAcck + Nkα, (4)\\nPk = X(Pk−1XT) +YAcck + Nkp , (5)\\nwhere αk−1 and Pk−1 denote the previous state and process error covariance matrices,\\nrespectively. The variable X represents the state transition matrix for the previous state\\nαk−1, and Y is the input transition matrix for the control vector. The Acck in (6) shows the\\nacceleration of the moving target, given as,\\nYAcck =\\n[ 1\\n2 ∆T2ax 1\\n2 ∆T2ay ∆Tax ∆Tay\\n]T, (6)\\nwhere ∆T represents the time for one cycle, while ax and ay are the acceleration control\\nvariables. In the updated step, we estimate a new measurement Mk for state prediction at\\ntime step k. The Kalman gain G is one of the main features in the Kalman ﬁlter method,\\nwhich gives the ratio of the uncertainty of error in prediction and measurement state [42].\\nMoreover, Kalman gain indicates how much the prediction state of the target should be pre-\\ncise. If the value of Kalman gain is increased gradually, which means the uncertainty error\\nof the measurement is small, and the value of the Kalman gain is low when the measure-\\nment error covariance is larger than the process error covariance. The new measurement\\nMk and gain G are described as follows:\\nMk = Z −Hαk, (7)\\nG = (Pk HT)\\nH.(Pk HT) +Me\\n, (8)\\nwhere Z, H, and Me represent the transition, identity matrix, and measurement error\\ncovariance matrix, respectively. After estimating the Kalman gain G, the predicted state αk\\nand process error covariance matrix Pk are updated in (9) and (10), respectively:\\nαk = Xαk + GMk, (9)\\nPk = [(I −(GH)) + Pk]. (10)\\nHere, Mk is the updated measurement which is obtained by subtracting the transition or\\nmeasured matrix (Z) from the predicted state (αk) as described in (7). The update predicted\\nstate and process error covariance matrix in (9) and (10) will be used in the next time step.\\n3.3. Best Sensor Selection\\nThe designed LSTM-DQN-epsilon-greedy system uses multiple sensors to track the\\ntarget position. We consider one target at a particular time in a speciﬁc subarea as shown\\nin Figure 2. The system operates in such a manner that it does not allow all sensors\\nconcurrently to track the target due to limited battery lifespan of the sensor devices.\\nTherefore, the system intelligently adjudicates to select the best sensor using our proposed\\nDeep RL method while the moving target arrives within that sensor’s range. The sensor'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content='Sensors 2021, 21, 3261 8 of 22\\nwith low energy consumption is considered the best sensor and is apportioned to acquire\\ntarget position information. In the example shown in Figure 2, if the energy consumption\\nof the four sensors (i.e., S1, S2, S3, and S4) are 6J, 5J, 7J, and 8J, respectively, then sensor S2\\nis selected to track the target. In this way, we can conserve the energy of the other three\\nsensors. As a result, the overall system capability has improved in a particular subarea.\\n3.4. Reinforcement Learning (RL)\\nThe RL agent is used as a decision-maker to take the best action ( at) from the set\\nof possible actions over the current state ( st). The RL agent does not learn with the\\nlabeled training dataset, but learns from its experience with environmental interaction.\\nDuring environmental interaction at a particular time, the agent receives an immediate\\nreward (rt) and jumps to the next state (st+1). The entire process continues until the agent\\nreaches the ﬁnal state and begins a new episode after resetting the environment.\\nTabular Q-learning (TQL) is a common model-free RL approach that is considered\\nan off-policy algorithm because the Q-function learns from the interactive environment\\nby taking random actions during exploration time [ 48]. Taking action with the help of\\nexploration is essential because initially, the agent has no idea about the new state in\\nan environment; therefore, the agent needs to explore the environment. After acquiring\\nenvironmental experience by exploration, the agent can easily exploit the environment by\\nutilizing the greedy strategy. The exploration and exploitation technique is also called the\\nepsilon-greedy technique [19]. As a result that the TQL is a value-based method, the agent\\nlearning policy is utilized through the value function (Q-value) of state-action pairs. In TQL,\\nthe Q-value Q(st, at) of an individual action of a particular state is stored in a matrix called\\nthe Q-table, which is updated in each time step in (11),\\nQ(st, at) = Q(st−1, at−1) + ∂(rt + γ max(Q(st+1, at+1)) −Q(st−1, at−1)), (11)\\nwhere ∂ and γ ∈[0, 1] represent the learning rate and discount factor, respectively. Note\\nthat, ∂(rt + γ max(Q(st+1, at+1))) denotes as discounted temporal difference (TD) target,\\nwhich gives the maximum Q value of next state in (11). Further, to estimate the TD error\\nduring the training of Q-learning, we subtract the value of TD target from previous Q value\\n(Q(st−1, at−1)). The learning rate is used, which tells how fast the Q-values are updated\\nalong with TD error. Moreover, the discount factor gives stability between immediate\\nand upcoming or future rewards. If the discount factor is near to 1, then the reward will\\nbe more in the future. Otherwise, the system focuses on the immediate reward when\\nthe discount factor is near to 0. However, TQL has difﬁculty in extending the Q-table\\nto a large environment, as it is only appropriate for a small environment. To extend the\\nmethod to a large environment it is necessary for an agent to learn the value function with\\na Q-approximator instead of saving all values into a Q-table.\\n3.5. Deep-Q-Network\\nThe DQN was introduced by Mnih et al. in [ 18] based on the Deep RL method\\nwith the help of a deep neural network, which is known as a Q-approximator. The Q-\\nvalues of different actions are predicted by utilizing the Q-approximator in a particular\\nstate. In DQN, there is a possibility of a signiﬁcant correlation between the data, forming\\nthe Q-approximator instability during the training period. Following this, experience\\nreplay memory and mini-batch techniques are utilized to obtain a stable Q-approximator.\\nExperience replay memory (E) stores the experience (st, at, rt, st+1) in each time step to\\nre-utilize previous experiences multiple times. After storing each experience, the DQN uses\\nthe mini-batch technique to randomly sample data from the experience replay memory to\\nconverge the Q-approximator loss. It can also reduce the correlation between the samples\\nand improve the agent’s learning performance. Moreover, we estimate the predicted and\\ntarget Q-values with two different Q-approximatorsθ and θ′, respectively, to obtain a stable'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9'}, page_content='Sensors 2021, 21, 3261 9 of 22\\nQ-approximator by optimizing the loss during the training period. The Q-approximator\\nloss L(θ) is described as,\\nL(θ) = (rt + γ max(Q(st+1, at+1; θ′)) −Q(st, at; θ))2. (12)\\n4. The Proposed LSTM-DQN-Epsilon-Greedy Method\\n4.1. Long Short-Term Memory-Based Q-approximator\\nIn our proposed system, we use LSTM as a Q-approximator to select the best sensor.\\nIn our target tracking scenario, the position of the target is updated over time. The LSTM is\\na speciﬁc type of recurrent neural network (RNN) with the ability to learn long-term de-\\npendencies that can memorize and connect related patterns over a time-series input [22,23].\\nMoreover, another reason behind deploying LSTM for our designed system is that it works\\nﬂawlessly in a dynamic environment because it depends on the gate operation. The gates\\nregulate the information ﬂow and can also decide which information should be stored or\\nremoved. The LSTM consists of four gates: forget (Fst), input (Xst), cell (Cst), and output\\n(Ost) states. These four gates store the combined information of the previously hidden\\n(ht−1) and the current input layer (xt) and apply the “sigmoid” operation to all gates\\nexcept the cell state that is ﬁnally activated by “tanh” operation, as shown in Figure 3.\\n𝒉𝒕−𝟏\\n𝒙𝒕\\n𝒄𝒕−𝟏\\n+\\n𝑭𝒔𝒕 𝑿𝒔𝒕 𝑪𝒔𝒕 𝑶𝒔𝒕~ ~~~\\nX\\nX\\n+\\n𝒄𝒕\\n~\\nX\\n𝒉𝒕\\n+Addition Multiply X ~Sigmoid ~tanh\\nFigure 3. LSTM architecture.\\nIn the LSTM mechanism, when the forget state output is near 1, it keeps the data\\nand transfers it to multiply with the previous cell state value ( Ct−1). The input and cell\\nstate gates receive the same information as the forget state gate. After separately applying\\n“sigmoid” and “tanh” operations to input and cell state gate, the outputs are multiplied\\nwith each other and added to the forget state output multiplying of the previous cell state\\nvalue for acquiring a new cell state (Ct). Finally, the output of the new cell state and output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10'}, page_content='Sensors 2021, 21, 3261 10 of 22\\nstate gate after the sigmoid operation multiply with each other to obtain the new hidden\\nstate (ht).\\n4.2. Mini-Max Normalization-Based State Space\\nThe proposed LSTM-DQN-epsilon-greedy model acts as an agent that takes the\\ncurrent state as the input. Estimated minimum distance leads to low energy consumption\\nat a speciﬁc time. The sensor with the minimum distance and energy consumption is\\nconsidered to be the best sensor for an individual area. Therefore, we organized our state\\nwith individual distances (i.e., dS1 , dS2 , ...,dSD ) between the target and sensors. The distance\\nis measured at each time step by using the Euclidean distance formula in (13),\\ndSD (t) =\\n√\\n(Ptarget xcord −PxcordSD\\n)2(t) + (Ptarget ycord −PycordSD\\n)2(t), (13)\\nwhere PxcordSD\\n, PycordSD\\n, Ptarget xcord , and Ptarget ycord are the positions of all the deployed sen-\\nsors and the moving target in the two dimensional x-y plane. Furthermore, the position\\nof any target is computed using the Kalman ﬁlter. Note that the state has different dis-\\ntance value ranges, which can create instability for the Q-approximator. Therefore, it is\\nnecessary to preprocess the state value by normalization before sending it to the LSTM\\nQ-approximator [25]. We use the mini-max normalization method, which is represented\\nas state normalized (t) = (st−min(st))\\nmax(st)−min(st) to scale the state between 0 and 1 to enhance the state\\nquality before sending it to our proposed LSTM Q-approximator.\\n4.3. Epsilon-Greedy Discrete Action Space\\nThe discrete action space (A = {AS1 , AS2 , ..., ASD }) represents all the allocated sensors\\n(i.e., S1, S2, ..., SD), respectively, in a deﬁned area. The LSTM-DQN-epsilon-greedy agent\\nselects the best sensor as an action that consumes minimum energy during target tracking.\\nThe energy consumption ( EconSD\\n) of each sensor at time step ( t) is estimated using (14),\\nwhere dSD , powSensor , and ttrack indicate the distance value between a particular sensor\\n(SD) and the target, the working mode sensor power, and time to track the target in a\\nsingle area, respectively. Similarly, we measured the energy consumption for the other\\nN areas. Note that the energy consumption of all sensors is stored in an array as (Econall )\\nin (15). Furthermore, the selected sensor energy consumption ( Econaction ) and minimum\\nenergy consumption (Econmin ) are obtained from (16) and (17). Finally, we estimate the total\\nenergy consumption (Econtotal ) and energy savings in a particular observation using (18)\\nand (19), respectively:\\nEconSD\\n(t) = dSD (t) ×powsensor (t) ×ttrack (t), (14)\\nEconall (t) = EconSD:1∼D\\n(t), (15)\\nEconaction (t) = Econall [ASD ](t), (16)\\nEconmin (t) = min(Econall (t)), (17)\\nEcontotal (t) =\\nD\\n∑\\nSD=1\\nEconSD\\n(t), (18)\\nEsave (t) = Econtotal (t) −Econaction (t). (19)\\nWe use epsilon-greedy as an action-selection strategy in the designed system be-\\ncause it is suitable for the discrete action space. In the epsilon-greedy approach, ini-\\ntially, the agent takes a random action to explore the environment through the epsilon\\nmethod. There are three key parameters: maximum-epsilon ( εmax ), minimum-epsilon\\n(εmin), and epsilon-decay ( εdecay ) that are considered to ﬁx the epsilon period. First, it\\nbegins with the maximum-epsilon value and then decays with an absolute epsilon-decay'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11'}, page_content='Sensors 2021, 21, 3261 11 of 22\\nvalue at each time step. The epsilon period is completed when the value of epsilon reaches\\nthe minimum-epsilon. Subsequently, the agent greedily exploits the environment to take\\nsuboptimal action with the proposed LSTM Q-approximator, as shown in Figure 4.\\nLSTM(8 Units)\\nDense(4)\\nOutput\\n~\\nDENSE(8)\\nDENSE(16)\\nDENSE(16)\\n~\\nDENSE(8)\\nRelu Sigmoid\\n~\\n~\\n~\\n~\\n~\\n~\\nNormalized-\\nState\\nFigure 4. Proposed LSTM Q-approximator.\\nThe rectiﬁed linear unit (ReLU) is used in the ﬁrst three layers, whereas the sig-\\nmoid activation function works at the output layer. The ReLU is used to obtain the\\nunbounded positive outcome, whereas sigmoid is used in the output layer to obtain a\\npositive bounded outcome between 0 and 1. Moreover, the LSTM Q-approximator pre-\\ndicts the Q-values for all possible actions, which are deﬁned in the action space. Finally,\\nthe agent selects the suboptimal action with the highest action-Q value that is obtained by\\narg max(Q(state normalized t,at;θ)).\\n4.4. Binary-Based Reward Space\\nThe primary goal of our proposed system is to maximize the cumulative rewards after\\na certain number of steps; therefore, it needs to generate a suitable reward mechanism\\nto improve the agent action. The binary reward function is used in the proposed system\\ndesign as follows:\\nrt =\\n{\\n1 if Econaction = Econmin\\n0 if Econaction ̸= Econmin ,\\nwhere rt is the reward at time t; further, if the energy Econaction is equal to Econmin , it returns\\n1; otherwise, the output will be 0. The proposed LSTM-DQN-epsilon-greedy system\\narchitecture and algorithm are shown in Figure 5 and Algorithm 1, respectively.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12'}, page_content='Sensors 2021, 21, 3261 12 of 22\\nInitial \\nPosition\\nEuclidian Distance \\nCalculator\\nIf \\nExplore\\nRandom Action\\nLSTM\\nSensor\\nNext-State\\nReward\\nStep()\\nMemory\\nKalman Position \\nPredictor\\nMinibatch\\nLoss & Update \\nWeights\\nPrevious\\nPosition\\nCalculate \\nEnergy \\nConsumption\\nyes\\nNo\\nConnector\\nNormalized\\n-State\\nState\\nSelected Best  \\nQ-approximator\\nFigure 5. Proposed LSTM-DQN-epsilon-greedy system architecture.\\nAlgorithm 1: The proposed LSTM-DQN-epsilon-greedy algorithm.\\nInput : Distance between sensor and target position ⊿ input = [dS1 →dSD ]\\nOutput :Best sensor selection accuracy and energy consumption\\ninitialization() ⊿ Total number of episodes eptotal , Total number of steps step total ,\\nTraining hyperparameters, Size of replay memory E, Sensor position, Target\\nkalman state\\nfor (Episode 1 to eptotal ) do\\nst = reset_environment() ⊿ Get the initial state using (13)\\nCumulative rewards, cr = 0\\nfor (time-step, t = 1 to steptotal ) do\\nPreprocess st as state normalized t ⊿ Mini-Max normalization\\nrand = random.uniform(0,1)\\nε = max(εmin, ε)\\nif (rand < ε) then\\ntake action randomly ⊿ Exploration\\nε = ε ×εdecay\\nelse\\naction = arg max(Q(state normalized t,at;θ)) ⊿ Exploitation\\nend\\nCalculate EconSD:1∼D , Econaction and Econmin ⊿ From (14), (16) and (17)\\nCalculate Econtotal and Esave ⊿ From (18) and (19)\\nPredict next target kalman state using Kalman Filter\\nCalculate st+1 ⊿ From (13)\\nNormalize st+1 as state normalized t+1\\nCalculate rt\\ncr = cr + rt ⊿ Sum of all rewards in any episode\\nE.append(state normalized t , at, rt, state normalized t+1 ) ⊿ Store experiences\\nPerform random mini-batch sampling from Experience Replay Memory E\\ntarget =\\n{\\nrt if rt = 0\\nrt + γ max(Q(state normalized t+1 , at+1; θ′)) if rt = 1\\nPerform gradient descent of (target −Q(st, at; θ))2 to update\\nQ-approximator\\nst = st+1\\nend\\nend'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13'}, page_content='Sensors 2021, 21, 3261 13 of 22\\n5. Simulation and Results\\n5.1. Environment Setup, Hyper Parameters, and Evaluation Metrics\\nTo evaluate our proposed system, a simulation platform with moving target obser-\\nvation of 16 sensor devices is considered with four subareas, where each subarea consists\\nof 200 m ×200 m. We allocated four sensors in each subarea, and each sensor can cover\\nan area of up to 50 m ×50 m. Thus, 16 sensors cover a total area of 800 m ×800 m.\\nFurthermore, the distance between each sensor was the same in each subarea. We assume\\none target in a particular subarea and extend it to four targets in four different subareas at\\na speciﬁc time. The environmental details are listed in Table 3.\\nTable 3. Details of the proposed environment.\\nParameters Value\\nTotal number of subareas (N) 4\\nSize of a subarea (XN) 200 m ×200 m\\nNumber of sensors in a subarea (XN) 4\\nTotal number of sensors in 4 subareas 16\\nEach sensor tracking range 50 m ×50 m\\nPower of sensor in working mode (powsensor ) 5 watts\\nTracking time of sensor per meter (ttrack ) 2 s\\nNumber of target (each subarea) 1\\nTotal number of targets in 4 subareas 4\\nTargets initial positions [0, 0]–[200, 200]–[400, 400]–[600, 600]\\nTarget initial velocity [0.1 m/s, 0.2 m/s]\\nTarget initial acceleration [5 m/s 2, 5 m/s2]\\nDuring our simulation, we assumed that the total number of episodes was 500, where\\neach episode consisted of 100 time steps. In each time step, the target positions are\\nupdated using the Kalman ﬁlter method. Thus, we can utilize 100 different states for our\\nproposed LSTM-DQN-epsilon-greedy system in one episode. Figure 6 shows a sample of\\ndata during the experiment that contains measured values. Moreover, Figure 7 shows a\\nsample of different state values in one area after applying the normalization (i.e., mini-max\\nnormalization, which was described in Section 4.2) at the time of the experiment. Here,\\nd1, d2, d3, and d4 represent the normalized distance values between the four sensors\\nand the target. The normalized state was near zero when the moving target passed near\\na particular sensor. Conversely, the particular distance values were greater than 0 and\\ngradually increased to 1 when the target moved far behind the sensor. The ﬁgure clearly\\nshows that the initial value of d1 (i.e., the distance between the ﬁrst sensor and the target) is\\nzero as the target moves very close to the ﬁrst sensor. The same is true for the other sensor\\ndistance values during the simulation period.\\nFigure 6. Some samples of measurement during simulation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14'}, page_content='Sensors 2021, 21, 3261 14 of 22\\n/uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013\\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056\\n/uni00000013/uni00000011/uni00000013\\n/uni00000013/uni00000011/uni00000014\\n/uni00000013/uni00000011/uni00000015\\n/uni00000013/uni00000011/uni00000016\\n/uni00000013/uni00000011/uni00000017\\n/uni00000013/uni00000011/uni00000018\\n/uni00000013/uni00000011/uni00000019\\n/uni00000013/uni00000011/uni0000001a\\n/uni00000013/uni00000011/uni0000001b\\n/uni00000013/uni00000011/uni0000001c\\n/uni00000014/uni00000011/uni00000013/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000036/uni00000057/uni00000044/uni00000057/uni00000048/uni00000003/uni0000003e/uni00000025/uni00000048/uni00000057/uni0000005a/uni00000048/uni00000048/uni00000051/uni00000003/uni00000013/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003/uni00000014/uni00000040\\n/uni00000047/uni00000014\\n/uni00000047/uni00000015\\n/uni00000047/uni00000016\\n/uni00000047/uni00000017\\nFigure 7. Normalized state value for each time step during the experiment.\\nNote that we restart each episode when the number of steps reaches 100, and targets\\nagain start moving from the initial position. Moreover, some useful hyperparameters were\\nset during the training session, as presented in Table 4. These parameters are used to tune\\nthe proposed LSTM-DQN-epsilon-greedy scheme to achieve a more stable output. These\\nhyperparameter values were chosen by a trial and error process. We performed simulations\\nusing Python 3.7.7 [ 49]. TensorFlow 2.0.0 and Keras 2.3.1 were used to implement the\\nLSTM Q-approximator [50,51].\\nTable 4. Hyperparameters for LSTM-DQN-epsilon-greedy during training.\\nHyperparameter Value\\nOptimizer adam\\nLoss categorical crossentropy\\nBatch Size 16\\nSize of experience replay memory (E) 50\\nLearning rate (∂) 0.001\\nDiscount factor (γ) 0.9\\nMaximum epsilon (εmax ) 1\\nMinimum epsilon (εmin) 0.01\\nEpsilon decay (εdecay ) 0.995\\nThe mathematical formulas to evaluate our proposed method are shown in Table 5.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15'}, page_content='Sensors 2021, 21, 3261 15 of 22\\nTable 5. A list of evaluation metrics.\\nDeﬁnition Formula\\nCumulative rewards\\n(described in Section 5.2.1) cr = ∑101\\nt=1 rt\\nBest sensor selection accuracy.\\nhere, TBest ASD\\n= total number of predicted\\nbest sensor and\\nTWrong ASD\\n= total number of predicted\\nwrong sensor\\n(described in Section 5.2.2) AccSD = (\\nTBest ASD\\nTBest ASD\\n+ TWrong ASD\\n) ×100\\nAverage cumulative reward.\\nhere, ep denotes the episode\\nand X1, X2, X3, and X4 are\\nfour system subareas.\\n(described in Section 5.3.1) avgcr =∑501\\nep=1\\ncrX1\\n(ep)+crX2\\n(ep)+crX3\\n(ep)+crX4\\n(ep)\\n4\\nThe categorical crossentropy loss convergence.\\nhere, yj = rt + γ max(Q(st+1, at+1; θ′)),\\ny′\\nj = Q(st, at; θ) and\\ns = size of the action space\\n(described in Section 5.3.2) CCLoss = −∑s\\njs=1 yj log(y′\\nj)\\nAverage best sensor selection accuracy\\nhere, D is the total number of sensor\\n(described in Section 5.3.3) avgAcc = ∑D\\nSD=1 AccSD\\nD\\nAverage cumulative energy consumption\\n(described in Section 5.3.4) avgEcon = ∑501\\nep=1\\nEconaction X1\\n(ep)+Econaction X2\\n(ep)+Econaction X3\\n(ep)+Econaction X4\\n(ep)\\n4\\n5.2. Results\\n5.2.1. Cumulative Rewards\\nIn our proposed LSTM-DQN-epsilon-greedy method, we ﬁrst measure the cumulative\\nrewards (cr) as shown in Table 5 for each episode. The estimation of the cumulative reward\\nis important because it indicates the agent’s learning performance during interaction with\\nthe target tracking environment. The proposed agent receives a reward of 1 when the agent\\nsuccessfully selects the best sensor, as discussed brieﬂy in Sections 4.3 and 4.4. In Figure 8,\\nthe cumulative reward is shown per episode for each subarea. It shows that the cumulative\\nreward is less than 35 for each subarea and does not reach the highest value in the ﬁrst\\ntwo episodes (200 steps), as it initially explores the environment. In general, the explo-\\nration duration depends on the epsilon parameter values (i.e., εmax , εmin, and εdecay ) given\\nin Table 3.\\nFollowing the exploration stage, the proposed agent starts exploiting the environ-\\nment through a greedy approach for selecting the best sensor to track the target. In this\\ncase, the agent selects the suboptimal action based on the maximum predicted action-Q\\nvalue. During the greedy process, the cumulative reward gradually increased after the\\nsecond episode for all subareas. As we have 100 different states in each episode, therefore,\\nthe maximum cumulative reward is 100. The proposed agent needs to obtain the highest\\ncumulative reward as early as possible to reduce the energy consumption of the sensor.\\nWith the proposed method, the highest cumulative reward up to 100 was achieved before\\nreaching 100 episodes for all subareas. The ﬂow of maximum cumulative rewards is\\nsigniﬁcantly stable, showing outstanding performance while selecting the best sensor.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16'}, page_content='Sensors 2021, 21, 3261 16 of 22\\n/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013\\n/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048\\n/uni00000014/uni00000013\\n/uni00000015/uni00000013\\n/uni00000016/uni00000013\\n/uni00000017/uni00000013\\n/uni00000018/uni00000013\\n/uni00000019/uni00000013\\n/uni0000001a/uni00000013\\n/uni0000001b/uni00000013\\n/uni0000001c/uni00000013\\n/uni00000014/uni00000013/uni00000013/uni00000026/uni00000058/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000056/uni00000003\\n/uni00000024/uni00000055/uni00000048/uni00000044/uni00000010/uni00000014\\n/uni00000024/uni00000055/uni00000048/uni00000044/uni00000010/uni00000015\\n/uni00000024/uni00000055/uni00000048/uni00000044/uni00000010/uni00000016\\n/uni00000024/uni00000055/uni00000048/uni00000044/uni00000010/uni00000017\\nFigure 8. Cumulative rewards for each area.\\n5.2.2. Best Sensor Selection Accuracy\\nAs a result that sensors have a limited battery lifetime, it is essential to reduce energy\\nconsumption as much as possible. In the proposed scheme, the system selects the four best\\nsensors at a particular time within an area of 800 m×800 m divided into Areas 1, 2, 3, and 4,\\nas shown in Figure 2. Due to having different ranges of state values, it is difﬁcult to achieve\\nbetter accuracy of best sensor selection by our proposed LSTM Q-approximator. As a result,\\nour proposed agent selects the energy-efﬁcient sensor based on normalized state, which has\\nbeen described in Section 4.2. Furthermore, the accuracy of selecting the best sensor affects\\nenergy consumption during the tracking target because the best sensor selection is based on\\nthe minimum energy consumption described in Section 4.3. Figure 9 shows the best sensor\\nselection accuracy for the 16 sensors (as formulated in Table 5). This demonstrates that the\\nproposed LSTM-DQN-epsilon-greedy system has a signiﬁcant accuracy of approximately\\n99% for sensors 1, 8, 12, 14, and 16. Similarly, the system achieved an accuracy of 98% for\\nsensors 4, 5, 6, and 10. Moreover, the proposed system provides more than 90% accuracy\\nin the case of all other sensors, leading to promising results.\\n5.3. Comparative Analysis\\nThe proposed LSTM-DQN-epsilon-greedy system is also compared with three bench-\\nmark schemes: LSTM-DQN-softmax, Dense-DQN-epsilon-greedy, and Dense-DQN-softmax\\nin terms of average cumulative reward, loss convergence, average best sensor selection\\naccuracy, and cumulative energy consumption. In DQN, the LSTM and dense-based Q-\\napproximator are used frequently for the dynamic environment. However, LSTM exhibits\\nbetter performance in handling such an environment because of memory features. We also\\nutilized different action-selection strategies (e.g., epsilon-greedy and softmax) compared\\nwith our scheme.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17'}, page_content='Sensors 2021, 21, 3261 17 of 22\\n(a) (b)\\n(c) (d)\\nFigure 9. Best sensor selection accuracy: (a) Sensor selection accuracy for Area 1; (b) Sensor selection\\naccuracy for Area 2; (c) Sensor selection accuracy for Area 3; (d) Sensor selection accuracy for Area 4.\\n5.3.1. Average Cumulative Reward\\nThe key designed method deployment objective is to increase the average cumulative\\nreward (avgcr ) as described in Table 5 to measure the agent’s performance. Figure 10 shows\\nthe average cumulative reward per episode for the four DQN-based schemes. The ﬁgure\\nshows that our proposed model and the LSTM-DQN-softmax model both achieved the\\nhighest average cumulative reward, which was up to 100 during the simulation period.\\nHowever, LSTM-DQN-epsilon-greedy reached achieved the highest value faster in 63\\nepisodes compared to the LSTM-DQN-softmax, which reached that level in 115 episodes.\\nThe efﬁciency of our proposed system is that the epsilon-greedy action selection strategy\\ndirectly learns from the action-Q-value function, which is suitable for discrete action space.\\n/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013/uni00000016/uni00000018/uni00000013/uni00000017/uni00000013/uni00000013/uni00000017/uni00000018/uni00000013/uni00000018/uni00000013/uni00000013\\n/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048\\n/uni00000014/uni00000013\\n/uni00000015/uni00000013\\n/uni00000016/uni00000013\\n/uni00000017/uni00000013\\n/uni00000018/uni00000013\\n/uni00000019/uni00000013\\n/uni0000001a/uni00000013\\n/uni0000001b/uni00000013\\n/uni0000001c/uni00000013\\n/uni00000014/uni00000013/uni00000013/uni00000003/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000026/uni00000058/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000056/uni00000003\\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000056/uni00000048/uni00000047/uni00000003/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000048/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000010/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c\\n/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000056/uni00000052/uni00000049/uni00000057/uni00000050/uni00000044/uni0000005b\\n/uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000048/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000010/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c\\n/uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000056/uni00000052/uni00000049/uni00000057/uni00000050/uni00000044/uni0000005b\\nFigure 10. Average cumulative rewards per episode.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18'}, page_content='Sensors 2021, 21, 3261 18 of 22\\nFurthermore, the comparison has been extended to the other two Dense-DQN-based\\nschemes: Dense-DQN-epsilon-greedy and Dense-DQN-softmax. The performance of both\\nLSTM-DQN-based approaches is better than that of Dense-DQN methods because of the\\nlong-term memory dependencies. Therefore, both the Dense-DQN-epsilon-greedy and\\nDense-DQN-softmax schemes are unable to reach the highest average cumulative reward\\nover the entire 500 episodes, and the average cumulative reward increase of both methods\\nis much slower than the proposed LSTM-DQN-epsilon-greedy scheme.\\n5.3.2. Loss Convergence\\nThe loss convergence depreciation to the minimum level is also vital, along with the\\nsystem stability. To estimate the loss of our proposed Q-approximator, we use categorical\\ncrossentropy because it is suitable for multiclass classiﬁcation problems (as presented in\\nTable 5). The proposed LSTM-DQN-epsilon-greedy system signiﬁes good convergence\\nbehavior around 200,000 epochs and is more stable, as illustrated in Figure 11. Moreover,\\nthe LSTM-DQN-softmax convergence also appeared around 200,000 epochs, but was less\\nstable than our proposed scheme. Furthermore, Dense-DQN-epsilon-greedy and Dense-\\nDQN-softmax methods show unstable behavior and converge at 500,000 epochs, which is\\ntime-consuming. Therefore, the proposed LSTM-DQN-epsilon-greedy algorithm is efﬁcient\\nand stable, leading to promising results.\\n(a) (b)\\n(d)(c)\\nFigure 11. Loss convergence per epoch during training: ( a) loss convergence for proposed epsilon-greedy-LSTM-DQN;\\n(b) loss convergence for softmax-LSTM-DQN; (c) loss convergence for epsilon-greedy-Dense-DQN; (d) loss convergence\\nfor softmax-Dense-DQN.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19'}, page_content='Sensors 2021, 21, 3261 19 of 22\\n5.3.3. Average Best Sensor Selection Accuracy\\nIn this section, we compared the average best sensor selection accuracy (as described\\nin Table 5) of the proposed system with that of the other three DQN methods, as presented\\nin Figure 12. In our study, the agent selects the best sensor that has minimum energy con-\\nsumption when the target moves in any particular area. The critical task is to signiﬁcantly\\nenhance the best sensor selection accuracy to reduce the average energy consumption.\\nAs shown in Figure 12, the proposed system agent selects the best sensor with a slightly\\nhigher average accuracy than LSTM-DQN-softmax. Furthermore, the proposed LSTM-\\nDQN-epsilon-greedy scheme achieved signiﬁcantly higher best sensor selection accuracy\\nthan the Dense-DQN-epsilon-greedy and Dense-DQN-softmax methods.\\n/uni0000001b/uni00000013/uni00000011/uni00000013\\n/uni0000001b/uni00000015/uni00000011/uni00000018\\n/uni0000001b/uni00000018/uni00000011/uni00000013\\n/uni0000001b/uni0000001a/uni00000011/uni00000018\\n/uni0000001c/uni00000013/uni00000011/uni00000013\\n/uni0000001c/uni00000015/uni00000011/uni00000018\\n/uni0000001c/uni00000018/uni00000011/uni00000013\\n/uni0000001c/uni0000001a/uni00000011/uni00000018\\n/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000056/uni00000048/uni00000051/uni00000056/uni00000052/uni00000055/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000003e/uni00000008/uni00000040\\n/uni0000001c/uni0000001a/uni00000011/uni00000014/uni0000001b/uni0000001c/uni00000019/uni00000011/uni0000001a/uni00000014\\n/uni0000001c/uni00000016/uni00000011/uni00000017/uni00000017\\n/uni0000001b/uni0000001b/uni00000011/uni00000014/uni00000019\\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000056/uni00000048/uni00000047/uni00000003/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000048/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000010/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c\\n/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000056/uni00000052/uni00000049/uni00000057/uni00000050/uni00000044/uni0000005b\\n/uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000048/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000010/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c\\n/uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000056/uni00000052/uni00000049/uni00000057/uni00000050/uni00000044/uni0000005b\\nFigure 12. Average best sensor selection accuracy.\\n5.3.4. Average Cumulative Energy Consumption\\nOur designed system was also utilized to reduce the average cumulative energy\\nconsumption while tracking the target. We already mentioned in Sections 5.3.1 and 5.3.3,\\nthat a higher average cumulative reward effectively enhances the best sensor selection\\naccuracy and reduces the average cumulative energy consumption. The average cumulative\\nenergy consumption (avgEcon ) is obtained using a formula, which is shown in Table 5.\\nFigure 13 shows the average cumulative energy consumption in 500 episodes. It can\\nbe observed from the ﬁgure that the average cumulative energy consumption for each\\nmethod is higher, particularly in the ﬁrst 100 episodes. The reason behind it is that initially,\\nthe agent has no experience with the environment. However, as the number of episodes\\nincreases, the average cumulative energy consumption decreases signiﬁcantly for both\\nLSTM-DQN- and Dense-DQN-based schemes.\\nIn contrast, both LSTM-DQN-epsilon-greedy and LSTM-DQN-softmax methods have\\nmuch lower average cumulative energy consumption compared to Dense-DQN-epsilon-\\ngreedy and Dense-DQN-softmax because the LSTM Q-approximator can regulate the\\ninformation ﬂow in memory in the long and short term. Furthermore, both the LSTM-\\nDQN-epsilon-greedy and LSTM-DQN-softmax schemes approximately reduce the same\\naverage cumulative energy consumption in each episode except 1 to 200. However, the pro-\\nposed LSTM-DQN-epsilon-greedy method shows a faster and better reduction of the\\naverage cumulative energy consumption than LSTM-DQN-softmax, particularly in the\\nﬁrst 100 episodes. Thus, our designed LSTM-DQN-epsilon-greedy method signiﬁcantly'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='Sensors 2021, 21, 3261 20 of 22\\nreduced the average cumulative energy consumption compared to the other three methods\\nby selecting the best energy-efﬁcient sensor in our designed target tracking environment.\\n/uni00000014/uni00000010/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000010/uni00000015/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000010/uni00000016/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000010/uni00000017/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000010/uni00000018/uni00000013/uni00000013\\n/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048\\n/uni00000015/uni00000013/uni00000013/uni00000013/uni00000013\\n/uni00000015/uni00000015/uni00000013/uni00000013/uni00000013\\n/uni00000015/uni00000017/uni00000013/uni00000013/uni00000013\\n/uni00000015/uni00000019/uni00000013/uni00000013/uni00000013\\n/uni00000015/uni0000001b/uni00000013/uni00000013/uni00000013\\n/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013\\n/uni00000016/uni00000015/uni00000013/uni00000013/uni00000013\\n/uni00000016/uni00000017/uni00000013/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000046/uni00000058/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000046/uni00000052/uni00000051/uni00000056/uni00000058/uni00000050/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000003e/uni0000002d/uni00000040\\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000056/uni00000048/uni00000047/uni00000003/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000048/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000010/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c\\n/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000056/uni00000052/uni00000049/uni00000057/uni00000050/uni00000044/uni0000005b\\n/uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000048/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000010/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c\\n/uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000056/uni00000052/uni00000049/uni00000057/uni00000050/uni00000044/uni0000005b\\nFigure 13. Average cumulative energy consumption.\\n6. Conclusions and Future Directions\\nSensors are widely used in IoT applications (e.g., tracking and attaining target location\\ninformation). In such scenarios, energy consumption optimization is a critical challenge\\nbecause of the sensor battery lifespan. For this reason, an adequate learning method with\\nDeep RL has been proposed to overcome the problem of energy consumption. The pro-\\nposed idea is based on selecting the best sensor with minimum energy using the proposed\\nDeep RL agent at a particular time to collect the target location information. The Kalman\\nﬁlter and LSTM-DQN-epsilon-greedy algorithms have been utilized to predict the target\\nposition and best sensor selection, respectively. Furthermore, we compared our proposed\\nLSTM-DQN-epsilon-greedy system with the other three benchmark schemes: LSTM-DQN-\\nsoftmax, Dense-DQN-epsilon-greedy, and Dense-DQN-softmax. A comparative analysis\\nwas performed in terms of average cumulative reward, loss convergence, average best\\nsensor selection accuracy, and cumulative energy consumption. Our proposed LSTM-\\nDQN-epsilon-greedy method addresses the problem of best sensor selection and converges\\nthe energy consumption issue efﬁciently, which is signiﬁcantly improved in our tracking\\nenvironment than the other three methods.\\nThe limitation of the proposed scheme is that we only considered the linear target\\ninformation using the Kalman ﬁlter. However, the target position can be non-linear, which\\nis out of scope of this study. Moreover, the framework is unable to track multiple targets in\\none subarea at a particular time. To track the multiple targets information simultaneously,\\nwe need to activate more than one sensor in one subarea. The framework will be extended\\nto use multi-agent-based Deep RL in future work to control the multiple sensors efﬁciently.\\nFinally, the system could also leverage hardware in the future to carry out real-time\\nhardware experimentation.\\nAuthor Contributions: Conceptualization, S.M.S., M.W. and T.-W.U.; methodology, S.M.S., M.W.;\\nsoftware, S.M.S. and M.W.; validation, J.-Y.P . and T.-W.U.; formal analysis, J.-Y.P . and T.-W.U.;\\ninvestigation, S.M.S. and M.W.; resources, J.-Y.P . and T.-W.U.; data curation, M.W. and S.M.S.;\\nwriting—original draft preparation, M.W. and S.M.S.; writing—review and editing, M.W., T.-W.U.\\nand J.-Y.P .; visualization, M.W., J.-Y.P . and T.-W.U.; project administration, J.-Y.P . and T.-W.U.; funding\\nacquisition, J.-Y.P . All authors have read and agreed to the published version of the manuscript.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='Sensors 2021, 21, 3261 21 of 22\\nFunding: This work was supported by the Institute for Information communications Technology\\nPromotion (IITP) grant funded by the Korea government (MSIT) (2018-0-00691, Development of\\nAutonomous Collaborative Swarm Intelligence Technologies).\\nInstitutional Review Board Statement: Not applicable.\\nInformed Consent Statement: Not applicable.\\nData Availability Statement: Not applicable.\\nConﬂicts of Interest: The authors declare that they have no conﬂict of interest.\\nReferences\\n1. Li, S.; Da Xu, L.; Zhao, S. 5G Internet of Things: A survey. J. Ind. Inf. Integr.2018, 10, 1–9. [CrossRef]\\n2. Farhad, A.; Kim, D.H.; Subedi, S.; Pyun, J.Y. Enhanced LoRaWAN Adaptive Data Rate for Mobile Internet of Things Devices.\\nSensors 2020, 20, 6466. [CrossRef]\\n3. Mihovska, A.; Sarkar, M. Smart Connectivity for Internet of Things (IoT) Applications. In New Advances in the Internet of Things;\\nSpringer: Berlin/Heidelberg, Germany, 2018; Volume 715, pp. 105–118. [CrossRef]\\n4. Farhad, A.; Kim, D.H.; Kim, B.H.; Mohammed, A.F.Y.; Pyun, J.Y. Mobility-Aware Resource Assignment to IoT Applications in\\nLong-Range Wide Area Networks. IEEE Access2020, 8, 186111–186124. [CrossRef]\\n5. Cabrera, R.S.; de la Cruz, A.P . Public transport vehicle tracking service for intermediate cities of developing countries, based\\non ITS architecture using Internet of Things (IoT). In Proceedings of the 2018 21st International Conference on Intelligent\\nTransportation Systems (ITSC), Maui, HI, USA, 4–7 November 2018; IEEE: New York, NY, USA, 2018; pp. 2784–2789.\\n6. Raad, M.W.; Deriche, M.; Sheltami, T. An IoT-Based School Bus and Vehicle Tracking System Using RFID Technology and Mobile\\nData Networks. Arab. J. Sci. Eng.2021, 46, 3087–3097. [CrossRef]\\n7. Zhang, R.; Wu, L.; Yang, Y.; Wu, W.; Chen, Y.; Xu, M. Multi-camera multi-player tracking with deep player identiﬁcation in sports\\nvideo. Pattern Recognit.2020, 102, 107260. [CrossRef]\\n8. Karthick, R.; Prabaharan, A.M.; Selvaprasanth, P . Internet of things based high security border surveillance strategy.Asian J. Appl.\\nSci. Technol. (AJAST)2019, 3, 94–100.\\n9. Zhang, R.; Xu, L.; Yu, Z.; Shi, Y.; Mu, C.; Xu, M. Deep-IRTarget: An Automatic Target Detector in Infrared Imagery using\\nDual-domain Feature Extraction and Allocation. IEEE Trans. Multimed.2021, 1. [CrossRef]\\n10. Zhang, R.; Mu, C.; Yang, Y.; Xu, L. Research on simulated infrared image utility evaluation using deep representation.\\nJ. Electron. Imaging2018, 27, 013012. [CrossRef]\\n11. Ez-Zaidi, A.; Rakrak, S. A comparative Study of Target Tracking Approaches in Wireless Sensor Networks. J. Sens.\\n2016, 2016, 1–11. [CrossRef]\\n12. Zhang, Y. Technology Framework of the Internet of Things and its Application. In Proceedings of the 2011 International\\nConference on Electrical and Control Engineering, Yichang, China, 16–18 September 2011; IEEE: New York, NY, USA, 2011;\\npp. 4109–4112. [CrossRef]\\n13. Kumar, S.; Tiwari, U.K. Energy efﬁcient target tracking with collision avoidance in WSNs. Wirel. Pers. Commun. 2018,\\n103, 2515–2528. [CrossRef]\\n14. Sebastian, B.; Ben-Tzvi, P . Support vector machine based real-time terrain estimation for tracked robots. Mechatronics\\n2019, 62, 102260. [CrossRef]\\n15. Montague, P .R. Reinforcement learning: An introduction, by Sutton, RS and Barto, AG. Trends Cogn. Sci.1999, 3, 360. [CrossRef]\\n16. Wang, D.; Chen, D.; Song, B.; Guizani, N.; Yu, X.; Du, X. From IoT to 5G I-IoT: The next generation IoT-based intelligent algorithms\\nand 5G technologies. IEEE Commun. Mag.2018, 56, 114–120. [CrossRef]\\n17. Lei, L.; Tan, Y.; Zheng, K.; Liu, S.; Zhang, K.; Shen, X. Deep reinforcement learning for autonomous internet of things: Model,\\napplications and challenges. IEEE Commun. Surv. Tutorials2020, 22, 1722–1760. [CrossRef]\\n18. Mnih, V .; Kavukcuoglu, K.; Silver, D.; Rusu, A.A.; Veness, J.; Bellemare, M.G.; Graves, A.; Riedmiller, M.; Fidjeland, A.K.;\\nOstrovski, G.; et al. Human-level Control through Deep Reinforcement Learning. Nature 2015, 518, 529–533. [CrossRef]\\n19. Ali Imran, M.; Flávia dos Reis, A.; Brante, G.; Valente Klaine, P .; Demo Souza, R. Machine Learning in Energy Efﬁciency\\nOptimization. In Machine Learning for Future Wireless Communications; Wiley Online Library: Hoboken, NJ, USA, 2020; pp. 105–117.\\n[CrossRef]\\n20. Liu, N.; Li, Z.; Xu, J.; Xu, Z.; Lin, S.; Qiu, Q.; Tang, J.; Wang, Y. A hierarchical framework of cloud resource allocation and power\\nmanagement using deep reinforcement learning. In Proceedings of the 2017 IEEE 37th International Conference on Distributed\\nComputing Systems (ICDCS), Atlanta, GA, USA, 5–8 June 2017; IEEE: New York, NY, USA, 2017; pp. 372–382. [CrossRef]\\n21. Xu, Z.; Wang, Y.; Tang, J.; Wang, J.; Gursoy, M.C. A deep reinforcement learning based framework for power-efﬁcient resource\\nallocation in cloud RANs. In Proceedings of the 2017 IEEE International Conference on Communications (ICC), Paris, France,\\n21–25 May 2017; IEEE: New York, NY, USA, 2017; pp. 1–6. [CrossRef]\\n22. Hochreiter, S.; Schmidhuber, J. Long short-term memory. Neural Comput.1997, 9, 1735–1780. [CrossRef] [PubMed]\\n23. Alﬁan, G.; Syafrudin, M.; Ijaz, M.F.; Syaekhoni, M.A.; Fitriyani, N.L.; Rhee, J. A personalized healthcare monitoring system for\\ndiabetic patients by utilizing BLE-based sensors and real-time data processing. Sensors 2018, 18, 2183. [CrossRef]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='Sensors 2021, 21, 3261 22 of 22\\n24. Ali, G.; Ali, T.; Irfan, M.; Draz, U.; Sohail, M.; Glowacz, A.; Sulowicz, M.; Mielnik, R.; Faheem, Z.B.; Martis, C. IoT Based Smart\\nParking System Using Deep Long Short Memory Network. Electronics 2020, 9, 1696. [CrossRef]\\n25. Nayak, S.; Misra, B.B.; Behera, H.S. Impact of data normalization on stock index forecasting.Int. J. Comput. Inf. Syst. Ind. Manag. Appl.\\n2014, 6, 257–269.\\n26. Ali, F.; El-Sappagh, S.; Islam, S.R.; Kwak, D.; Ali, A.; Imran, M.; Kwak, K.S. A smart healthcare monitoring system for heart\\ndisease prediction based on ensemble deep learning and feature fusion. Inf. Fusion2020, 63, 208–222. [CrossRef]\\n27. Li, J.; Xing, Z.; Zhang, W.; Lin, Y.; Shu, F. Vehicle Tracking in Wireless Sensor Networks via Deep Reinforcement Learning.\\nIEEE Sensors Lett.2020, 4, 1–4. [CrossRef]\\n28. Ma, H.; Ng, B. Collaborative signal processing framework and algorithms for targets tracking in wireless sensor networks.\\nIn Microelectronics: Design, Technology, and Packaging II; International Society for Optics and Photonics: Bellingham, WA, USA,\\n2006; Volume 6035, p. 60351K. [CrossRef]\\n29. Zhao, F.; Shin, J.; Reich, J. Information-driven Dynamic Sensor Collaboration. IEEE Signal Process. Mag.2002, 19, 61–72. [CrossRef]\\n30. Li, W.; Han, C. Dual sensor control scheme for multi-target tracking. Sensors 2018, 18, 1653. [CrossRef] [PubMed]\\n31. Wang, P .; Ma, L.; Xue, K. Multitarget tracking in sensor networks via efﬁcient information-theoretic sensor selection.\\nInt. J. Adv. Robot. Syst.2017, 14, 1729881417728466. [CrossRef]\\n32. Waleed, M.; Um, T.W.; Kamal, T.; Usman, S.M. Classiﬁcation of Agriculture Farm Machinery Using Machine Learning and\\nInternet of Things. Symmetry 2021, 13, 403. [CrossRef]\\n33. Alsheikh, M.A.; Lin, S.; Niyato, D.; Tan, H.P . Machine learning in wireless sensor networks: Algorithms, strategies, and\\napplications. IEEE Commun. Surv. Tutorials2014, 16, 1996–2018. [CrossRef]\\n34. Lata, S.; Mehfuz, S. Machine Learning based Energy Efficient Wireless Sensor Network. In Proceedings of the 2019 International\\nConference on Power Electronics, Control and Automation (ICPECA), New Delhi, India, 16–17 November 2019; IEEE: New York, NY ,\\nUSA, 2019; pp. 1–5. [CrossRef]\\n35. Waleed, M.; Um, T.W.; Kamal, T.; Khan, A.; Iqbal, A. Determining the Precise Work Area of Agriculture Machinery Using Internet\\nof Things and Artiﬁcial Intelligence. Appl. Sci.2020, 10, 3365. [CrossRef]\\n36. Hosseini, R.; Mirvaziri, H. A New Clustering-Based Approach for Target Tracking to Optimize Energy Consumption in Wireless\\nSensor Networks. Wirel. Pers. Commun.2020, 114, 3337–3349. [CrossRef]\\n37. Zou, T.; Li, Z.; Li, S.; Lin, S. Adaptive Energy-Efﬁcient Target Detection Based on Mobile Wireless Sensor Networks. Sensors\\n2017, 17, 1028. [CrossRef]\\n38. Feng, J.; Zhao, H. Energy-Balanced Multisensory Scheduling for Target Tracking in Wireless Sensor Networks. Sensors\\n2018, 18, 3585. [CrossRef]\\n39. Khan, M.I.; Rinner, B. Energy-aware task scheduling in wireless sensor networks based on cooperative reinforcement learning.\\nIn Proceedings of the 2014 IEEE International Conference on Communications Workshops (ICC), Sydney, NSW, Australia,\\n10–14 June 2014; IEEE: New York, NY, USA, 2014; pp. 871–877. [CrossRef]\\n40. Zhu, J.; Song, Y.; Jiang, D.; Song, H. A New Deep-Q-learning-based Transmission Scheduling Mechanism for the Cognitive\\nInternet of Things. IEEE Internet Things J.2017, 5, 2375–2385. [CrossRef]\\n41. Mohammadi, M.; Al-Fuqaha, A.; Guizani, M.; Oh, J.S. Semisupervised Deep Reinforcement Learning in Support of IoT and Smart\\nCity Services. IEEE Internet Things J.2017, 5, 624–635. [CrossRef]\\n42. Kim, Y.; Bang, H. Introduction to Kalman Filter and its Applications. In Introduction and Implementations of the Kalman Filter;\\nIntechOpen Limited 5 Princes Gate Court: London, UK, 2018. [CrossRef]\\n43. Li, Q.; Li, R.; Ji, K.; Dai, W. Kalman ﬁlter and its application. In Proceedings of the 2015 8th International Conference on Intelligent\\nNetworks and Intelligent Systems (ICINIS), Tianjin, China, 1–3 November 2015; IEEE: New York, NY, USA, 2015; pp. 74–77.\\n44. Akca, A.; Efe, M.O. Multiple model Kalman and Particle ﬁlters and applications: A survey. IFAC-PapersOnLine 2019, 52, 73–78.\\n[CrossRef]\\n45. Patel, H.A.; Thakore, D.G. Moving object tracking using kalman ﬁlter. Int. J. Comput. Sci. Mob. Comput.2013, 2, 326–332.\\n46. Mahfouz, S.; Mourad-Chehade, F.; Honeine, P .; Farah, J.; Snoussi, H. Target tracking using machine learning and Kalman ﬁlter in\\nwireless sensor networks. IEEE Sens. J.2014, 14, 3715–3725. [CrossRef]\\n47. Gunjal, P .R.; Gunjal, B.R.; Shinde, H.A.; Vanam, S.M.; Aher, S.S. Moving object tracking using kalman filter. In Proceedings of the 2018\\nInternational Conference On Advances in Communication and Computing T echnology (ICACCT), Sangamner, India, 8–9 February 2018;\\nIEEE: New York, NY, USA, 2018; pp. 544–547.\\n48. Nguyen, H.; La, H. Review of deep reinforcement learning for robot manipulation. In Proceedings of the 2019 Third IEEE\\nInternational Conference on Robotic Computing (IRC), Naples, Italy, 25–27 February 2019; IEEE: New York, NY, USA, 2019;\\npp. 590–595. [CrossRef]\\n49. Rossum, G.V . Python. 1991. Available online: https://www.python.org/ (accessed on 12 March 2020).\\n50. Abadi, M.; Agarwal, A.; Barham, P .; Brevdo, E.; Chen, Z.; Citro, C.; Corrado, G.S.; Davis, A.; Dean, J.; Devin, M.; et al. Tensorﬂow.\\n2015. Available online: https://www.tensorﬂow.org/ (accessed on 15 April 2020).\\n51. Chollet, F. Keras. 2015. Available online: https://keras.io/ (accessed on 15 April 2020).')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_pdf = PyPDFLoader('../pdf/sensors-21-03261-v2.pdf')\n",
    "documents = load_pdf.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a624813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6bdfa911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16bb58b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='sensors\\nArticle\\nEnergy Conservation for Internet of Things Tracking\\nApplications Using Deep Reinforcement Learning\\nSalman Md Sultan 1\\n , Muhammad Waleed 1\\n , Jae-Young Pyun 1,*\\n and Tai-Won Um2,*\\n/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045/gid00001\\n/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046\\nCitation: Sultan, S.M.; Waleed, M.;\\nPyun, J.-Y.; Um, T.-W. Energy\\nConservation for Internet of Things\\nTracking Applications Using Deep\\nReinforcement Learning. Sensors 2021,\\n21, 3261. https://doi.org/10.3390/\\ns21093261\\nAcademic Editor: Maria Gabriella\\nXibilia\\nReceived: 6 April 2021\\nAccepted: 6 May 2021\\nPublished: 8 May 2021\\nPublisher’s Note:MDPI stays neutral\\nwith regard to jurisdictional claims in\\npublished maps and institutional afﬁl-\\niations.\\nCopyright: © 2021 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='distributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\n1 Department of Information and Communication Engineering, Chosun University, Gwangju 61452, Korea;\\nsultanmohammadsalman@chosun.kr (S.M.S.); mwk.uet@gmail.com (M.W.)\\n2 Department of Cyber Security, College of Science and Technology, Duksung Women’s University,\\nSeoul 01369, Korea\\n* Correspondence: jypyun@chosun.ac.kr (J.-Y.P .); twum@duksung.ac.kr (T.-W.U.)\\nAbstract: The Internet of Things (IoT)-based target tracking system is required for applications such\\nas smart farm, smart factory, and smart city where many sensor devices are jointly connected to\\ncollect the moving target positions. Each sensor device continuously runs on battery-operated power,\\nconsuming energy while perceiving target information in a particular environment. To reduce sensor\\ndevice energy consumption in real-time IoT tracking applications, many traditional methods such'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='device energy consumption in real-time IoT tracking applications, many traditional methods such\\nas clustering, information-driven, and other approaches have previously been utilized to select\\nthe best sensor. However, applying machine learning methods, particularly deep reinforcement\\nlearning (Deep RL), to address the problem of sensor selection in tracking applications is quite\\ndemanding because of the limited sensor node battery lifetime. In this study, we proposed a long\\nshort-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the\\nproblem of energy consumption in IoT target applications. The proposed method is utilized to select\\nthe energy-efﬁcient best sensor while tracking the target. The best sensor is deﬁned by the minimum\\ndistance function (i.e., derived as the state), which leads to lower energy consumption. The simulation\\nresults show favorable features in terms of the best sensor selection and energy consumption.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='results show favorable features in terms of the best sensor selection and energy consumption.\\nKeywords: deep reinforcement learning; internet of things; target tracking; best sensor selection;\\nenergy consumption\\n1. Introduction\\nIn a 5G sensor network, a massive amount of data are handled via sensor devices in a\\nlarge area. International Data Corporation (IDC) research states that 70% of companies will\\ndrive to use 1.2 billion devices for the connectivity management solution by 5G services\\nworldwide [1]. The Internet of Things (IoT) is the future of massive connectivity under\\n5G sensor networks. Currently, the IoT is performing a vital role in collecting a large\\namount of data via numerous sensors in real-time applications [2]. Kevin Ashton initially\\ncoined the IoT concept in 1999 [1,3]. Sensor-based IoT devices can provide various types of\\nservices, such as health, trafﬁc congestion control, robotics, and data analysis, which play'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='services, such as health, trafﬁc congestion control, robotics, and data analysis, which play\\na signiﬁcant role in daily life assistance [4]. Target tracking is another critical area where\\nthe sensors can be utilized to collect the target real-time position and report it to a server\\nwith its relevant information. The practice of tracking one or multiple targets has vast\\napplications in different research areas, such as object tracking (e.g., player, vehicle) [5–7],\\nborder monitoring to prevent illegal crossing, or battleﬁeld surveillance [8], infrared target\\nrecognition [9,10].\\nIn IoT target-tracking scenarios, tracking single or multiple targets can be realized\\nusing one or more sensors. However, it is impractical to utilize a single sensor for col-\\nlecting the target position information owing to an extended area and will take increased\\ncomputation with low tracking accuracy [ 11]. Therefore, it is pertinent to use multiple'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='computation with low tracking accuracy [ 11]. Therefore, it is pertinent to use multiple\\nsensors, particularly in tracking applications. Energy consumption in sensor applications is\\nSensors 2021, 21, 3261. https://doi.org/10.3390/s21093261 https://www.mdpi.com/journal/sensors'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2'}, page_content='Sensors 2021, 21, 3261 2 of 22\\na key task because of the sensor battery lifetime [11,12]. Moreover, it is unable to recharge\\nthe sensor battery in most cases. As a result, it is essential to efﬁciently reduce energy\\nconsumption because energy conservation leads to an increased battery lifespan. There\\nare various energy consumption reduction methods used in recent years (e.g., clustering,\\nsupport vector machine) [13,14]. However, large-scale functional implementation of these\\napproaches precludes more time and resources.\\nReinforcement learning (RL) is a machine learning subﬁeld that solves a problem\\nwithout any predeﬁned model. The RL agent learns the suboptimal policy by interacting\\nwith an unknown environment in real-time decision-based applications [ 15]. The use\\nof RL comprises two main elements: action and reward. In any dynamic interactive\\nenvironment, a precisely selected action will provide the best reward. Thus, providing the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2'}, page_content='environment, a precisely selected action will provide the best reward. Thus, providing the\\nbest outcome, based on current observations after acquiring a good reward in a real-time\\nenvironment. However, a massive number of autonomous IoT sensors are employed to\\nintelligently work with a dynamic environment to handle big data in next-generation\\n5G-based IoT applications (i.e., vehicle tracking, pedestrian tracking) [16]. Figure 1 shows\\nsome applications (e.g., smart transportation system, Intelligent Security System) including\\ndifferent types of sensors in the area of autonomous IoT. These autonomous IoT sensors\\ninteract and sense the environment to collect and send the relevant information to agent\\nfor taking the suboptimal action. The conventional RL algorithm (e.g., Tabular Q-learning)\\ntakes a higher time to handle this IoT environment because of large dimension sensor\\ndata [17].\\nAgentState\\nReward\\nAction\\nSmart Transportation System\\n▪ Traffic Signal Control\\n▪ Smart Parking System'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2'}, page_content='Reward\\nAction\\nSmart Transportation System\\n▪ Traffic Signal Control\\n▪ Smart Parking System\\n▪ Autonomous Driving\\nIntelligent Security System\\n▪ Vehicle Tracking\\n▪ Border Monitoring\\n▪ Pedestrian Tracking\\nGPS Speed Sensor Camera RFID\\nApplication\\nAutonomous IoT Sensor\\nData Collection\\nFigure 1. Autonomous IoT applications.\\nDeep reinforcement learning (Deep RL) is an extended version of the conventional\\nRL algorithm to overcome iteration complexity in any large dynamic and interactive\\nenvironment [18]. Deep neural network (described as Q-approximator in this paper) is\\nthe main feature of Deep RL, predicting a suboptimal action from a speciﬁc state. In an\\nautonomous IoT target tracking system, Deep RL can be deployed to the sensor devices to\\nminimize the overall system computational complexity and energy consumption [17,19].\\nMoreover, there are different kinds of Q-approximators used in the Deep RL method to\\nsolve the energy consumption problem. Dense and long short-term memory (LSTM)-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2'}, page_content='solve the energy consumption problem. Dense and long short-term memory (LSTM)-\\nbased Q-approximators are frequently utilized to increase energy efﬁciency in time-series\\nenvironments [20,21]. Note that the LSTM Q-approximator is more suitable than the dense\\nQ-approximator because of long-term dependencies in an IoT target tracking environment.\\nThe long-term memory features regulate the essential information sequentially (i.e., time-\\ndependent) to achieve better performance in the learning period [22–24].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3'}, page_content='Sensors 2021, 21, 3261 3 of 22\\nIn this study, we proposed a novel Deep RL framework that predicts the suboptimal\\nenergy-efﬁcient sensor to track the target in IoT tracking applications. Our proposed\\nsystem utilizes an LSTM deep Q-network (LSTM-DQN) as Q-approximator. Moreover,\\na data pre-processing approach is used for better state representation before applying\\nLSTM Q-approximator. The data pre-processing (e.g., normalization, feature selection)\\nis signiﬁcant for achieving stable LSTM Q-approximator [ 25,26]. In this paper, we use\\nmini-max normalization into our designed state space to improve LSTM Q-approximator\\nperformance. Furthermore, we also study epsilon-greedy and softmax action-selection\\nstrategies [27] in our proposed target tracking environment. However, the epsilon-greedy\\nmethod has faster improvement and convergence ability than the softmax method in our\\naction space. Therefore, we proposed an LSTM-DQN-epsilon-greedy method and com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3'}, page_content='action space. Therefore, we proposed an LSTM-DQN-epsilon-greedy method and com-\\npare it with LSTM-DQN-softmax, Dense-DQN-epsilon-greedy, and Dense-DQN-softmax\\napproaches in terms of average cumulative rewards, loss convergence, average sensor\\nselection accuracy, and average cumulative energy consumption.\\nThe remainder of this paper is organized as follows. A description of the related work\\nis provided in Section 2. Section 3 presents the system preliminaries. Sections 4 and 5 show\\nour proposed LSTM-DQN-epsilon-greedy algorithm and numerical results, respectively,\\nfor a detailed comparison. Finally, Section 6 presents the conclusion and future directions\\nof the research work.\\n2. Related Work\\nIn recent years, researchers have been working and investing much of their time to\\nsolve the problem of excessive energy consumption in tracking-based applications. Below,\\napplications based on the respective techniques from background studies are presented.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3'}, page_content='applications based on the respective techniques from background studies are presented.\\n2.1. Tracking Application Based on Information-Driven Approaches\\nInformation-driven is a collaborative sensing technique for various target tracking\\napplications, where each deployed sensor is responsible for collaborating with other de-\\nployed sensors to collect moving target information [ 28]. Information-driven methods\\nwere ﬁrst proposed in terms of collaborative sensor selection via the information utility\\nfunction [29]. In this information-driven sensor selection method, the authors considered\\ndifferent Bayesian estimation problems (e.g., entropy and Mahalanobis distance-based util-\\nity measurements) to determine which sensor would track the moving target.Wei et al. [30]\\nproposed a dual-sensor control technique based on the information utility function in a\\nmulti-target tracking application. In this work, the authors used the posterior distance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3'}, page_content='multi-target tracking application. In this work, the authors used the posterior distance\\nbetween sensor and targets (PDST) function to minimize the distance between sensors\\nand targets, which helped the sensors directly drive the targets. Ping et el. in [ 31] used\\na partially observed Markov decision process (POMDP) to select suboptimal sensors for\\ntracking multiple targets. The POMDP sensor selection approach is implemented by maxi-\\nmizing the information gain via a probability hypothesis density (PHD)-based Bayesian\\nframework. Although the techniques proposed in [29–31] illustrated good tracking results,\\nthere is a limitation in choosing an energy-efﬁcient sensor to make their model work in an\\nintelligent manner to reduce the computational complexity.\\n2.2. Machine Learning-Based Techniques for Tracking Application\\nMachine learning is an excellent technique to overcome the computational complexity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3'}, page_content='Machine learning is an excellent technique to overcome the computational complexity\\nissue in any complicated engineering problem because it is a self-learner, and it does not\\nneed to be reprogrammed [32–35]. Based on background studies, there are three types of\\nmachine learning approaches (i.e., supervised, unsupervised, and reinforcement learning),\\nwhich have been intelligently utilized for energy optimization. The study of supervised\\nlearning techniques is beyond the scope of this research.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}, page_content='Sensors 2021, 21, 3261 4 of 22\\n2.2.1. Unsupervised Learning-based Clustering Approaches\\nTo address the energy consumption problem, Hosseini and Mirvaziri in [ 36] intro-\\nduced a dynamic K-means clustering-based approach to minimize the target tracking error\\nand energy consumption in wireless sensor networks (WSNs). The proposed technique\\nuses a tube-shaped layering method for the sensor nodes to reduce energy dissipation\\nduring target tracking. In addition, Tengyue et al. [37] employed a clustering algorithm to\\ncontrol the sensor energy, which detected the target in a real-time mobile sensor network.\\nThey used the k-means++ algorithm to separate the sensor nodes into sub-groups. The k-\\nmeans++ separated the sensor nodes, which carried a higher weighted probability for\\ntarget detection, and the remaining unnecessary sensors remained in sleep mode to save\\nenergy consumption. Juan and Hongwei in [ 38] proposed another clustering approach'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}, page_content='energy consumption. Juan and Hongwei in [ 38] proposed another clustering approach\\nto balance energy in terms of multisensory distributed scheduling. Their work used the\\nenergy-balance technique to control the activation and deactivation modes of communi-\\ncation modules. They employed a multi-hop coordination strategy to decrease energy\\nconsumption. However, these types of unsupervised techniques are time-consuming to\\naddress because of the lack of available prior data labeling [34].\\n2.2.2. Reinforcement Learning Approaches\\nSensor scheduling is a promising approach for reducing energy consumption in many\\ntracking applications. Muhidul et al. in [39] proposed a cooperative RL to schedule the task\\nof each node based on the current tracking environment observation. The proposed method\\nhelped the deployed sensor nodes cooperate by sharing the adjacent node information\\nduring tracking. They applied a weighted reward function that combined both energy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}, page_content='during tracking. They applied a weighted reward function that combined both energy\\nconsumption and tracking quality matrices to improve the sensor node task scheduling at\\na particular time. Moreover, transmission scheduling is another necessary task in which\\nDeep RL can be applied. Jiang et al. in [ 40] proposed an approximation technique for\\ntransmitting packets in a scheduling manner for cognitive IoT networks. Their DQN model\\nutilized two parameters (i.e., the power for packet sending via multiple channels and\\npacket dropping) to enhance the system capacity in throughput terms. They used a stacked\\nauto-encoder as a Q-function approximator that mapped the policy to maximize system\\nperformance via a utility-based reward technique. However, they exploited the action\\nusing a comprehensive index evaluation method in a single relay to sync transmission.\\nTo reduce IoT device energy consumption, Mehdi et al. [ 41] employed a Deep RL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}, page_content='To reduce IoT device energy consumption, Mehdi et al. [ 41] employed a Deep RL\\ntechnique to learn an optimal policy for indoor localization problems in IoT-based smart\\ncity services. They deployed a semi-supervised technique to classify unlabeled data and\\nintegrated classiﬁed data with label data. They used iBeacons to provide a received\\nsignal strength indicator (RSSI) as an input for a semi-supervised Deep RL model, which\\nconsists of a variational autoencoder neural network Q-learning technique to enhance\\nindoor localization performance. In [ 27], the authors used two Deep RL methods (e.g.,\\nDQN and DDPG) to adjust the activation area radius so the system can minimize the\\naverage energy consumption in terms of vehicle-to-infrastructure (V2I) technology-based\\ntracking applications. They also used two action selection strategies (e.g., epsilon-greedy\\nand softmax) to determine the activation area radius.\\nThe Deep RL method has not been widely applied for energy saving in IoT target'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}, page_content='The Deep RL method has not been widely applied for energy saving in IoT target\\ntracking applications, particularly in energy-efﬁcient sensor selection approaches. Intel-\\nligently selecting the appropriate sensor to track the target is challenging because the\\ntarget position varies over time, creating tracking environment uncertainty. In this case,\\nthe DQN-based Deep RL is a sophisticated method because it has the best learning ca-\\npability when interacting with an uncertain dynamic environment. In DQN, selecting a\\nQ-approximator for the tracking environment is vital for obtaining improved learning\\nperformance. Therefore, we utilized the LSTM Q-approximator to predict the suboptimal\\ndecisions (i.e., sensor selection) based on sequential information (i.e., target position) with\\nthe assistance of different gate operations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5'}, page_content='Sensors 2021, 21, 3261 5 of 22\\nOur study is based on a discrete action space, which means that the proposed LSTM Q-\\napproximator selects the most energy-efﬁcient sensor among a ﬁnite set of sensors. Authors\\nin [27] showed epsilon-greedy and softmax-based action selection methods for the discrete\\naction space. The epsilon-greedy-based sensor-selection technique presented improved\\nefﬁciency compared to the softmax technique in the simulation results. Thus, we proposed\\nthe LSTM-DQN method with epsilon-greedy action selection (described as LSTM-DQN-\\nepsilon-greedy in this study) in a target tracking environment to select the best sensor for\\nmaximum energy conservation. Table 1 represents a comparison of different existed RL\\nmethods to reduce the energy consumption of the sensor.\\nTable 1. Related work that use RL-based methods to reduce the energy consumption of the sensor.\\nStudy RL-Based Methods Action-Selection Solution Evaluation Metrics'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5'}, page_content='Study RL-Based Methods Action-Selection Solution Evaluation Metrics\\n[39] SARSA ( λ) epsilon-greedy sensor scheduling energy consumption\\n[40]\\nQ-table with\\nstacked autoencoder epsilon-greedy transmission scheduling\\naverage power\\nconsumption and system utility\\n[27]\\nDQN, DDPG with\\nLSTM\\nepsilon-greedy\\nand softmax\\nradius adjustment\\nof the activated area\\naverage cumulative rewards\\nand energy consumption\\nProposed\\nmethod LSTM-DQN\\nepsilon-greedy\\nand softmax best sensor selection\\naverage cumulative rewards,\\nloss convergence,\\naverage best sensor selection\\naccuracy, and average\\naverage cumulative energy\\nconsumption\\n3. Preliminaries\\n3.1. System Overview\\nFigure 2 illustrates the tracking environment where multiple sensor devices repre-\\nsented as S = {S1, S2, .....,SD} are deployed at different positions to observe the moving\\ntargets, T = { T1, T2, .....,TL}, where L is the number of targets moving in the test area.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5'}, page_content='targets, T = { T1, T2, .....,TL}, where L is the number of targets moving in the test area.\\nThe area consists of subareas X = {X1, X2, .....,XN}, where N is the number of subareas.\\nIn this study , our proposed LSTM-DQN-epsilon-greedy scheme allows one sensor to\\ntrack a single target at timet in a particular area, which eventually leads to trackingT targets\\nin N subareas. For instance, the selected sensors shown in green detect the targets, as shown\\nin Figure 2. The remaining sensors remained unselected to minimize energy consumption.\\nDeep RL \\nAgent\\nTarget State \\nInitialized\\nAction\\n(Select Best Sensor)\\nReward\\nTarget Next \\nState\\nEnvironment\\nTarget Detection Selected Sensor Unselected Sensor\\n𝟏) Area (𝑿𝟐)\\nArea (𝑿𝟑) Area (𝑿𝟒)\\nArea (𝑿\\nFigure 2. Deployed sensors for tracking target-based environment.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6'}, page_content='Sensors 2021, 21, 3261 6 of 22\\nFor suboptimal sensor selection, our proposed LSTM-DQN-epsilon-greedy-based IoT\\ntracking system tracks more than one target simultaneously in four subareas X1, X2, X3,\\nand X4, as shown in Figure 2, thus allowing the system to track all T targets in the ﬁrst\\nattempt. If we apply a single DQN algorithm for all N subareas, there is a possibility of not\\nachieving the required goal because when the system interacts with a large area, the sensor\\nselection space is more complicated to utilize the algorithm for effective simultaneous\\ntracking more than one target.\\nTo select the best sensor, it is imperative to estimate the distance between the moving\\ntarget and the sensors. A sensor with the minimum distance to the target location was\\nselected. However, in any practical scenario, the sensor has some noisy (i.e., Gaussian\\nnoise) measurements; thus, it can not collect the target position precisely. This study'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6'}, page_content='noise) measurements; thus, it can not collect the target position precisely. This study\\nconsiders that our target tracking environment is linear, including normally distributed or\\nGaussian process noise and some measurement errors. Kalman ﬁlter is suitable for any\\nlinear environment along with Gaussian noise to predict the target information with more\\nprecision [42–44]. Moreover, because of having linear features, the Kalman ﬁlter does not\\nrequire signiﬁcant memory except knowing only the prior state, which assists in predicting\\nthe target state over time [44]. Therefore, For the accurate measurement in a linear and\\nnoisy environment, the Kalman ﬁlter was used to localize the target position.\\n3.2. Kalman Filter\\nThe Kalman ﬁlter estimates the current system state from a series of noisy measure-\\nments, which is useful in tracking applications [42,45–47]. The Kalman ﬁlter is a recursive\\nestimator based on Bayesian ﬁlter theory that can compute the target state along with the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6'}, page_content='estimator based on Bayesian ﬁlter theory that can compute the target state along with the\\nuncertainty [43,44]. The system has two signiﬁcant steps: prediction and updating. Various\\nessential Kalman ﬁlter parameters are listed in Table 2.\\nTable 2. Kalman ﬁlter parameters.\\nSymbols Description\\nα0 Initial state matrix\\nP0 Initial process covariance matrix\\nαk−1 Previous state matrix\\nMk Measurement input\\nG Kalman gain\\nAcck Control variable matrix\\nPk−1 Previous process covariance matrix\\nNkα Predicted noise matrix\\nNkp Process noise matrix\\nX, Y, Z Transition matrix\\nMe Measurement error covariance matrix\\nH, I Identity matrix\\nThe initial state matrix α0 indicates the early stage target observation and consists of\\nfour key information pieces such as the x- (x) and y-axis (y) positions, velocity along the x-\\n(vx) and y-axis (vy). In general, the covariance process measures the variation in random\\nvariables. The covariance for the four random variables is deﬁned as follows:\\nσ(x, y, vx, vy) = 1\\nn −1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6'}, page_content='σ(x, y, vx, vy) = 1\\nn −1\\nn\\n∑\\ni=1\\n(xi −x)(yi −y)(x′\\ni −vx)(y′\\ni −vy), (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7'}, page_content='Sensors 2021, 21, 3261 7 of 22\\nwhere n is the number of samples, and the covariance matrix is deﬁned as σ(x, y, vx, vy)T .\\nThe initial state α0 and process covariance matrices P0 are expressed as,\\nα0 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx\\ny\\nvx\\nvy\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb (2)\\nP0 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nσ2x σxσy σxσvx σxσvy\\nσyσx σ2y σyσvx σyσvy\\nσvx σx σvx σy σ2vx σvx σvy\\nσvyσx σvyσy σvyσvx σ2vy\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (3)\\nIn the Kalman ﬁlter, the prediction step estimates the current predicted state αk and\\nthe process error covariance matrix Pk, which are expressed as,\\nαk = Xαk−1 + YAcck + Nkα, (4)\\nPk = X(Pk−1XT) +YAcck + Nkp , (5)\\nwhere αk−1 and Pk−1 denote the previous state and process error covariance matrices,\\nrespectively. The variable X represents the state transition matrix for the previous state\\nαk−1, and Y is the input transition matrix for the control vector. The Acck in (6) shows the\\nacceleration of the moving target, given as,\\nYAcck =\\n[ 1\\n2 ∆T2ax 1\\n2 ∆T2ay ∆Tax ∆Tay\\n]T, (6)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7'}, page_content='acceleration of the moving target, given as,\\nYAcck =\\n[ 1\\n2 ∆T2ax 1\\n2 ∆T2ay ∆Tax ∆Tay\\n]T, (6)\\nwhere ∆T represents the time for one cycle, while ax and ay are the acceleration control\\nvariables. In the updated step, we estimate a new measurement Mk for state prediction at\\ntime step k. The Kalman gain G is one of the main features in the Kalman ﬁlter method,\\nwhich gives the ratio of the uncertainty of error in prediction and measurement state [42].\\nMoreover, Kalman gain indicates how much the prediction state of the target should be pre-\\ncise. If the value of Kalman gain is increased gradually, which means the uncertainty error\\nof the measurement is small, and the value of the Kalman gain is low when the measure-\\nment error covariance is larger than the process error covariance. The new measurement\\nMk and gain G are described as follows:\\nMk = Z −Hαk, (7)\\nG = (Pk HT)\\nH.(Pk HT) +Me\\n, (8)\\nwhere Z, H, and Me represent the transition, identity matrix, and measurement error'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7'}, page_content=', (8)\\nwhere Z, H, and Me represent the transition, identity matrix, and measurement error\\ncovariance matrix, respectively. After estimating the Kalman gain G, the predicted state αk\\nand process error covariance matrix Pk are updated in (9) and (10), respectively:\\nαk = Xαk + GMk, (9)\\nPk = [(I −(GH)) + Pk]. (10)\\nHere, Mk is the updated measurement which is obtained by subtracting the transition or\\nmeasured matrix (Z) from the predicted state (αk) as described in (7). The update predicted\\nstate and process error covariance matrix in (9) and (10) will be used in the next time step.\\n3.3. Best Sensor Selection\\nThe designed LSTM-DQN-epsilon-greedy system uses multiple sensors to track the\\ntarget position. We consider one target at a particular time in a speciﬁc subarea as shown\\nin Figure 2. The system operates in such a manner that it does not allow all sensors\\nconcurrently to track the target due to limited battery lifespan of the sensor devices.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7'}, page_content='concurrently to track the target due to limited battery lifespan of the sensor devices.\\nTherefore, the system intelligently adjudicates to select the best sensor using our proposed\\nDeep RL method while the moving target arrives within that sensor’s range. The sensor'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content='Sensors 2021, 21, 3261 8 of 22\\nwith low energy consumption is considered the best sensor and is apportioned to acquire\\ntarget position information. In the example shown in Figure 2, if the energy consumption\\nof the four sensors (i.e., S1, S2, S3, and S4) are 6J, 5J, 7J, and 8J, respectively, then sensor S2\\nis selected to track the target. In this way, we can conserve the energy of the other three\\nsensors. As a result, the overall system capability has improved in a particular subarea.\\n3.4. Reinforcement Learning (RL)\\nThe RL agent is used as a decision-maker to take the best action ( at) from the set\\nof possible actions over the current state ( st). The RL agent does not learn with the\\nlabeled training dataset, but learns from its experience with environmental interaction.\\nDuring environmental interaction at a particular time, the agent receives an immediate\\nreward (rt) and jumps to the next state (st+1). The entire process continues until the agent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content='reward (rt) and jumps to the next state (st+1). The entire process continues until the agent\\nreaches the ﬁnal state and begins a new episode after resetting the environment.\\nTabular Q-learning (TQL) is a common model-free RL approach that is considered\\nan off-policy algorithm because the Q-function learns from the interactive environment\\nby taking random actions during exploration time [ 48]. Taking action with the help of\\nexploration is essential because initially, the agent has no idea about the new state in\\nan environment; therefore, the agent needs to explore the environment. After acquiring\\nenvironmental experience by exploration, the agent can easily exploit the environment by\\nutilizing the greedy strategy. The exploration and exploitation technique is also called the\\nepsilon-greedy technique [19]. As a result that the TQL is a value-based method, the agent\\nlearning policy is utilized through the value function (Q-value) of state-action pairs. In TQL,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content='learning policy is utilized through the value function (Q-value) of state-action pairs. In TQL,\\nthe Q-value Q(st, at) of an individual action of a particular state is stored in a matrix called\\nthe Q-table, which is updated in each time step in (11),\\nQ(st, at) = Q(st−1, at−1) + ∂(rt + γ max(Q(st+1, at+1)) −Q(st−1, at−1)), (11)\\nwhere ∂ and γ ∈[0, 1] represent the learning rate and discount factor, respectively. Note\\nthat, ∂(rt + γ max(Q(st+1, at+1))) denotes as discounted temporal difference (TD) target,\\nwhich gives the maximum Q value of next state in (11). Further, to estimate the TD error\\nduring the training of Q-learning, we subtract the value of TD target from previous Q value\\n(Q(st−1, at−1)). The learning rate is used, which tells how fast the Q-values are updated\\nalong with TD error. Moreover, the discount factor gives stability between immediate\\nand upcoming or future rewards. If the discount factor is near to 1, then the reward will'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content='and upcoming or future rewards. If the discount factor is near to 1, then the reward will\\nbe more in the future. Otherwise, the system focuses on the immediate reward when\\nthe discount factor is near to 0. However, TQL has difﬁculty in extending the Q-table\\nto a large environment, as it is only appropriate for a small environment. To extend the\\nmethod to a large environment it is necessary for an agent to learn the value function with\\na Q-approximator instead of saving all values into a Q-table.\\n3.5. Deep-Q-Network\\nThe DQN was introduced by Mnih et al. in [ 18] based on the Deep RL method\\nwith the help of a deep neural network, which is known as a Q-approximator. The Q-\\nvalues of different actions are predicted by utilizing the Q-approximator in a particular\\nstate. In DQN, there is a possibility of a signiﬁcant correlation between the data, forming\\nthe Q-approximator instability during the training period. Following this, experience'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content='the Q-approximator instability during the training period. Following this, experience\\nreplay memory and mini-batch techniques are utilized to obtain a stable Q-approximator.\\nExperience replay memory (E) stores the experience (st, at, rt, st+1) in each time step to\\nre-utilize previous experiences multiple times. After storing each experience, the DQN uses\\nthe mini-batch technique to randomly sample data from the experience replay memory to\\nconverge the Q-approximator loss. It can also reduce the correlation between the samples\\nand improve the agent’s learning performance. Moreover, we estimate the predicted and\\ntarget Q-values with two different Q-approximatorsθ and θ′, respectively, to obtain a stable'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9'}, page_content='Sensors 2021, 21, 3261 9 of 22\\nQ-approximator by optimizing the loss during the training period. The Q-approximator\\nloss L(θ) is described as,\\nL(θ) = (rt + γ max(Q(st+1, at+1; θ′)) −Q(st, at; θ))2. (12)\\n4. The Proposed LSTM-DQN-Epsilon-Greedy Method\\n4.1. Long Short-Term Memory-Based Q-approximator\\nIn our proposed system, we use LSTM as a Q-approximator to select the best sensor.\\nIn our target tracking scenario, the position of the target is updated over time. The LSTM is\\na speciﬁc type of recurrent neural network (RNN) with the ability to learn long-term de-\\npendencies that can memorize and connect related patterns over a time-series input [22,23].\\nMoreover, another reason behind deploying LSTM for our designed system is that it works\\nﬂawlessly in a dynamic environment because it depends on the gate operation. The gates\\nregulate the information ﬂow and can also decide which information should be stored or'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9'}, page_content='regulate the information ﬂow and can also decide which information should be stored or\\nremoved. The LSTM consists of four gates: forget (Fst), input (Xst), cell (Cst), and output\\n(Ost) states. These four gates store the combined information of the previously hidden\\n(ht−1) and the current input layer (xt) and apply the “sigmoid” operation to all gates\\nexcept the cell state that is ﬁnally activated by “tanh” operation, as shown in Figure 3.\\n𝒉𝒕−𝟏\\n𝒙𝒕\\n𝒄𝒕−𝟏\\n+\\n𝑭𝒔𝒕 𝑿𝒔𝒕 𝑪𝒔𝒕 𝑶𝒔𝒕~ ~~~\\nX\\nX\\n+\\n𝒄𝒕\\n~\\nX\\n𝒉𝒕\\n+Addition Multiply X ~Sigmoid ~tanh\\nFigure 3. LSTM architecture.\\nIn the LSTM mechanism, when the forget state output is near 1, it keeps the data\\nand transfers it to multiply with the previous cell state value ( Ct−1). The input and cell\\nstate gates receive the same information as the forget state gate. After separately applying\\n“sigmoid” and “tanh” operations to input and cell state gate, the outputs are multiplied'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9'}, page_content='“sigmoid” and “tanh” operations to input and cell state gate, the outputs are multiplied\\nwith each other and added to the forget state output multiplying of the previous cell state\\nvalue for acquiring a new cell state (Ct). Finally, the output of the new cell state and output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10'}, page_content='Sensors 2021, 21, 3261 10 of 22\\nstate gate after the sigmoid operation multiply with each other to obtain the new hidden\\nstate (ht).\\n4.2. Mini-Max Normalization-Based State Space\\nThe proposed LSTM-DQN-epsilon-greedy model acts as an agent that takes the\\ncurrent state as the input. Estimated minimum distance leads to low energy consumption\\nat a speciﬁc time. The sensor with the minimum distance and energy consumption is\\nconsidered to be the best sensor for an individual area. Therefore, we organized our state\\nwith individual distances (i.e., dS1 , dS2 , ...,dSD ) between the target and sensors. The distance\\nis measured at each time step by using the Euclidean distance formula in (13),\\ndSD (t) =\\n√\\n(Ptarget xcord −PxcordSD\\n)2(t) + (Ptarget ycord −PycordSD\\n)2(t), (13)\\nwhere PxcordSD\\n, PycordSD\\n, Ptarget xcord , and Ptarget ycord are the positions of all the deployed sen-\\nsors and the moving target in the two dimensional x-y plane. Furthermore, the position'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10'}, page_content='sors and the moving target in the two dimensional x-y plane. Furthermore, the position\\nof any target is computed using the Kalman ﬁlter. Note that the state has different dis-\\ntance value ranges, which can create instability for the Q-approximator. Therefore, it is\\nnecessary to preprocess the state value by normalization before sending it to the LSTM\\nQ-approximator [25]. We use the mini-max normalization method, which is represented\\nas state normalized (t) = (st−min(st))\\nmax(st)−min(st) to scale the state between 0 and 1 to enhance the state\\nquality before sending it to our proposed LSTM Q-approximator.\\n4.3. Epsilon-Greedy Discrete Action Space\\nThe discrete action space (A = {AS1 , AS2 , ..., ASD }) represents all the allocated sensors\\n(i.e., S1, S2, ..., SD), respectively, in a deﬁned area. The LSTM-DQN-epsilon-greedy agent\\nselects the best sensor as an action that consumes minimum energy during target tracking.\\nThe energy consumption ( EconSD'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10'}, page_content='The energy consumption ( EconSD\\n) of each sensor at time step ( t) is estimated using (14),\\nwhere dSD , powSensor , and ttrack indicate the distance value between a particular sensor\\n(SD) and the target, the working mode sensor power, and time to track the target in a\\nsingle area, respectively. Similarly, we measured the energy consumption for the other\\nN areas. Note that the energy consumption of all sensors is stored in an array as (Econall )\\nin (15). Furthermore, the selected sensor energy consumption ( Econaction ) and minimum\\nenergy consumption (Econmin ) are obtained from (16) and (17). Finally, we estimate the total\\nenergy consumption (Econtotal ) and energy savings in a particular observation using (18)\\nand (19), respectively:\\nEconSD\\n(t) = dSD (t) ×powsensor (t) ×ttrack (t), (14)\\nEconall (t) = EconSD:1∼D\\n(t), (15)\\nEconaction (t) = Econall [ASD ](t), (16)\\nEconmin (t) = min(Econall (t)), (17)\\nEcontotal (t) =\\nD\\n∑\\nSD=1\\nEconSD\\n(t), (18)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10'}, page_content='Econmin (t) = min(Econall (t)), (17)\\nEcontotal (t) =\\nD\\n∑\\nSD=1\\nEconSD\\n(t), (18)\\nEsave (t) = Econtotal (t) −Econaction (t). (19)\\nWe use epsilon-greedy as an action-selection strategy in the designed system be-\\ncause it is suitable for the discrete action space. In the epsilon-greedy approach, ini-\\ntially, the agent takes a random action to explore the environment through the epsilon\\nmethod. There are three key parameters: maximum-epsilon ( εmax ), minimum-epsilon\\n(εmin), and epsilon-decay ( εdecay ) that are considered to ﬁx the epsilon period. First, it\\nbegins with the maximum-epsilon value and then decays with an absolute epsilon-decay'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11'}, page_content='Sensors 2021, 21, 3261 11 of 22\\nvalue at each time step. The epsilon period is completed when the value of epsilon reaches\\nthe minimum-epsilon. Subsequently, the agent greedily exploits the environment to take\\nsuboptimal action with the proposed LSTM Q-approximator, as shown in Figure 4.\\nLSTM(8 Units)\\nDense(4)\\nOutput\\n~\\nDENSE(8)\\nDENSE(16)\\nDENSE(16)\\n~\\nDENSE(8)\\nRelu Sigmoid\\n~\\n~\\n~\\n~\\n~\\n~\\nNormalized-\\nState\\nFigure 4. Proposed LSTM Q-approximator.\\nThe rectiﬁed linear unit (ReLU) is used in the ﬁrst three layers, whereas the sig-\\nmoid activation function works at the output layer. The ReLU is used to obtain the\\nunbounded positive outcome, whereas sigmoid is used in the output layer to obtain a\\npositive bounded outcome between 0 and 1. Moreover, the LSTM Q-approximator pre-\\ndicts the Q-values for all possible actions, which are deﬁned in the action space. Finally,\\nthe agent selects the suboptimal action with the highest action-Q value that is obtained by\\narg max(Q(state normalized t,at;θ)).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11'}, page_content='arg max(Q(state normalized t,at;θ)).\\n4.4. Binary-Based Reward Space\\nThe primary goal of our proposed system is to maximize the cumulative rewards after\\na certain number of steps; therefore, it needs to generate a suitable reward mechanism\\nto improve the agent action. The binary reward function is used in the proposed system\\ndesign as follows:\\nrt =\\n{\\n1 if Econaction = Econmin\\n0 if Econaction ̸= Econmin ,\\nwhere rt is the reward at time t; further, if the energy Econaction is equal to Econmin , it returns\\n1; otherwise, the output will be 0. The proposed LSTM-DQN-epsilon-greedy system\\narchitecture and algorithm are shown in Figure 5 and Algorithm 1, respectively.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12'}, page_content='Sensors 2021, 21, 3261 12 of 22\\nInitial \\nPosition\\nEuclidian Distance \\nCalculator\\nIf \\nExplore\\nRandom Action\\nLSTM\\nSensor\\nNext-State\\nReward\\nStep()\\nMemory\\nKalman Position \\nPredictor\\nMinibatch\\nLoss & Update \\nWeights\\nPrevious\\nPosition\\nCalculate \\nEnergy \\nConsumption\\nyes\\nNo\\nConnector\\nNormalized\\n-State\\nState\\nSelected Best  \\nQ-approximator\\nFigure 5. Proposed LSTM-DQN-epsilon-greedy system architecture.\\nAlgorithm 1: The proposed LSTM-DQN-epsilon-greedy algorithm.\\nInput : Distance between sensor and target position ⊿ input = [dS1 →dSD ]\\nOutput :Best sensor selection accuracy and energy consumption\\ninitialization() ⊿ Total number of episodes eptotal , Total number of steps step total ,\\nTraining hyperparameters, Size of replay memory E, Sensor position, Target\\nkalman state\\nfor (Episode 1 to eptotal ) do\\nst = reset_environment() ⊿ Get the initial state using (13)\\nCumulative rewards, cr = 0\\nfor (time-step, t = 1 to steptotal ) do\\nPreprocess st as state normalized t ⊿ Mini-Max normalization'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12'}, page_content='Preprocess st as state normalized t ⊿ Mini-Max normalization\\nrand = random.uniform(0,1)\\nε = max(εmin, ε)\\nif (rand < ε) then\\ntake action randomly ⊿ Exploration\\nε = ε ×εdecay\\nelse\\naction = arg max(Q(state normalized t,at;θ)) ⊿ Exploitation\\nend\\nCalculate EconSD:1∼D , Econaction and Econmin ⊿ From (14), (16) and (17)\\nCalculate Econtotal and Esave ⊿ From (18) and (19)\\nPredict next target kalman state using Kalman Filter\\nCalculate st+1 ⊿ From (13)\\nNormalize st+1 as state normalized t+1\\nCalculate rt\\ncr = cr + rt ⊿ Sum of all rewards in any episode\\nE.append(state normalized t , at, rt, state normalized t+1 ) ⊿ Store experiences\\nPerform random mini-batch sampling from Experience Replay Memory E\\ntarget =\\n{\\nrt if rt = 0\\nrt + γ max(Q(state normalized t+1 , at+1; θ′)) if rt = 1\\nPerform gradient descent of (target −Q(st, at; θ))2 to update\\nQ-approximator\\nst = st+1\\nend\\nend'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13'}, page_content='Sensors 2021, 21, 3261 13 of 22\\n5. Simulation and Results\\n5.1. Environment Setup, Hyper Parameters, and Evaluation Metrics\\nTo evaluate our proposed system, a simulation platform with moving target obser-\\nvation of 16 sensor devices is considered with four subareas, where each subarea consists\\nof 200 m ×200 m. We allocated four sensors in each subarea, and each sensor can cover\\nan area of up to 50 m ×50 m. Thus, 16 sensors cover a total area of 800 m ×800 m.\\nFurthermore, the distance between each sensor was the same in each subarea. We assume\\none target in a particular subarea and extend it to four targets in four different subareas at\\na speciﬁc time. The environmental details are listed in Table 3.\\nTable 3. Details of the proposed environment.\\nParameters Value\\nTotal number of subareas (N) 4\\nSize of a subarea (XN) 200 m ×200 m\\nNumber of sensors in a subarea (XN) 4\\nTotal number of sensors in 4 subareas 16\\nEach sensor tracking range 50 m ×50 m'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13'}, page_content='Total number of sensors in 4 subareas 16\\nEach sensor tracking range 50 m ×50 m\\nPower of sensor in working mode (powsensor ) 5 watts\\nTracking time of sensor per meter (ttrack ) 2 s\\nNumber of target (each subarea) 1\\nTotal number of targets in 4 subareas 4\\nTargets initial positions [0, 0]–[200, 200]–[400, 400]–[600, 600]\\nTarget initial velocity [0.1 m/s, 0.2 m/s]\\nTarget initial acceleration [5 m/s 2, 5 m/s2]\\nDuring our simulation, we assumed that the total number of episodes was 500, where\\neach episode consisted of 100 time steps. In each time step, the target positions are\\nupdated using the Kalman ﬁlter method. Thus, we can utilize 100 different states for our\\nproposed LSTM-DQN-epsilon-greedy system in one episode. Figure 6 shows a sample of\\ndata during the experiment that contains measured values. Moreover, Figure 7 shows a\\nsample of different state values in one area after applying the normalization (i.e., mini-max'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13'}, page_content='sample of different state values in one area after applying the normalization (i.e., mini-max\\nnormalization, which was described in Section 4.2) at the time of the experiment. Here,\\nd1, d2, d3, and d4 represent the normalized distance values between the four sensors\\nand the target. The normalized state was near zero when the moving target passed near\\na particular sensor. Conversely, the particular distance values were greater than 0 and\\ngradually increased to 1 when the target moved far behind the sensor. The ﬁgure clearly\\nshows that the initial value of d1 (i.e., the distance between the ﬁrst sensor and the target) is\\nzero as the target moves very close to the ﬁrst sensor. The same is true for the other sensor\\ndistance values during the simulation period.\\nFigure 6. Some samples of measurement during simulation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14'}, page_content='Sensors 2021, 21, 3261 14 of 22\\n/uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013\\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056\\n/uni00000013/uni00000011/uni00000013\\n/uni00000013/uni00000011/uni00000014\\n/uni00000013/uni00000011/uni00000015\\n/uni00000013/uni00000011/uni00000016\\n/uni00000013/uni00000011/uni00000017\\n/uni00000013/uni00000011/uni00000018\\n/uni00000013/uni00000011/uni00000019\\n/uni00000013/uni00000011/uni0000001a\\n/uni00000013/uni00000011/uni0000001b\\n/uni00000013/uni00000011/uni0000001c'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14'}, page_content='/uni00000013/uni00000011/uni0000001b\\n/uni00000013/uni00000011/uni0000001c\\n/uni00000014/uni00000011/uni00000013/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000036/uni00000057/uni00000044/uni00000057/uni00000048/uni00000003/uni0000003e/uni00000025/uni00000048/uni00000057/uni0000005a/uni00000048/uni00000048/uni00000051/uni00000003/uni00000013/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003/uni00000014/uni00000040\\n/uni00000047/uni00000014\\n/uni00000047/uni00000015\\n/uni00000047/uni00000016\\n/uni00000047/uni00000017\\nFigure 7. Normalized state value for each time step during the experiment.\\nNote that we restart each episode when the number of steps reaches 100, and targets\\nagain start moving from the initial position. Moreover, some useful hyperparameters were\\nset during the training session, as presented in Table 4. These parameters are used to tune'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14'}, page_content='set during the training session, as presented in Table 4. These parameters are used to tune\\nthe proposed LSTM-DQN-epsilon-greedy scheme to achieve a more stable output. These\\nhyperparameter values were chosen by a trial and error process. We performed simulations\\nusing Python 3.7.7 [ 49]. TensorFlow 2.0.0 and Keras 2.3.1 were used to implement the\\nLSTM Q-approximator [50,51].\\nTable 4. Hyperparameters for LSTM-DQN-epsilon-greedy during training.\\nHyperparameter Value\\nOptimizer adam\\nLoss categorical crossentropy\\nBatch Size 16\\nSize of experience replay memory (E) 50\\nLearning rate (∂) 0.001\\nDiscount factor (γ) 0.9\\nMaximum epsilon (εmax ) 1\\nMinimum epsilon (εmin) 0.01\\nEpsilon decay (εdecay ) 0.995\\nThe mathematical formulas to evaluate our proposed method are shown in Table 5.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15'}, page_content='Sensors 2021, 21, 3261 15 of 22\\nTable 5. A list of evaluation metrics.\\nDeﬁnition Formula\\nCumulative rewards\\n(described in Section 5.2.1) cr = ∑101\\nt=1 rt\\nBest sensor selection accuracy.\\nhere, TBest ASD\\n= total number of predicted\\nbest sensor and\\nTWrong ASD\\n= total number of predicted\\nwrong sensor\\n(described in Section 5.2.2) AccSD = (\\nTBest ASD\\nTBest ASD\\n+ TWrong ASD\\n) ×100\\nAverage cumulative reward.\\nhere, ep denotes the episode\\nand X1, X2, X3, and X4 are\\nfour system subareas.\\n(described in Section 5.3.1) avgcr =∑501\\nep=1\\ncrX1\\n(ep)+crX2\\n(ep)+crX3\\n(ep)+crX4\\n(ep)\\n4\\nThe categorical crossentropy loss convergence.\\nhere, yj = rt + γ max(Q(st+1, at+1; θ′)),\\ny′\\nj = Q(st, at; θ) and\\ns = size of the action space\\n(described in Section 5.3.2) CCLoss = −∑s\\njs=1 yj log(y′\\nj)\\nAverage best sensor selection accuracy\\nhere, D is the total number of sensor\\n(described in Section 5.3.3) avgAcc = ∑D\\nSD=1 AccSD\\nD\\nAverage cumulative energy consumption\\n(described in Section 5.3.4) avgEcon = ∑501\\nep=1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15'}, page_content='SD=1 AccSD\\nD\\nAverage cumulative energy consumption\\n(described in Section 5.3.4) avgEcon = ∑501\\nep=1\\nEconaction X1\\n(ep)+Econaction X2\\n(ep)+Econaction X3\\n(ep)+Econaction X4\\n(ep)\\n4\\n5.2. Results\\n5.2.1. Cumulative Rewards\\nIn our proposed LSTM-DQN-epsilon-greedy method, we ﬁrst measure the cumulative\\nrewards (cr) as shown in Table 5 for each episode. The estimation of the cumulative reward\\nis important because it indicates the agent’s learning performance during interaction with\\nthe target tracking environment. The proposed agent receives a reward of 1 when the agent\\nsuccessfully selects the best sensor, as discussed brieﬂy in Sections 4.3 and 4.4. In Figure 8,\\nthe cumulative reward is shown per episode for each subarea. It shows that the cumulative\\nreward is less than 35 for each subarea and does not reach the highest value in the ﬁrst\\ntwo episodes (200 steps), as it initially explores the environment. In general, the explo-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15'}, page_content='two episodes (200 steps), as it initially explores the environment. In general, the explo-\\nration duration depends on the epsilon parameter values (i.e., εmax , εmin, and εdecay ) given\\nin Table 3.\\nFollowing the exploration stage, the proposed agent starts exploiting the environ-\\nment through a greedy approach for selecting the best sensor to track the target. In this\\ncase, the agent selects the suboptimal action based on the maximum predicted action-Q\\nvalue. During the greedy process, the cumulative reward gradually increased after the\\nsecond episode for all subareas. As we have 100 different states in each episode, therefore,\\nthe maximum cumulative reward is 100. The proposed agent needs to obtain the highest\\ncumulative reward as early as possible to reduce the energy consumption of the sensor.\\nWith the proposed method, the highest cumulative reward up to 100 was achieved before\\nreaching 100 episodes for all subareas. The ﬂow of maximum cumulative rewards is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15'}, page_content='reaching 100 episodes for all subareas. The ﬂow of maximum cumulative rewards is\\nsigniﬁcantly stable, showing outstanding performance while selecting the best sensor.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16'}, page_content='Sensors 2021, 21, 3261 16 of 22\\n/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013\\n/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048\\n/uni00000014/uni00000013\\n/uni00000015/uni00000013\\n/uni00000016/uni00000013\\n/uni00000017/uni00000013\\n/uni00000018/uni00000013\\n/uni00000019/uni00000013\\n/uni0000001a/uni00000013\\n/uni0000001b/uni00000013\\n/uni0000001c/uni00000013\\n/uni00000014/uni00000013/uni00000013/uni00000026/uni00000058/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000056/uni00000003\\n/uni00000024/uni00000055/uni00000048/uni00000044/uni00000010/uni00000014\\n/uni00000024/uni00000055/uni00000048/uni00000044/uni00000010/uni00000015'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16'}, page_content='/uni00000024/uni00000055/uni00000048/uni00000044/uni00000010/uni00000015\\n/uni00000024/uni00000055/uni00000048/uni00000044/uni00000010/uni00000016\\n/uni00000024/uni00000055/uni00000048/uni00000044/uni00000010/uni00000017\\nFigure 8. Cumulative rewards for each area.\\n5.2.2. Best Sensor Selection Accuracy\\nAs a result that sensors have a limited battery lifetime, it is essential to reduce energy\\nconsumption as much as possible. In the proposed scheme, the system selects the four best\\nsensors at a particular time within an area of 800 m×800 m divided into Areas 1, 2, 3, and 4,\\nas shown in Figure 2. Due to having different ranges of state values, it is difﬁcult to achieve\\nbetter accuracy of best sensor selection by our proposed LSTM Q-approximator. As a result,\\nour proposed agent selects the energy-efﬁcient sensor based on normalized state, which has\\nbeen described in Section 4.2. Furthermore, the accuracy of selecting the best sensor affects'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16'}, page_content='been described in Section 4.2. Furthermore, the accuracy of selecting the best sensor affects\\nenergy consumption during the tracking target because the best sensor selection is based on\\nthe minimum energy consumption described in Section 4.3. Figure 9 shows the best sensor\\nselection accuracy for the 16 sensors (as formulated in Table 5). This demonstrates that the\\nproposed LSTM-DQN-epsilon-greedy system has a signiﬁcant accuracy of approximately\\n99% for sensors 1, 8, 12, 14, and 16. Similarly, the system achieved an accuracy of 98% for\\nsensors 4, 5, 6, and 10. Moreover, the proposed system provides more than 90% accuracy\\nin the case of all other sensors, leading to promising results.\\n5.3. Comparative Analysis\\nThe proposed LSTM-DQN-epsilon-greedy system is also compared with three bench-\\nmark schemes: LSTM-DQN-softmax, Dense-DQN-epsilon-greedy, and Dense-DQN-softmax\\nin terms of average cumulative reward, loss convergence, average best sensor selection'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16'}, page_content='in terms of average cumulative reward, loss convergence, average best sensor selection\\naccuracy, and cumulative energy consumption. In DQN, the LSTM and dense-based Q-\\napproximator are used frequently for the dynamic environment. However, LSTM exhibits\\nbetter performance in handling such an environment because of memory features. We also\\nutilized different action-selection strategies (e.g., epsilon-greedy and softmax) compared\\nwith our scheme.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17'}, page_content='Sensors 2021, 21, 3261 17 of 22\\n(a) (b)\\n(c) (d)\\nFigure 9. Best sensor selection accuracy: (a) Sensor selection accuracy for Area 1; (b) Sensor selection\\naccuracy for Area 2; (c) Sensor selection accuracy for Area 3; (d) Sensor selection accuracy for Area 4.\\n5.3.1. Average Cumulative Reward\\nThe key designed method deployment objective is to increase the average cumulative\\nreward (avgcr ) as described in Table 5 to measure the agent’s performance. Figure 10 shows\\nthe average cumulative reward per episode for the four DQN-based schemes. The ﬁgure\\nshows that our proposed model and the LSTM-DQN-softmax model both achieved the\\nhighest average cumulative reward, which was up to 100 during the simulation period.\\nHowever, LSTM-DQN-epsilon-greedy reached achieved the highest value faster in 63\\nepisodes compared to the LSTM-DQN-softmax, which reached that level in 115 episodes.\\nThe efﬁciency of our proposed system is that the epsilon-greedy action selection strategy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17'}, page_content='The efﬁciency of our proposed system is that the epsilon-greedy action selection strategy\\ndirectly learns from the action-Q-value function, which is suitable for discrete action space.\\n/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013/uni00000016/uni00000018/uni00000013/uni00000017/uni00000013/uni00000013/uni00000017/uni00000018/uni00000013/uni00000018/uni00000013/uni00000013\\n/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048\\n/uni00000014/uni00000013\\n/uni00000015/uni00000013\\n/uni00000016/uni00000013\\n/uni00000017/uni00000013\\n/uni00000018/uni00000013\\n/uni00000019/uni00000013\\n/uni0000001a/uni00000013\\n/uni0000001b/uni00000013\\n/uni0000001c/uni00000013'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17'}, page_content='/uni00000019/uni00000013\\n/uni0000001a/uni00000013\\n/uni0000001b/uni00000013\\n/uni0000001c/uni00000013\\n/uni00000014/uni00000013/uni00000013/uni00000003/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000026/uni00000058/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000056/uni00000003\\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000056/uni00000048/uni00000047/uni00000003/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000048/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000010/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17'}, page_content='/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000056/uni00000052/uni00000049/uni00000057/uni00000050/uni00000044/uni0000005b\\n/uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000048/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000010/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c\\n/uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000056/uni00000052/uni00000049/uni00000057/uni00000050/uni00000044/uni0000005b\\nFigure 10. Average cumulative rewards per episode.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18'}, page_content='Sensors 2021, 21, 3261 18 of 22\\nFurthermore, the comparison has been extended to the other two Dense-DQN-based\\nschemes: Dense-DQN-epsilon-greedy and Dense-DQN-softmax. The performance of both\\nLSTM-DQN-based approaches is better than that of Dense-DQN methods because of the\\nlong-term memory dependencies. Therefore, both the Dense-DQN-epsilon-greedy and\\nDense-DQN-softmax schemes are unable to reach the highest average cumulative reward\\nover the entire 500 episodes, and the average cumulative reward increase of both methods\\nis much slower than the proposed LSTM-DQN-epsilon-greedy scheme.\\n5.3.2. Loss Convergence\\nThe loss convergence depreciation to the minimum level is also vital, along with the\\nsystem stability. To estimate the loss of our proposed Q-approximator, we use categorical\\ncrossentropy because it is suitable for multiclass classiﬁcation problems (as presented in\\nTable 5). The proposed LSTM-DQN-epsilon-greedy system signiﬁes good convergence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18'}, page_content='Table 5). The proposed LSTM-DQN-epsilon-greedy system signiﬁes good convergence\\nbehavior around 200,000 epochs and is more stable, as illustrated in Figure 11. Moreover,\\nthe LSTM-DQN-softmax convergence also appeared around 200,000 epochs, but was less\\nstable than our proposed scheme. Furthermore, Dense-DQN-epsilon-greedy and Dense-\\nDQN-softmax methods show unstable behavior and converge at 500,000 epochs, which is\\ntime-consuming. Therefore, the proposed LSTM-DQN-epsilon-greedy algorithm is efﬁcient\\nand stable, leading to promising results.\\n(a) (b)\\n(d)(c)\\nFigure 11. Loss convergence per epoch during training: ( a) loss convergence for proposed epsilon-greedy-LSTM-DQN;\\n(b) loss convergence for softmax-LSTM-DQN; (c) loss convergence for epsilon-greedy-Dense-DQN; (d) loss convergence\\nfor softmax-Dense-DQN.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19'}, page_content='Sensors 2021, 21, 3261 19 of 22\\n5.3.3. Average Best Sensor Selection Accuracy\\nIn this section, we compared the average best sensor selection accuracy (as described\\nin Table 5) of the proposed system with that of the other three DQN methods, as presented\\nin Figure 12. In our study, the agent selects the best sensor that has minimum energy con-\\nsumption when the target moves in any particular area. The critical task is to signiﬁcantly\\nenhance the best sensor selection accuracy to reduce the average energy consumption.\\nAs shown in Figure 12, the proposed system agent selects the best sensor with a slightly\\nhigher average accuracy than LSTM-DQN-softmax. Furthermore, the proposed LSTM-\\nDQN-epsilon-greedy scheme achieved signiﬁcantly higher best sensor selection accuracy\\nthan the Dense-DQN-epsilon-greedy and Dense-DQN-softmax methods.\\n/uni0000001b/uni00000013/uni00000011/uni00000013\\n/uni0000001b/uni00000015/uni00000011/uni00000018\\n/uni0000001b/uni00000018/uni00000011/uni00000013'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19'}, page_content='/uni0000001b/uni00000015/uni00000011/uni00000018\\n/uni0000001b/uni00000018/uni00000011/uni00000013\\n/uni0000001b/uni0000001a/uni00000011/uni00000018\\n/uni0000001c/uni00000013/uni00000011/uni00000013\\n/uni0000001c/uni00000015/uni00000011/uni00000018\\n/uni0000001c/uni00000018/uni00000011/uni00000013\\n/uni0000001c/uni0000001a/uni00000011/uni00000018\\n/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000056/uni00000048/uni00000051/uni00000056/uni00000052/uni00000055/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000003e/uni00000008/uni00000040\\n/uni0000001c/uni0000001a/uni00000011/uni00000014/uni0000001b/uni0000001c/uni00000019/uni00000011/uni0000001a/uni00000014'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19'}, page_content='/uni0000001c/uni00000016/uni00000011/uni00000017/uni00000017\\n/uni0000001b/uni0000001b/uni00000011/uni00000014/uni00000019\\n/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000056/uni00000048/uni00000047/uni00000003/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000048/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000010/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c\\n/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000056/uni00000052/uni00000049/uni00000057/uni00000050/uni00000044/uni0000005b\\n/uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000048/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000010/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19'}, page_content='/uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000056/uni00000052/uni00000049/uni00000057/uni00000050/uni00000044/uni0000005b\\nFigure 12. Average best sensor selection accuracy.\\n5.3.4. Average Cumulative Energy Consumption\\nOur designed system was also utilized to reduce the average cumulative energy\\nconsumption while tracking the target. We already mentioned in Sections 5.3.1 and 5.3.3,\\nthat a higher average cumulative reward effectively enhances the best sensor selection\\naccuracy and reduces the average cumulative energy consumption. The average cumulative\\nenergy consumption (avgEcon ) is obtained using a formula, which is shown in Table 5.\\nFigure 13 shows the average cumulative energy consumption in 500 episodes. It can\\nbe observed from the ﬁgure that the average cumulative energy consumption for each\\nmethod is higher, particularly in the ﬁrst 100 episodes. The reason behind it is that initially,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19'}, page_content='method is higher, particularly in the ﬁrst 100 episodes. The reason behind it is that initially,\\nthe agent has no experience with the environment. However, as the number of episodes\\nincreases, the average cumulative energy consumption decreases signiﬁcantly for both\\nLSTM-DQN- and Dense-DQN-based schemes.\\nIn contrast, both LSTM-DQN-epsilon-greedy and LSTM-DQN-softmax methods have\\nmuch lower average cumulative energy consumption compared to Dense-DQN-epsilon-\\ngreedy and Dense-DQN-softmax because the LSTM Q-approximator can regulate the\\ninformation ﬂow in memory in the long and short term. Furthermore, both the LSTM-\\nDQN-epsilon-greedy and LSTM-DQN-softmax schemes approximately reduce the same\\naverage cumulative energy consumption in each episode except 1 to 200. However, the pro-\\nposed LSTM-DQN-epsilon-greedy method shows a faster and better reduction of the\\naverage cumulative energy consumption than LSTM-DQN-softmax, particularly in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19'}, page_content='average cumulative energy consumption than LSTM-DQN-softmax, particularly in the\\nﬁrst 100 episodes. Thus, our designed LSTM-DQN-epsilon-greedy method signiﬁcantly'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='Sensors 2021, 21, 3261 20 of 22\\nreduced the average cumulative energy consumption compared to the other three methods\\nby selecting the best energy-efﬁcient sensor in our designed target tracking environment.\\n/uni00000014/uni00000010/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000010/uni00000015/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000010/uni00000016/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000010/uni00000017/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000010/uni00000018/uni00000013/uni00000013\\n/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048\\n/uni00000015/uni00000013/uni00000013/uni00000013/uni00000013\\n/uni00000015/uni00000015/uni00000013/uni00000013/uni00000013\\n/uni00000015/uni00000017/uni00000013/uni00000013/uni00000013\\n/uni00000015/uni00000019/uni00000013/uni00000013/uni00000013\\n/uni00000015/uni0000001b/uni00000013/uni00000013/uni00000013'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='/uni00000015/uni0000001b/uni00000013/uni00000013/uni00000013\\n/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013\\n/uni00000016/uni00000015/uni00000013/uni00000013/uni00000013\\n/uni00000016/uni00000017/uni00000013/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000046/uni00000058/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000046/uni00000052/uni00000051/uni00000056/uni00000058/uni00000050/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000003e/uni0000002d/uni00000040'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000056/uni00000048/uni00000047/uni00000003/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000048/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000010/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c\\n/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000056/uni00000052/uni00000049/uni00000057/uni00000050/uni00000044/uni0000005b\\n/uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000048/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000010/uni0000004a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='/uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000010/uni00000027/uni00000034/uni00000031/uni00000010/uni00000056/uni00000052/uni00000049/uni00000057/uni00000050/uni00000044/uni0000005b\\nFigure 13. Average cumulative energy consumption.\\n6. Conclusions and Future Directions\\nSensors are widely used in IoT applications (e.g., tracking and attaining target location\\ninformation). In such scenarios, energy consumption optimization is a critical challenge\\nbecause of the sensor battery lifespan. For this reason, an adequate learning method with\\nDeep RL has been proposed to overcome the problem of energy consumption. The pro-\\nposed idea is based on selecting the best sensor with minimum energy using the proposed\\nDeep RL agent at a particular time to collect the target location information. The Kalman\\nﬁlter and LSTM-DQN-epsilon-greedy algorithms have been utilized to predict the target\\nposition and best sensor selection, respectively. Furthermore, we compared our proposed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='position and best sensor selection, respectively. Furthermore, we compared our proposed\\nLSTM-DQN-epsilon-greedy system with the other three benchmark schemes: LSTM-DQN-\\nsoftmax, Dense-DQN-epsilon-greedy, and Dense-DQN-softmax. A comparative analysis\\nwas performed in terms of average cumulative reward, loss convergence, average best\\nsensor selection accuracy, and cumulative energy consumption. Our proposed LSTM-\\nDQN-epsilon-greedy method addresses the problem of best sensor selection and converges\\nthe energy consumption issue efﬁciently, which is signiﬁcantly improved in our tracking\\nenvironment than the other three methods.\\nThe limitation of the proposed scheme is that we only considered the linear target\\ninformation using the Kalman ﬁlter. However, the target position can be non-linear, which\\nis out of scope of this study. Moreover, the framework is unable to track multiple targets in\\none subarea at a particular time. To track the multiple targets information simultaneously,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='one subarea at a particular time. To track the multiple targets information simultaneously,\\nwe need to activate more than one sensor in one subarea. The framework will be extended\\nto use multi-agent-based Deep RL in future work to control the multiple sensors efﬁciently.\\nFinally, the system could also leverage hardware in the future to carry out real-time\\nhardware experimentation.\\nAuthor Contributions: Conceptualization, S.M.S., M.W. and T.-W.U.; methodology, S.M.S., M.W.;\\nsoftware, S.M.S. and M.W.; validation, J.-Y.P . and T.-W.U.; formal analysis, J.-Y.P . and T.-W.U.;\\ninvestigation, S.M.S. and M.W.; resources, J.-Y.P . and T.-W.U.; data curation, M.W. and S.M.S.;\\nwriting—original draft preparation, M.W. and S.M.S.; writing—review and editing, M.W., T.-W.U.\\nand J.-Y.P .; visualization, M.W., J.-Y.P . and T.-W.U.; project administration, J.-Y.P . and T.-W.U.; funding\\nacquisition, J.-Y.P . All authors have read and agreed to the published version of the manuscript.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='Sensors 2021, 21, 3261 21 of 22\\nFunding: This work was supported by the Institute for Information communications Technology\\nPromotion (IITP) grant funded by the Korea government (MSIT) (2018-0-00691, Development of\\nAutonomous Collaborative Swarm Intelligence Technologies).\\nInstitutional Review Board Statement: Not applicable.\\nInformed Consent Statement: Not applicable.\\nData Availability Statement: Not applicable.\\nConﬂicts of Interest: The authors declare that they have no conﬂict of interest.\\nReferences\\n1. Li, S.; Da Xu, L.; Zhao, S. 5G Internet of Things: A survey. J. Ind. Inf. Integr.2018, 10, 1–9. [CrossRef]\\n2. Farhad, A.; Kim, D.H.; Subedi, S.; Pyun, J.Y. Enhanced LoRaWAN Adaptive Data Rate for Mobile Internet of Things Devices.\\nSensors 2020, 20, 6466. [CrossRef]\\n3. Mihovska, A.; Sarkar, M. Smart Connectivity for Internet of Things (IoT) Applications. In New Advances in the Internet of Things;\\nSpringer: Berlin/Heidelberg, Germany, 2018; Volume 715, pp. 105–118. [CrossRef]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='Springer: Berlin/Heidelberg, Germany, 2018; Volume 715, pp. 105–118. [CrossRef]\\n4. Farhad, A.; Kim, D.H.; Kim, B.H.; Mohammed, A.F.Y.; Pyun, J.Y. Mobility-Aware Resource Assignment to IoT Applications in\\nLong-Range Wide Area Networks. IEEE Access2020, 8, 186111–186124. [CrossRef]\\n5. Cabrera, R.S.; de la Cruz, A.P . Public transport vehicle tracking service for intermediate cities of developing countries, based\\non ITS architecture using Internet of Things (IoT). In Proceedings of the 2018 21st International Conference on Intelligent\\nTransportation Systems (ITSC), Maui, HI, USA, 4–7 November 2018; IEEE: New York, NY, USA, 2018; pp. 2784–2789.\\n6. Raad, M.W.; Deriche, M.; Sheltami, T. An IoT-Based School Bus and Vehicle Tracking System Using RFID Technology and Mobile\\nData Networks. Arab. J. Sci. Eng.2021, 46, 3087–3097. [CrossRef]\\n7. Zhang, R.; Wu, L.; Yang, Y.; Wu, W.; Chen, Y.; Xu, M. Multi-camera multi-player tracking with deep player identiﬁcation in sports'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='video. Pattern Recognit.2020, 102, 107260. [CrossRef]\\n8. Karthick, R.; Prabaharan, A.M.; Selvaprasanth, P . Internet of things based high security border surveillance strategy.Asian J. Appl.\\nSci. Technol. (AJAST)2019, 3, 94–100.\\n9. Zhang, R.; Xu, L.; Yu, Z.; Shi, Y.; Mu, C.; Xu, M. Deep-IRTarget: An Automatic Target Detector in Infrared Imagery using\\nDual-domain Feature Extraction and Allocation. IEEE Trans. Multimed.2021, 1. [CrossRef]\\n10. Zhang, R.; Mu, C.; Yang, Y.; Xu, L. Research on simulated infrared image utility evaluation using deep representation.\\nJ. Electron. Imaging2018, 27, 013012. [CrossRef]\\n11. Ez-Zaidi, A.; Rakrak, S. A comparative Study of Target Tracking Approaches in Wireless Sensor Networks. J. Sens.\\n2016, 2016, 1–11. [CrossRef]\\n12. Zhang, Y. Technology Framework of the Internet of Things and its Application. In Proceedings of the 2011 International'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='Conference on Electrical and Control Engineering, Yichang, China, 16–18 September 2011; IEEE: New York, NY, USA, 2011;\\npp. 4109–4112. [CrossRef]\\n13. Kumar, S.; Tiwari, U.K. Energy efﬁcient target tracking with collision avoidance in WSNs. Wirel. Pers. Commun. 2018,\\n103, 2515–2528. [CrossRef]\\n14. Sebastian, B.; Ben-Tzvi, P . Support vector machine based real-time terrain estimation for tracked robots. Mechatronics\\n2019, 62, 102260. [CrossRef]\\n15. Montague, P .R. Reinforcement learning: An introduction, by Sutton, RS and Barto, AG. Trends Cogn. Sci.1999, 3, 360. [CrossRef]\\n16. Wang, D.; Chen, D.; Song, B.; Guizani, N.; Yu, X.; Du, X. From IoT to 5G I-IoT: The next generation IoT-based intelligent algorithms\\nand 5G technologies. IEEE Commun. Mag.2018, 56, 114–120. [CrossRef]\\n17. Lei, L.; Tan, Y.; Zheng, K.; Liu, S.; Zhang, K.; Shen, X. Deep reinforcement learning for autonomous internet of things: Model,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='applications and challenges. IEEE Commun. Surv. Tutorials2020, 22, 1722–1760. [CrossRef]\\n18. Mnih, V .; Kavukcuoglu, K.; Silver, D.; Rusu, A.A.; Veness, J.; Bellemare, M.G.; Graves, A.; Riedmiller, M.; Fidjeland, A.K.;\\nOstrovski, G.; et al. Human-level Control through Deep Reinforcement Learning. Nature 2015, 518, 529–533. [CrossRef]\\n19. Ali Imran, M.; Flávia dos Reis, A.; Brante, G.; Valente Klaine, P .; Demo Souza, R. Machine Learning in Energy Efﬁciency\\nOptimization. In Machine Learning for Future Wireless Communications; Wiley Online Library: Hoboken, NJ, USA, 2020; pp. 105–117.\\n[CrossRef]\\n20. Liu, N.; Li, Z.; Xu, J.; Xu, Z.; Lin, S.; Qiu, Q.; Tang, J.; Wang, Y. A hierarchical framework of cloud resource allocation and power\\nmanagement using deep reinforcement learning. In Proceedings of the 2017 IEEE 37th International Conference on Distributed\\nComputing Systems (ICDCS), Atlanta, GA, USA, 5–8 June 2017; IEEE: New York, NY, USA, 2017; pp. 372–382. [CrossRef]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='21. Xu, Z.; Wang, Y.; Tang, J.; Wang, J.; Gursoy, M.C. A deep reinforcement learning based framework for power-efﬁcient resource\\nallocation in cloud RANs. In Proceedings of the 2017 IEEE International Conference on Communications (ICC), Paris, France,\\n21–25 May 2017; IEEE: New York, NY, USA, 2017; pp. 1–6. [CrossRef]\\n22. Hochreiter, S.; Schmidhuber, J. Long short-term memory. Neural Comput.1997, 9, 1735–1780. [CrossRef] [PubMed]\\n23. Alﬁan, G.; Syafrudin, M.; Ijaz, M.F.; Syaekhoni, M.A.; Fitriyani, N.L.; Rhee, J. A personalized healthcare monitoring system for\\ndiabetic patients by utilizing BLE-based sensors and real-time data processing. Sensors 2018, 18, 2183. [CrossRef]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='Sensors 2021, 21, 3261 22 of 22\\n24. Ali, G.; Ali, T.; Irfan, M.; Draz, U.; Sohail, M.; Glowacz, A.; Sulowicz, M.; Mielnik, R.; Faheem, Z.B.; Martis, C. IoT Based Smart\\nParking System Using Deep Long Short Memory Network. Electronics 2020, 9, 1696. [CrossRef]\\n25. Nayak, S.; Misra, B.B.; Behera, H.S. Impact of data normalization on stock index forecasting.Int. J. Comput. Inf. Syst. Ind. Manag. Appl.\\n2014, 6, 257–269.\\n26. Ali, F.; El-Sappagh, S.; Islam, S.R.; Kwak, D.; Ali, A.; Imran, M.; Kwak, K.S. A smart healthcare monitoring system for heart\\ndisease prediction based on ensemble deep learning and feature fusion. Inf. Fusion2020, 63, 208–222. [CrossRef]\\n27. Li, J.; Xing, Z.; Zhang, W.; Lin, Y.; Shu, F. Vehicle Tracking in Wireless Sensor Networks via Deep Reinforcement Learning.\\nIEEE Sensors Lett.2020, 4, 1–4. [CrossRef]\\n28. Ma, H.; Ng, B. Collaborative signal processing framework and algorithms for targets tracking in wireless sensor networks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='In Microelectronics: Design, Technology, and Packaging II; International Society for Optics and Photonics: Bellingham, WA, USA,\\n2006; Volume 6035, p. 60351K. [CrossRef]\\n29. Zhao, F.; Shin, J.; Reich, J. Information-driven Dynamic Sensor Collaboration. IEEE Signal Process. Mag.2002, 19, 61–72. [CrossRef]\\n30. Li, W.; Han, C. Dual sensor control scheme for multi-target tracking. Sensors 2018, 18, 1653. [CrossRef] [PubMed]\\n31. Wang, P .; Ma, L.; Xue, K. Multitarget tracking in sensor networks via efﬁcient information-theoretic sensor selection.\\nInt. J. Adv. Robot. Syst.2017, 14, 1729881417728466. [CrossRef]\\n32. Waleed, M.; Um, T.W.; Kamal, T.; Usman, S.M. Classiﬁcation of Agriculture Farm Machinery Using Machine Learning and\\nInternet of Things. Symmetry 2021, 13, 403. [CrossRef]\\n33. Alsheikh, M.A.; Lin, S.; Niyato, D.; Tan, H.P . Machine learning in wireless sensor networks: Algorithms, strategies, and\\napplications. IEEE Commun. Surv. Tutorials2014, 16, 1996–2018. [CrossRef]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='applications. IEEE Commun. Surv. Tutorials2014, 16, 1996–2018. [CrossRef]\\n34. Lata, S.; Mehfuz, S. Machine Learning based Energy Efficient Wireless Sensor Network. In Proceedings of the 2019 International\\nConference on Power Electronics, Control and Automation (ICPECA), New Delhi, India, 16–17 November 2019; IEEE: New York, NY ,\\nUSA, 2019; pp. 1–5. [CrossRef]\\n35. Waleed, M.; Um, T.W.; Kamal, T.; Khan, A.; Iqbal, A. Determining the Precise Work Area of Agriculture Machinery Using Internet\\nof Things and Artiﬁcial Intelligence. Appl. Sci.2020, 10, 3365. [CrossRef]\\n36. Hosseini, R.; Mirvaziri, H. A New Clustering-Based Approach for Target Tracking to Optimize Energy Consumption in Wireless\\nSensor Networks. Wirel. Pers. Commun.2020, 114, 3337–3349. [CrossRef]\\n37. Zou, T.; Li, Z.; Li, S.; Lin, S. Adaptive Energy-Efﬁcient Target Detection Based on Mobile Wireless Sensor Networks. Sensors\\n2017, 17, 1028. [CrossRef]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='2017, 17, 1028. [CrossRef]\\n38. Feng, J.; Zhao, H. Energy-Balanced Multisensory Scheduling for Target Tracking in Wireless Sensor Networks. Sensors\\n2018, 18, 3585. [CrossRef]\\n39. Khan, M.I.; Rinner, B. Energy-aware task scheduling in wireless sensor networks based on cooperative reinforcement learning.\\nIn Proceedings of the 2014 IEEE International Conference on Communications Workshops (ICC), Sydney, NSW, Australia,\\n10–14 June 2014; IEEE: New York, NY, USA, 2014; pp. 871–877. [CrossRef]\\n40. Zhu, J.; Song, Y.; Jiang, D.; Song, H. A New Deep-Q-learning-based Transmission Scheduling Mechanism for the Cognitive\\nInternet of Things. IEEE Internet Things J.2017, 5, 2375–2385. [CrossRef]\\n41. Mohammadi, M.; Al-Fuqaha, A.; Guizani, M.; Oh, J.S. Semisupervised Deep Reinforcement Learning in Support of IoT and Smart\\nCity Services. IEEE Internet Things J.2017, 5, 624–635. [CrossRef]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='City Services. IEEE Internet Things J.2017, 5, 624–635. [CrossRef]\\n42. Kim, Y.; Bang, H. Introduction to Kalman Filter and its Applications. In Introduction and Implementations of the Kalman Filter;\\nIntechOpen Limited 5 Princes Gate Court: London, UK, 2018. [CrossRef]\\n43. Li, Q.; Li, R.; Ji, K.; Dai, W. Kalman ﬁlter and its application. In Proceedings of the 2015 8th International Conference on Intelligent\\nNetworks and Intelligent Systems (ICINIS), Tianjin, China, 1–3 November 2015; IEEE: New York, NY, USA, 2015; pp. 74–77.\\n44. Akca, A.; Efe, M.O. Multiple model Kalman and Particle ﬁlters and applications: A survey. IFAC-PapersOnLine 2019, 52, 73–78.\\n[CrossRef]\\n45. Patel, H.A.; Thakore, D.G. Moving object tracking using kalman ﬁlter. Int. J. Comput. Sci. Mob. Comput.2013, 2, 326–332.\\n46. Mahfouz, S.; Mourad-Chehade, F.; Honeine, P .; Farah, J.; Snoussi, H. Target tracking using machine learning and Kalman ﬁlter in\\nwireless sensor networks. IEEE Sens. J.2014, 14, 3715–3725. [CrossRef]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='wireless sensor networks. IEEE Sens. J.2014, 14, 3715–3725. [CrossRef]\\n47. Gunjal, P .R.; Gunjal, B.R.; Shinde, H.A.; Vanam, S.M.; Aher, S.S. Moving object tracking using kalman filter. In Proceedings of the 2018\\nInternational Conference On Advances in Communication and Computing T echnology (ICACCT), Sangamner, India, 8–9 February 2018;\\nIEEE: New York, NY, USA, 2018; pp. 544–547.\\n48. Nguyen, H.; La, H. Review of deep reinforcement learning for robot manipulation. In Proceedings of the 2019 Third IEEE\\nInternational Conference on Robotic Computing (IRC), Naples, Italy, 25–27 February 2019; IEEE: New York, NY, USA, 2019;\\npp. 590–595. [CrossRef]\\n49. Rossum, G.V . Python. 1991. Available online: https://www.python.org/ (accessed on 12 March 2020).\\n50. Abadi, M.; Agarwal, A.; Barham, P .; Brevdo, E.; Chen, Z.; Citro, C.; Corrado, G.S.; Davis, A.; Dean, J.; Devin, M.; et al. Tensorﬂow.\\n2015. Available online: https://www.tensorﬂow.org/ (accessed on 15 April 2020).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='2015. Available online: https://www.tensorﬂow.org/ (accessed on 15 April 2020).\\n51. Chollet, F. Keras. 2015. Available online: https://keras.io/ (accessed on 15 April 2020).')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "split_document = text_splitter.split_documents(documents)\n",
    "split_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e844217e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e47082a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\" \n",
    "        write a short summary of the following speech\n",
    "        speech: {text}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ab8c6b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    provide the full summary of the entier speech with these important points.\n",
    "    Section headings as paragraph titles (e.g., Introduction, Abstract, Preliminaries, Conclusion). Keep the summary concise, clear, and preserve the technical meaning.    \n",
    "    Here is the content to summarize:\n",
    "    {text}\n",
    "    Summarize it below with section headings:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9af55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorter prompts\n",
    "# map_prompt = ChatPromptTemplate.from_template(\"Summarize this text:\\n\\n{text}\")\n",
    "# combine_prompt = ChatPromptTemplate.from_template(\"Combine the summaries below:\\n\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f78efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_chain = load_summarize_chain(\n",
    "        llm=llm,\n",
    "        chain_type=\"map_reduce\",\n",
    "        map_prompt=map_prompt,\n",
    "        combine_prompt=combine_prompt \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f9ba6e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8156 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here is a summary of the speech with section headings:\\n\\n**Introduction**\\nThe speech discusses the use of deep reinforcement learning to conserve energy in Internet of Things (IoT) tracking applications.\\n\\n**Importance of Energy Efficiency in IoT Tracking Applications**\\nThe speech highlights the importance of using multiple sensors in tracking applications and reducing energy consumption in sensors to increase battery lifespan.\\n\\n**Challenges in IoT Tracking Applications**\\nThe speech discusses the challenge of energy-efficient sensor selection in IoT tracking applications and proposes a Deep Reinforcement Learning (Deep RL) model using a Long Short-Term Memory network.\\n\\n**Applications of Deep Reinforcement Learning in IoT**\\nThe speech discusses the benefits of using deep reinforcement learning in 5G sensor networks, including the best sensor selection and reduced energy consumption.\\n\\n**Proposed Method: LSTM-DQN-Epsilon-Greedy**\\nThe speech proposes a novel Deep Reinforcement Learning (RL) framework that predicts the most energy-efficient sensor to track a target in IoT tracking applications.\\n\\n**Comparison with Other Approaches**\\nThe speech compares the proposed method with other approaches in terms of rewards, loss convergence, accuracy, and energy consumption.\\n\\n**Information-Driven Approaches in Target Tracking**\\nThe speech discusses applications of information-driven approaches in target tracking, including collaborative sensor selection and utility functions.\\n\\n**Machine Learning in Energy Optimization**\\nThe speech discusses the use of machine learning in energy optimization, including supervised, unsupervised, and reinforcement learning.\\n\\n**Unsupervised Learning-Based Clustering Approaches**\\nThe speech discusses unsupervised learning-based clustering approaches to minimize energy consumption in wireless sensor networks.\\n\\n**Deep Reinforcement Learning in IoT Devices**\\nThe speech discusses the application of Deep Reinforcement Learning (Deep RL) in IoT device energy consumption and transmission scheduling.\\n\\n**Kalman Filter in Target Tracking**\\nThe speech discusses the use of the Kalman filter in target tracking in a linear environment with Gaussian process noise and measurement errors.\\n\\n**Reinforcement Learning Methods**\\nThe speech proposes a novel approach to reduce energy consumption in IoT tracking applications using Deep Reinforcement Learning.\\n\\n**Stability of Q-Approximators**\\nThe speech discusses the instability of Q-approximators during training and proposes two solutions to achieve stability: experience replay memory and mini-batch techniques.\\n\\n**LSTM-DQN-Epsilon-Greedy Method**\\nThe speech proposes a new method for sensor selection in target tracking scenarios, called LSTM-DQN-Epsilon-Greedy method.\\n\\n**LSTM Networks**\\nThe speech explains the architecture and mechanism of Long Short-Term Memory (LSTM) networks.\\n\\n**Cell State Update**\\nThe speech describes the process of updating a cell state (Ct) in a recurrent neural network.\\n\\n**LSTM-DQN-Epsilon-Greedy Model**\\nThe speech discusses a proposed model, LSTM-DQN-Epsilon-Greedy, which acts as an agent that takes in the current state as input.\\n\\n**Target Tracking**\\nThe speech discusses a method for tracking a target in a 2D plane using a LSTM-DQN (Deep Q-Network) agent.\\n\\n**Energy Consumption**\\nThe speech explains how to estimate energy consumption of sensors in different areas.\\n\\n**Economic Models and Epsilon-Greedy Strategy**\\nThe speech presents three equations for economic models: Econmin, Econtotal, and Esave. The speech also explains the Epsilon-greedy action-selection strategy used in the system.\\n\\n**LSTM Q-Approximator**\\nThe speech proposes a new method for an agent to select suboptimal actions.\\n\\n**Reward Mechanism**\\nThe speech discusses a reward mechanism to improve agent actions.\\n\\n**System Architecture and Algorithm**\\nThe speech proposes a system architecture and algorithm for sensor selection using a combination of Long Short-Term Memory (LSTM) networks, Deep Q-Networks (DQN), and Epsilon-greedy strategy.\\n\\n**Reinforcement Learning Algorithm**\\nThe speech describes a reinforcement learning algorithm that balances exploration and exploitation.\\n\\n**Simulation Setup**\\nThe speech describes the setup for simulating the proposed system.\\n\\n**Simulation Experiment**\\nThe speech describes a simulation experiment involving 4 subareas, each with 1 sensor tracking a 50x50m area, and 1 target moving at 0.1-0.2m/s velocity and 5m/s² acceleration.\\n\\n**Results**\\nThe speech describes the results of a simulation experiment where a moving target passed near four sensors.\\n\\n**Training Session**\\nThe speech discusses the training session of the proposed LSTM-DQN-Epsilon-Greedy scheme.\\n\\n**Evaluation Metrics**\\nThe speech discusses evaluation metrics for a sensor selection system.\\n\\n**Results and Discussion**\\nThe speech discusses the results of the proposed method, LSTM-DQN-Epsilon-Greedy, in a target tracking environment.\\n\\n**Conclusion**\\nThe proposed LSTM-DQN-Epsilon-Greedy system has demonstrated high accuracy in selecting the best sensor, with an accuracy of 99% for certain sensors and over 90% for all others.\\n\\n**Deep Q-Networks (DQN) and Energy Consumption**\\nThe LSTM-DQN-based approaches due to their ability to handle long-term memory dependencies. The proposed LSTM-DQN-Epsilon-Greedy system demonstrates good convergence behavior around 200,000 epochs and is more stable compared to other methods.\\n\\n**Target Tracking and Energy Consumption**\\nThe method effectively enhances sensor selection accuracy and reduces energy consumption over time, with significant improvements in the first 100 episodes.\\n\\n**Deep Reinforcement Learning Methods**\\nThe LSTM-DQN methods (with Epsilon-greedy and softmax) outperform the Dense-DQN methods. The LSTM-DQN-Epsilon-Greedy method shows the fastest and best reduction in energy consumption.\\n\\n**Sensor Selection and Energy Consumption**\\nThe proposed LSTM-DQN-Epsilon-Greedy method outperforms LSTM-DQN-softmax in terms of cumulative energy consumption, particularly in the first 100 episodes. Sensors 2021, 21, 3261 20 of 22 reduced the average cumulative energy consumption compared to the other three methods by selecting the best energy-efficient sensor in the designed target tracking environment.\\n\\n**IoT Applications**\\nThe speech discusses the use of sensors in IoT applications, specifically in tracking and attaining target location information. It highlights the critical challenge of energy consumption to prolong sensor battery lifespan. To address this, a Deep RL approach is suggested, utilizing the Kalman filter and LSTM-DQN-Epsilon-Greedy algorithms to predict the target position and select the best sensor with minimum energy consumption.\\n\\n**System Limitations**\\nThe system has limitations, including only being able to track linear target information and not being able to track multiple targets at the same time.\\n\\n**Future Improvements**\\nThe speech discusses a system that tracks multiple targets in a subarea by activating multiple sensors. The system will be improved in the future by using reinforcement learning and real-time hardware experimentation.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = map_chain.run(split_document)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817a63da",
   "metadata": {},
   "source": [
    "#### Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f5ca6ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After reviewing the new context, I found that it does not provide any new information that would refine the original summary. The new context appears to provide references to other articles and research papers related to TensorFlow and Keras, which are not relevant to refining the original summary.\\n\\nTherefore, I will return the original summary, which remains a comprehensive and accurate summary of the article:\\n\\nThe article proposes a novel Deep RL framework that predicts the suboptimal energy-efficient sensor to track the target in IoT tracking applications. The proposed system utilizes an LSTM-DQN-epsilon-greedy method, which combines an LSTM deep Q-network (LSTM-DQN) as Q-approximator with an epsilon-greedy action-selection strategy. The system also employs a data pre-processing approach, including mini-max normalization, to improve the performance of the LSTM Q-approximator. Additionally, the system uses a Kalman filter to localize the target position, which is particularly useful in linear and noisy environments. Simulation results show favorable features in terms of the best sensor selection and energy consumption, making it suitable for various IoT target tracking applications, including smart farms, smart factories, and smart cities. Furthermore, the system demonstrates outstanding stability in selecting the best sensor, achieving significantly stable maximum cumulative rewards over 100 episodes in all subareas. The proposed system also achieves high accuracy in selecting the best sensor, with an accuracy of approximately 99% for some sensors and more than 90% for all other sensors. Additionally, the system achieves the highest average cumulative reward of up to 100 during the simulation period, with the epsilon-greedy action selection strategy reaching this level in just 63 episodes. Comparative analysis with other benchmark schemes, including Dense-DQN-epsilon-greedy and Dense-DQN-softmax, shows the superiority of the proposed LSTM-DQN-epsilon-greedy system in terms of average cumulative reward and loss convergence. Specifically, the LSTM-DQN-based approaches outperform Dense-DQN methods due to their ability to capture long-term memory dependencies. The proposed system also exhibits good convergence in terms of loss depreciation, reaching the minimum level, which is vital for system stability. Moreover, the LSTM-DQN-epsilon-greedy system converges efficiently around 200,000 epochs and is more stable, whereas the Dense-DQN methods show unstable behavior and converge at 500,000 epochs, which is time-consuming. Furthermore, the proposed LSTM-DQN-epsilon-greedy method addresses the problem of best sensor selection and converges the energy consumption issue efficiently, which is significantly improved in the tracking environment than the other three methods. However, the proposed scheme has limitations, including only considering linear target information using the Kalman filter, and being unable to track multiple targets at a particular time. Future work will extend the framework to use multi-agent-based Deep RL to control multiple sensors efficiently and leverage hardware for real-time hardware experimentation.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the initial summary prompt\n",
    "refine_summary = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\", \n",
    ")\n",
    "output_summary = refine_summary.run(split_document)\n",
    "output_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083486dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
