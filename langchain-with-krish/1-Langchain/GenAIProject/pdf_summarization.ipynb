{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23a178a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# load env variable\n",
    "# laod all env variable\n",
    "os.environ['HUGGINGFACE_TOKEN'] = os.getenv('HUGGINGFACE_TOKEN')\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d92235b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000266C4F8F050>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000266C56B0E90>, model_name='llama3-70b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(model=\"llama3-70b-8192\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d70c3d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='sensors\\nArticle\\nEnergy Conservation for Internet of Things Tracking\\nApplications Using Deep Reinforcement Learning\\nSalman Md Sultan 1\\n , Muhammad Waleed 1\\n , Jae-Young Pyun 1,*\\n and Tai-Won Um2,*\\n/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045/gid00001\\n/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046\\nCitation: Sultan, S.M.; Waleed, M.;\\nPyun, J.-Y.; Um, T.-W. Energy\\nConservation for Internet of Things\\nTracking Applications Using Deep\\nReinforcement Learning. Sensors 2021,\\n21, 3261. https://doi.org/10.3390/\\ns21093261\\nAcademic Editor: Maria Gabriella\\nXibilia\\nReceived: 6 April 2021\\nAccepted: 6 May 2021\\nPublished: 8 May 2021\\nPublisher‚Äôs Note:MDPI stays neutral\\nwith regard to jurisdictional claims in\\npublished maps and institutional afÔ¨Ål-\\niations.\\nCopyright: ¬© 2021 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\n1 Department of Information and Communication Engineering, Chosun University, Gwangju 61452, Korea;\\nsultanmohammadsalman@chosun.kr (S.M.S.); mwk.uet@gmail.com (M.W.)\\n2 Department of Cyber Security, College of Science and Technology, Duksung Women‚Äôs University,\\nSeoul 01369, Korea\\n* Correspondence: jypyun@chosun.ac.kr (J.-Y.P .); twum@duksung.ac.kr (T.-W.U.)\\nAbstract: The Internet of Things (IoT)-based target tracking system is required for applications such\\nas smart farm, smart factory, and smart city where many sensor devices are jointly connected to\\ncollect the moving target positions. Each sensor device continuously runs on battery-operated power,\\nconsuming energy while perceiving target information in a particular environment. To reduce sensor\\ndevice energy consumption in real-time IoT tracking applications, many traditional methods such\\nas clustering, information-driven, and other approaches have previously been utilized to select\\nthe best sensor. However, applying machine learning methods, particularly deep reinforcement\\nlearning (Deep RL), to address the problem of sensor selection in tracking applications is quite\\ndemanding because of the limited sensor node battery lifetime. In this study, we proposed a long\\nshort-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the\\nproblem of energy consumption in IoT target applications. The proposed method is utilized to select\\nthe energy-efÔ¨Åcient best sensor while tracking the target. The best sensor is deÔ¨Åned by the minimum\\ndistance function (i.e., derived as the state), which leads to lower energy consumption. The simulation\\nresults show favorable features in terms of the best sensor selection and energy consumption.\\nKeywords: deep reinforcement learning; internet of things; target tracking; best sensor selection;\\nenergy consumption\\n1. Introduction\\nIn a 5G sensor network, a massive amount of data are handled via sensor devices in a\\nlarge area. International Data Corporation (IDC) research states that 70% of companies will\\ndrive to use 1.2 billion devices for the connectivity management solution by 5G services\\nworldwide [1]. The Internet of Things (IoT) is the future of massive connectivity under\\n5G sensor networks. Currently, the IoT is performing a vital role in collecting a large\\namount of data via numerous sensors in real-time applications [2]. Kevin Ashton initially\\ncoined the IoT concept in 1999 [1,3]. Sensor-based IoT devices can provide various types of\\nservices, such as health, trafÔ¨Åc congestion control, robotics, and data analysis, which play\\na signiÔ¨Åcant role in daily life assistance [4]. Target tracking is another critical area where\\nthe sensors can be utilized to collect the target real-time position and report it to a server\\nwith its relevant information. The practice of tracking one or multiple targets has vast\\napplications in different research areas, such as object tracking (e.g., player, vehicle) [5‚Äì7],\\nborder monitoring to prevent illegal crossing, or battleÔ¨Åeld surveillance [8], infrared target\\nrecognition [9,10].\\nIn IoT target-tracking scenarios, tracking single or multiple targets can be realized\\nusing one or more sensors. However, it is impractical to utilize a single sensor for col-\\nlecting the target position information owing to an extended area and will take increased\\ncomputation with low tracking accuracy [ 11]. Therefore, it is pertinent to use multiple\\nsensors, particularly in tracking applications. Energy consumption in sensor applications is\\nSensors 2021, 21, 3261. https://doi.org/10.3390/s21093261 https://www.mdpi.com/journal/sensors'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2'}, page_content='Sensors 2021, 21, 3261 2 of 22\\na key task because of the sensor battery lifetime [11,12]. Moreover, it is unable to recharge\\nthe sensor battery in most cases. As a result, it is essential to efÔ¨Åciently reduce energy\\nconsumption because energy conservation leads to an increased battery lifespan. There\\nare various energy consumption reduction methods used in recent years (e.g., clustering,\\nsupport vector machine) [13,14]. However, large-scale functional implementation of these\\napproaches precludes more time and resources.\\nReinforcement learning (RL) is a machine learning subÔ¨Åeld that solves a problem\\nwithout any predeÔ¨Åned model. The RL agent learns the suboptimal policy by interacting\\nwith an unknown environment in real-time decision-based applications [ 15]. The use\\nof RL comprises two main elements: action and reward. In any dynamic interactive\\nenvironment, a precisely selected action will provide the best reward. Thus, providing the\\nbest outcome, based on current observations after acquiring a good reward in a real-time\\nenvironment. However, a massive number of autonomous IoT sensors are employed to\\nintelligently work with a dynamic environment to handle big data in next-generation\\n5G-based IoT applications (i.e., vehicle tracking, pedestrian tracking) [16]. Figure 1 shows\\nsome applications (e.g., smart transportation system, Intelligent Security System) including\\ndifferent types of sensors in the area of autonomous IoT. These autonomous IoT sensors\\ninteract and sense the environment to collect and send the relevant information to agent\\nfor taking the suboptimal action. The conventional RL algorithm (e.g., Tabular Q-learning)\\ntakes a higher time to handle this IoT environment because of large dimension sensor\\ndata [17].\\nAgentState\\nReward\\nAction\\nSmart Transportation System\\n‚ñ™ Traffic Signal Control\\n‚ñ™ Smart Parking System\\n‚ñ™ Autonomous Driving\\nIntelligent Security System\\n‚ñ™ Vehicle Tracking\\n‚ñ™ Border Monitoring\\n‚ñ™ Pedestrian Tracking\\nGPS Speed Sensor Camera RFID\\nApplication\\nAutonomous IoT Sensor\\nData Collection\\nFigure 1. Autonomous IoT applications.\\nDeep reinforcement learning (Deep RL) is an extended version of the conventional\\nRL algorithm to overcome iteration complexity in any large dynamic and interactive\\nenvironment [18]. Deep neural network (described as Q-approximator in this paper) is\\nthe main feature of Deep RL, predicting a suboptimal action from a speciÔ¨Åc state. In an\\nautonomous IoT target tracking system, Deep RL can be deployed to the sensor devices to\\nminimize the overall system computational complexity and energy consumption [17,19].\\nMoreover, there are different kinds of Q-approximators used in the Deep RL method to\\nsolve the energy consumption problem. Dense and long short-term memory (LSTM)-\\nbased Q-approximators are frequently utilized to increase energy efÔ¨Åciency in time-series\\nenvironments [20,21]. Note that the LSTM Q-approximator is more suitable than the dense\\nQ-approximator because of long-term dependencies in an IoT target tracking environment.\\nThe long-term memory features regulate the essential information sequentially (i.e., time-\\ndependent) to achieve better performance in the learning period [22‚Äì24].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3'}, page_content='Sensors 2021, 21, 3261 3 of 22\\nIn this study, we proposed a novel Deep RL framework that predicts the suboptimal\\nenergy-efÔ¨Åcient sensor to track the target in IoT tracking applications. Our proposed\\nsystem utilizes an LSTM deep Q-network (LSTM-DQN) as Q-approximator. Moreover,\\na data pre-processing approach is used for better state representation before applying\\nLSTM Q-approximator. The data pre-processing (e.g., normalization, feature selection)\\nis signiÔ¨Åcant for achieving stable LSTM Q-approximator [ 25,26]. In this paper, we use\\nmini-max normalization into our designed state space to improve LSTM Q-approximator\\nperformance. Furthermore, we also study epsilon-greedy and softmax action-selection\\nstrategies [27] in our proposed target tracking environment. However, the epsilon-greedy\\nmethod has faster improvement and convergence ability than the softmax method in our\\naction space. Therefore, we proposed an LSTM-DQN-epsilon-greedy method and com-\\npare it with LSTM-DQN-softmax, Dense-DQN-epsilon-greedy, and Dense-DQN-softmax\\napproaches in terms of average cumulative rewards, loss convergence, average sensor\\nselection accuracy, and average cumulative energy consumption.\\nThe remainder of this paper is organized as follows. A description of the related work\\nis provided in Section 2. Section 3 presents the system preliminaries. Sections 4 and 5 show\\nour proposed LSTM-DQN-epsilon-greedy algorithm and numerical results, respectively,\\nfor a detailed comparison. Finally, Section 6 presents the conclusion and future directions\\nof the research work.\\n2. Related Work\\nIn recent years, researchers have been working and investing much of their time to\\nsolve the problem of excessive energy consumption in tracking-based applications. Below,\\napplications based on the respective techniques from background studies are presented.\\n2.1. Tracking Application Based on Information-Driven Approaches\\nInformation-driven is a collaborative sensing technique for various target tracking\\napplications, where each deployed sensor is responsible for collaborating with other de-\\nployed sensors to collect moving target information [ 28]. Information-driven methods\\nwere Ô¨Årst proposed in terms of collaborative sensor selection via the information utility\\nfunction [29]. In this information-driven sensor selection method, the authors considered\\ndifferent Bayesian estimation problems (e.g., entropy and Mahalanobis distance-based util-\\nity measurements) to determine which sensor would track the moving target.Wei et al. [30]\\nproposed a dual-sensor control technique based on the information utility function in a\\nmulti-target tracking application. In this work, the authors used the posterior distance\\nbetween sensor and targets (PDST) function to minimize the distance between sensors\\nand targets, which helped the sensors directly drive the targets. Ping et el. in [ 31] used\\na partially observed Markov decision process (POMDP) to select suboptimal sensors for\\ntracking multiple targets. The POMDP sensor selection approach is implemented by maxi-\\nmizing the information gain via a probability hypothesis density (PHD)-based Bayesian\\nframework. Although the techniques proposed in [29‚Äì31] illustrated good tracking results,\\nthere is a limitation in choosing an energy-efÔ¨Åcient sensor to make their model work in an\\nintelligent manner to reduce the computational complexity.\\n2.2. Machine Learning-Based Techniques for Tracking Application\\nMachine learning is an excellent technique to overcome the computational complexity\\nissue in any complicated engineering problem because it is a self-learner, and it does not\\nneed to be reprogrammed [32‚Äì35]. Based on background studies, there are three types of\\nmachine learning approaches (i.e., supervised, unsupervised, and reinforcement learning),\\nwhich have been intelligently utilized for energy optimization. The study of supervised\\nlearning techniques is beyond the scope of this research.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}, page_content='Sensors 2021, 21, 3261 4 of 22\\n2.2.1. Unsupervised Learning-based Clustering Approaches\\nTo address the energy consumption problem, Hosseini and Mirvaziri in [ 36] intro-\\nduced a dynamic K-means clustering-based approach to minimize the target tracking error\\nand energy consumption in wireless sensor networks (WSNs). The proposed technique\\nuses a tube-shaped layering method for the sensor nodes to reduce energy dissipation\\nduring target tracking. In addition, Tengyue et al. [37] employed a clustering algorithm to\\ncontrol the sensor energy, which detected the target in a real-time mobile sensor network.\\nThey used the k-means++ algorithm to separate the sensor nodes into sub-groups. The k-\\nmeans++ separated the sensor nodes, which carried a higher weighted probability for\\ntarget detection, and the remaining unnecessary sensors remained in sleep mode to save\\nenergy consumption. Juan and Hongwei in [ 38] proposed another clustering approach\\nto balance energy in terms of multisensory distributed scheduling. Their work used the\\nenergy-balance technique to control the activation and deactivation modes of communi-\\ncation modules. They employed a multi-hop coordination strategy to decrease energy\\nconsumption. However, these types of unsupervised techniques are time-consuming to\\naddress because of the lack of available prior data labeling [34].\\n2.2.2. Reinforcement Learning Approaches\\nSensor scheduling is a promising approach for reducing energy consumption in many\\ntracking applications. Muhidul et al. in [39] proposed a cooperative RL to schedule the task\\nof each node based on the current tracking environment observation. The proposed method\\nhelped the deployed sensor nodes cooperate by sharing the adjacent node information\\nduring tracking. They applied a weighted reward function that combined both energy\\nconsumption and tracking quality matrices to improve the sensor node task scheduling at\\na particular time. Moreover, transmission scheduling is another necessary task in which\\nDeep RL can be applied. Jiang et al. in [ 40] proposed an approximation technique for\\ntransmitting packets in a scheduling manner for cognitive IoT networks. Their DQN model\\nutilized two parameters (i.e., the power for packet sending via multiple channels and\\npacket dropping) to enhance the system capacity in throughput terms. They used a stacked\\nauto-encoder as a Q-function approximator that mapped the policy to maximize system\\nperformance via a utility-based reward technique. However, they exploited the action\\nusing a comprehensive index evaluation method in a single relay to sync transmission.\\nTo reduce IoT device energy consumption, Mehdi et al. [ 41] employed a Deep RL\\ntechnique to learn an optimal policy for indoor localization problems in IoT-based smart\\ncity services. They deployed a semi-supervised technique to classify unlabeled data and\\nintegrated classiÔ¨Åed data with label data. They used iBeacons to provide a received\\nsignal strength indicator (RSSI) as an input for a semi-supervised Deep RL model, which\\nconsists of a variational autoencoder neural network Q-learning technique to enhance\\nindoor localization performance. In [ 27], the authors used two Deep RL methods (e.g.,\\nDQN and DDPG) to adjust the activation area radius so the system can minimize the\\naverage energy consumption in terms of vehicle-to-infrastructure (V2I) technology-based\\ntracking applications. They also used two action selection strategies (e.g., epsilon-greedy\\nand softmax) to determine the activation area radius.\\nThe Deep RL method has not been widely applied for energy saving in IoT target\\ntracking applications, particularly in energy-efÔ¨Åcient sensor selection approaches. Intel-\\nligently selecting the appropriate sensor to track the target is challenging because the\\ntarget position varies over time, creating tracking environment uncertainty. In this case,\\nthe DQN-based Deep RL is a sophisticated method because it has the best learning ca-\\npability when interacting with an uncertain dynamic environment. In DQN, selecting a\\nQ-approximator for the tracking environment is vital for obtaining improved learning\\nperformance. Therefore, we utilized the LSTM Q-approximator to predict the suboptimal\\ndecisions (i.e., sensor selection) based on sequential information (i.e., target position) with\\nthe assistance of different gate operations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5'}, page_content='Sensors 2021, 21, 3261 5 of 22\\nOur study is based on a discrete action space, which means that the proposed LSTM Q-\\napproximator selects the most energy-efÔ¨Åcient sensor among a Ô¨Ånite set of sensors. Authors\\nin [27] showed epsilon-greedy and softmax-based action selection methods for the discrete\\naction space. The epsilon-greedy-based sensor-selection technique presented improved\\nefÔ¨Åciency compared to the softmax technique in the simulation results. Thus, we proposed\\nthe LSTM-DQN method with epsilon-greedy action selection (described as LSTM-DQN-\\nepsilon-greedy in this study) in a target tracking environment to select the best sensor for\\nmaximum energy conservation. Table 1 represents a comparison of different existed RL\\nmethods to reduce the energy consumption of the sensor.\\nTable 1. Related work that use RL-based methods to reduce the energy consumption of the sensor.\\nStudy RL-Based Methods Action-Selection Solution Evaluation Metrics\\n[39] SARSA ( Œª) epsilon-greedy sensor scheduling energy consumption\\n[40]\\nQ-table with\\nstacked autoencoder epsilon-greedy transmission scheduling\\naverage power\\nconsumption and system utility\\n[27]\\nDQN, DDPG with\\nLSTM\\nepsilon-greedy\\nand softmax\\nradius adjustment\\nof the activated area\\naverage cumulative rewards\\nand energy consumption\\nProposed\\nmethod LSTM-DQN\\nepsilon-greedy\\nand softmax best sensor selection\\naverage cumulative rewards,\\nloss convergence,\\naverage best sensor selection\\naccuracy, and average\\naverage cumulative energy\\nconsumption\\n3. Preliminaries\\n3.1. System Overview\\nFigure 2 illustrates the tracking environment where multiple sensor devices repre-\\nsented as S = {S1, S2, .....,SD} are deployed at different positions to observe the moving\\ntargets, T = { T1, T2, .....,TL}, where L is the number of targets moving in the test area.\\nThe area consists of subareas X = {X1, X2, .....,XN}, where N is the number of subareas.\\nIn this study , our proposed LSTM-DQN-epsilon-greedy scheme allows one sensor to\\ntrack a single target at timet in a particular area, which eventually leads to trackingT targets\\nin N subareas. For instance, the selected sensors shown in green detect the targets, as shown\\nin Figure 2. The remaining sensors remained unselected to minimize energy consumption.\\nDeep RL \\nAgent\\nTarget State \\nInitialized\\nAction\\n(Select Best Sensor)\\nReward\\nTarget Next \\nState\\nEnvironment\\nTarget Detection Selected Sensor Unselected Sensor\\nùüè) Area (ùëøùüê)\\nArea (ùëøùüë) Area (ùëøùüí)\\nArea (ùëø\\nFigure 2. Deployed sensors for tracking target-based environment.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6'}, page_content='Sensors 2021, 21, 3261 6 of 22\\nFor suboptimal sensor selection, our proposed LSTM-DQN-epsilon-greedy-based IoT\\ntracking system tracks more than one target simultaneously in four subareas X1, X2, X3,\\nand X4, as shown in Figure 2, thus allowing the system to track all T targets in the Ô¨Årst\\nattempt. If we apply a single DQN algorithm for all N subareas, there is a possibility of not\\nachieving the required goal because when the system interacts with a large area, the sensor\\nselection space is more complicated to utilize the algorithm for effective simultaneous\\ntracking more than one target.\\nTo select the best sensor, it is imperative to estimate the distance between the moving\\ntarget and the sensors. A sensor with the minimum distance to the target location was\\nselected. However, in any practical scenario, the sensor has some noisy (i.e., Gaussian\\nnoise) measurements; thus, it can not collect the target position precisely. This study\\nconsiders that our target tracking environment is linear, including normally distributed or\\nGaussian process noise and some measurement errors. Kalman Ô¨Ålter is suitable for any\\nlinear environment along with Gaussian noise to predict the target information with more\\nprecision [42‚Äì44]. Moreover, because of having linear features, the Kalman Ô¨Ålter does not\\nrequire signiÔ¨Åcant memory except knowing only the prior state, which assists in predicting\\nthe target state over time [44]. Therefore, For the accurate measurement in a linear and\\nnoisy environment, the Kalman Ô¨Ålter was used to localize the target position.\\n3.2. Kalman Filter\\nThe Kalman Ô¨Ålter estimates the current system state from a series of noisy measure-\\nments, which is useful in tracking applications [42,45‚Äì47]. The Kalman Ô¨Ålter is a recursive\\nestimator based on Bayesian Ô¨Ålter theory that can compute the target state along with the\\nuncertainty [43,44]. The system has two signiÔ¨Åcant steps: prediction and updating. Various\\nessential Kalman Ô¨Ålter parameters are listed in Table 2.\\nTable 2. Kalman Ô¨Ålter parameters.\\nSymbols Description\\nŒ±0 Initial state matrix\\nP0 Initial process covariance matrix\\nŒ±k‚àí1 Previous state matrix\\nMk Measurement input\\nG Kalman gain\\nAcck Control variable matrix\\nPk‚àí1 Previous process covariance matrix\\nNkŒ± Predicted noise matrix\\nNkp Process noise matrix\\nX, Y, Z Transition matrix\\nMe Measurement error covariance matrix\\nH, I Identity matrix\\nThe initial state matrix Œ±0 indicates the early stage target observation and consists of\\nfour key information pieces such as the x- (x) and y-axis (y) positions, velocity along the x-\\n(vx) and y-axis (vy). In general, the covariance process measures the variation in random\\nvariables. The covariance for the four random variables is deÔ¨Åned as follows:\\nœÉ(x, y, vx, vy) = 1\\nn ‚àí1\\nn\\n‚àë\\ni=1\\n(xi ‚àíx)(yi ‚àíy)(x‚Ä≤\\ni ‚àívx)(y‚Ä≤\\ni ‚àívy), (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7'}, page_content='Sensors 2021, 21, 3261 7 of 22\\nwhere n is the number of samples, and the covariance matrix is deÔ¨Åned as œÉ(x, y, vx, vy)T .\\nThe initial state Œ±0 and process covariance matrices P0 are expressed as,\\nŒ±0 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nx\\ny\\nvx\\nvy\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb (2)\\nP0 =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nœÉ2x œÉxœÉy œÉxœÉvx œÉxœÉvy\\nœÉyœÉx œÉ2y œÉyœÉvx œÉyœÉvy\\nœÉvx œÉx œÉvx œÉy œÉ2vx œÉvx œÉvy\\nœÉvyœÉx œÉvyœÉy œÉvyœÉvx œÉ2vy\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (3)\\nIn the Kalman Ô¨Ålter, the prediction step estimates the current predicted state Œ±k and\\nthe process error covariance matrix Pk, which are expressed as,\\nŒ±k = XŒ±k‚àí1 + YAcck + NkŒ±, (4)\\nPk = X(Pk‚àí1XT) +YAcck + Nkp , (5)\\nwhere Œ±k‚àí1 and Pk‚àí1 denote the previous state and process error covariance matrices,\\nrespectively. The variable X represents the state transition matrix for the previous state\\nŒ±k‚àí1, and Y is the input transition matrix for the control vector. The Acck in (6) shows the\\nacceleration of the moving target, given as,\\nYAcck =\\n[ 1\\n2 ‚àÜT2ax 1\\n2 ‚àÜT2ay ‚àÜTax ‚àÜTay\\n]T, (6)\\nwhere ‚àÜT represents the time for one cycle, while ax and ay are the acceleration control\\nvariables. In the updated step, we estimate a new measurement Mk for state prediction at\\ntime step k. The Kalman gain G is one of the main features in the Kalman Ô¨Ålter method,\\nwhich gives the ratio of the uncertainty of error in prediction and measurement state [42].\\nMoreover, Kalman gain indicates how much the prediction state of the target should be pre-\\ncise. If the value of Kalman gain is increased gradually, which means the uncertainty error\\nof the measurement is small, and the value of the Kalman gain is low when the measure-\\nment error covariance is larger than the process error covariance. The new measurement\\nMk and gain G are described as follows:\\nMk = Z ‚àíHŒ±k, (7)\\nG = (Pk HT)\\nH.(Pk HT) +Me\\n, (8)\\nwhere Z, H, and Me represent the transition, identity matrix, and measurement error\\ncovariance matrix, respectively. After estimating the Kalman gain G, the predicted state Œ±k\\nand process error covariance matrix Pk are updated in (9) and (10), respectively:\\nŒ±k = XŒ±k + GMk, (9)\\nPk = [(I ‚àí(GH)) + Pk]. (10)\\nHere, Mk is the updated measurement which is obtained by subtracting the transition or\\nmeasured matrix (Z) from the predicted state (Œ±k) as described in (7). The update predicted\\nstate and process error covariance matrix in (9) and (10) will be used in the next time step.\\n3.3. Best Sensor Selection\\nThe designed LSTM-DQN-epsilon-greedy system uses multiple sensors to track the\\ntarget position. We consider one target at a particular time in a speciÔ¨Åc subarea as shown\\nin Figure 2. The system operates in such a manner that it does not allow all sensors\\nconcurrently to track the target due to limited battery lifespan of the sensor devices.\\nTherefore, the system intelligently adjudicates to select the best sensor using our proposed\\nDeep RL method while the moving target arrives within that sensor‚Äôs range. The sensor'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content='Sensors 2021, 21, 3261 8 of 22\\nwith low energy consumption is considered the best sensor and is apportioned to acquire\\ntarget position information. In the example shown in Figure 2, if the energy consumption\\nof the four sensors (i.e., S1, S2, S3, and S4) are 6J, 5J, 7J, and 8J, respectively, then sensor S2\\nis selected to track the target. In this way, we can conserve the energy of the other three\\nsensors. As a result, the overall system capability has improved in a particular subarea.\\n3.4. Reinforcement Learning (RL)\\nThe RL agent is used as a decision-maker to take the best action ( at) from the set\\nof possible actions over the current state ( st). The RL agent does not learn with the\\nlabeled training dataset, but learns from its experience with environmental interaction.\\nDuring environmental interaction at a particular time, the agent receives an immediate\\nreward (rt) and jumps to the next state (st+1). The entire process continues until the agent\\nreaches the Ô¨Ånal state and begins a new episode after resetting the environment.\\nTabular Q-learning (TQL) is a common model-free RL approach that is considered\\nan off-policy algorithm because the Q-function learns from the interactive environment\\nby taking random actions during exploration time [ 48]. Taking action with the help of\\nexploration is essential because initially, the agent has no idea about the new state in\\nan environment; therefore, the agent needs to explore the environment. After acquiring\\nenvironmental experience by exploration, the agent can easily exploit the environment by\\nutilizing the greedy strategy. The exploration and exploitation technique is also called the\\nepsilon-greedy technique [19]. As a result that the TQL is a value-based method, the agent\\nlearning policy is utilized through the value function (Q-value) of state-action pairs. In TQL,\\nthe Q-value Q(st, at) of an individual action of a particular state is stored in a matrix called\\nthe Q-table, which is updated in each time step in (11),\\nQ(st, at) = Q(st‚àí1, at‚àí1) + ‚àÇ(rt + Œ≥ max(Q(st+1, at+1)) ‚àíQ(st‚àí1, at‚àí1)), (11)\\nwhere ‚àÇ and Œ≥ ‚àà[0, 1] represent the learning rate and discount factor, respectively. Note\\nthat, ‚àÇ(rt + Œ≥ max(Q(st+1, at+1))) denotes as discounted temporal difference (TD) target,\\nwhich gives the maximum Q value of next state in (11). Further, to estimate the TD error\\nduring the training of Q-learning, we subtract the value of TD target from previous Q value\\n(Q(st‚àí1, at‚àí1)). The learning rate is used, which tells how fast the Q-values are updated\\nalong with TD error. Moreover, the discount factor gives stability between immediate\\nand upcoming or future rewards. If the discount factor is near to 1, then the reward will\\nbe more in the future. Otherwise, the system focuses on the immediate reward when\\nthe discount factor is near to 0. However, TQL has difÔ¨Åculty in extending the Q-table\\nto a large environment, as it is only appropriate for a small environment. To extend the\\nmethod to a large environment it is necessary for an agent to learn the value function with\\na Q-approximator instead of saving all values into a Q-table.\\n3.5. Deep-Q-Network\\nThe DQN was introduced by Mnih et al. in [ 18] based on the Deep RL method\\nwith the help of a deep neural network, which is known as a Q-approximator. The Q-\\nvalues of different actions are predicted by utilizing the Q-approximator in a particular\\nstate. In DQN, there is a possibility of a signiÔ¨Åcant correlation between the data, forming\\nthe Q-approximator instability during the training period. Following this, experience\\nreplay memory and mini-batch techniques are utilized to obtain a stable Q-approximator.\\nExperience replay memory (E) stores the experience (st, at, rt, st+1) in each time step to\\nre-utilize previous experiences multiple times. After storing each experience, the DQN uses\\nthe mini-batch technique to randomly sample data from the experience replay memory to\\nconverge the Q-approximator loss. It can also reduce the correlation between the samples\\nand improve the agent‚Äôs learning performance. Moreover, we estimate the predicted and\\ntarget Q-values with two different Q-approximatorsŒ∏ and Œ∏‚Ä≤, respectively, to obtain a stable'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9'}, page_content='Sensors 2021, 21, 3261 9 of 22\\nQ-approximator by optimizing the loss during the training period. The Q-approximator\\nloss L(Œ∏) is described as,\\nL(Œ∏) = (rt + Œ≥ max(Q(st+1, at+1; Œ∏‚Ä≤)) ‚àíQ(st, at; Œ∏))2. (12)\\n4. The Proposed LSTM-DQN-Epsilon-Greedy Method\\n4.1. Long Short-Term Memory-Based Q-approximator\\nIn our proposed system, we use LSTM as a Q-approximator to select the best sensor.\\nIn our target tracking scenario, the position of the target is updated over time. The LSTM is\\na speciÔ¨Åc type of recurrent neural network (RNN) with the ability to learn long-term de-\\npendencies that can memorize and connect related patterns over a time-series input [22,23].\\nMoreover, another reason behind deploying LSTM for our designed system is that it works\\nÔ¨Çawlessly in a dynamic environment because it depends on the gate operation. The gates\\nregulate the information Ô¨Çow and can also decide which information should be stored or\\nremoved. The LSTM consists of four gates: forget (Fst), input (Xst), cell (Cst), and output\\n(Ost) states. These four gates store the combined information of the previously hidden\\n(ht‚àí1) and the current input layer (xt) and apply the ‚Äúsigmoid‚Äù operation to all gates\\nexcept the cell state that is Ô¨Ånally activated by ‚Äútanh‚Äù operation, as shown in Figure 3.\\nùíâùíï‚àíùüè\\nùíôùíï\\nùíÑùíï‚àíùüè\\n+\\nùë≠ùíîùíï ùëøùíîùíï ùë™ùíîùíï ùë∂ùíîùíï~ ~~~\\nX\\nX\\n+\\nùíÑùíï\\n~\\nX\\nùíâùíï\\n+Addition Multiply X ~Sigmoid ~tanh\\nFigure 3. LSTM architecture.\\nIn the LSTM mechanism, when the forget state output is near 1, it keeps the data\\nand transfers it to multiply with the previous cell state value ( Ct‚àí1). The input and cell\\nstate gates receive the same information as the forget state gate. After separately applying\\n‚Äúsigmoid‚Äù and ‚Äútanh‚Äù operations to input and cell state gate, the outputs are multiplied\\nwith each other and added to the forget state output multiplying of the previous cell state\\nvalue for acquiring a new cell state (Ct). Finally, the output of the new cell state and output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10'}, page_content='Sensors 2021, 21, 3261 10 of 22\\nstate gate after the sigmoid operation multiply with each other to obtain the new hidden\\nstate (ht).\\n4.2. Mini-Max Normalization-Based State Space\\nThe proposed LSTM-DQN-epsilon-greedy model acts as an agent that takes the\\ncurrent state as the input. Estimated minimum distance leads to low energy consumption\\nat a speciÔ¨Åc time. The sensor with the minimum distance and energy consumption is\\nconsidered to be the best sensor for an individual area. Therefore, we organized our state\\nwith individual distances (i.e., dS1 , dS2 , ...,dSD ) between the target and sensors. The distance\\nis measured at each time step by using the Euclidean distance formula in (13),\\ndSD (t) =\\n‚àö\\n(Ptarget xcord ‚àíPxcordSD\\n)2(t) + (Ptarget ycord ‚àíPycordSD\\n)2(t), (13)\\nwhere PxcordSD\\n, PycordSD\\n, Ptarget xcord , and Ptarget ycord are the positions of all the deployed sen-\\nsors and the moving target in the two dimensional x-y plane. Furthermore, the position\\nof any target is computed using the Kalman Ô¨Ålter. Note that the state has different dis-\\ntance value ranges, which can create instability for the Q-approximator. Therefore, it is\\nnecessary to preprocess the state value by normalization before sending it to the LSTM\\nQ-approximator [25]. We use the mini-max normalization method, which is represented\\nas state normalized (t) = (st‚àímin(st))\\nmax(st)‚àímin(st) to scale the state between 0 and 1 to enhance the state\\nquality before sending it to our proposed LSTM Q-approximator.\\n4.3. Epsilon-Greedy Discrete Action Space\\nThe discrete action space (A = {AS1 , AS2 , ..., ASD }) represents all the allocated sensors\\n(i.e., S1, S2, ..., SD), respectively, in a deÔ¨Åned area. The LSTM-DQN-epsilon-greedy agent\\nselects the best sensor as an action that consumes minimum energy during target tracking.\\nThe energy consumption ( EconSD\\n) of each sensor at time step ( t) is estimated using (14),\\nwhere dSD , powSensor , and ttrack indicate the distance value between a particular sensor\\n(SD) and the target, the working mode sensor power, and time to track the target in a\\nsingle area, respectively. Similarly, we measured the energy consumption for the other\\nN areas. Note that the energy consumption of all sensors is stored in an array as (Econall )\\nin (15). Furthermore, the selected sensor energy consumption ( Econaction ) and minimum\\nenergy consumption (Econmin ) are obtained from (16) and (17). Finally, we estimate the total\\nenergy consumption (Econtotal ) and energy savings in a particular observation using (18)\\nand (19), respectively:\\nEconSD\\n(t) = dSD (t) √ópowsensor (t) √óttrack (t), (14)\\nEconall (t) = EconSD:1‚àºD\\n(t), (15)\\nEconaction (t) = Econall [ASD ](t), (16)\\nEconmin (t) = min(Econall (t)), (17)\\nEcontotal (t) =\\nD\\n‚àë\\nSD=1\\nEconSD\\n(t), (18)\\nEsave (t) = Econtotal (t) ‚àíEconaction (t). (19)\\nWe use epsilon-greedy as an action-selection strategy in the designed system be-\\ncause it is suitable for the discrete action space. In the epsilon-greedy approach, ini-\\ntially, the agent takes a random action to explore the environment through the epsilon\\nmethod. There are three key parameters: maximum-epsilon ( Œµmax ), minimum-epsilon\\n(Œµmin), and epsilon-decay ( Œµdecay ) that are considered to Ô¨Åx the epsilon period. First, it\\nbegins with the maximum-epsilon value and then decays with an absolute epsilon-decay')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pdf\n",
    "loader = PyPDFLoader('../pdf/sensors-21-03261-v2.pdf')\n",
    "docs = loader.load()\n",
    "docs[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b5659a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='sensors\\nArticle\\nEnergy Conservation for Internet of Things Tracking\\nApplications Using Deep Reinforcement Learning\\nSalman Md Sultan 1\\n , Muhammad Waleed 1\\n , Jae-Young Pyun 1,*\\n and Tai-Won Um2,*\\n/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045/gid00001\\n/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046\\nCitation: Sultan, S.M.; Waleed, M.;\\nPyun, J.-Y.; Um, T.-W. Energy\\nConservation for Internet of Things\\nTracking Applications Using Deep'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='Tracking Applications Using Deep\\nReinforcement Learning. Sensors 2021,\\n21, 3261. https://doi.org/10.3390/\\ns21093261\\nAcademic Editor: Maria Gabriella\\nXibilia\\nReceived: 6 April 2021\\nAccepted: 6 May 2021\\nPublished: 8 May 2021\\nPublisher‚Äôs Note:MDPI stays neutral\\nwith regard to jurisdictional claims in\\npublished maps and institutional afÔ¨Ål-\\niations.\\nCopyright: ¬© 2021 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='distributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\n1 Department of Information and Communication Engineering, Chosun University, Gwangju 61452, Korea;\\nsultanmohammadsalman@chosun.kr (S.M.S.); mwk.uet@gmail.com (M.W.)\\n2 Department of Cyber Security, College of Science and Technology, Duksung Women‚Äôs University,\\nSeoul 01369, Korea'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='Seoul 01369, Korea\\n* Correspondence: jypyun@chosun.ac.kr (J.-Y.P .); twum@duksung.ac.kr (T.-W.U.)\\nAbstract: The Internet of Things (IoT)-based target tracking system is required for applications such\\nas smart farm, smart factory, and smart city where many sensor devices are jointly connected to\\ncollect the moving target positions. Each sensor device continuously runs on battery-operated power,\\nconsuming energy while perceiving target information in a particular environment. To reduce sensor'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='device energy consumption in real-time IoT tracking applications, many traditional methods such\\nas clustering, information-driven, and other approaches have previously been utilized to select\\nthe best sensor. However, applying machine learning methods, particularly deep reinforcement\\nlearning (Deep RL), to address the problem of sensor selection in tracking applications is quite\\ndemanding because of the limited sensor node battery lifetime. In this study, we proposed a long'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the\\nproblem of energy consumption in IoT target applications. The proposed method is utilized to select\\nthe energy-efÔ¨Åcient best sensor while tracking the target. The best sensor is deÔ¨Åned by the minimum\\ndistance function (i.e., derived as the state), which leads to lower energy consumption. The simulation\\nresults show favorable features in terms of the best sensor selection and energy consumption.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='Keywords: deep reinforcement learning; internet of things; target tracking; best sensor selection;\\nenergy consumption\\n1. Introduction\\nIn a 5G sensor network, a massive amount of data are handled via sensor devices in a\\nlarge area. International Data Corporation (IDC) research states that 70% of companies will\\ndrive to use 1.2 billion devices for the connectivity management solution by 5G services\\nworldwide [1]. The Internet of Things (IoT) is the future of massive connectivity under'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='5G sensor networks. Currently, the IoT is performing a vital role in collecting a large\\namount of data via numerous sensors in real-time applications [2]. Kevin Ashton initially\\ncoined the IoT concept in 1999 [1,3]. Sensor-based IoT devices can provide various types of\\nservices, such as health, trafÔ¨Åc congestion control, robotics, and data analysis, which play\\na signiÔ¨Åcant role in daily life assistance [4]. Target tracking is another critical area where'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='the sensors can be utilized to collect the target real-time position and report it to a server\\nwith its relevant information. The practice of tracking one or multiple targets has vast\\napplications in different research areas, such as object tracking (e.g., player, vehicle) [5‚Äì7],\\nborder monitoring to prevent illegal crossing, or battleÔ¨Åeld surveillance [8], infrared target\\nrecognition [9,10].\\nIn IoT target-tracking scenarios, tracking single or multiple targets can be realized'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-05-10T11:33:04+08:00', 'author': 'Salman Md Sultan; Muhammad Waleed; Jae-Young Pyun; Tai-Won Um', 'keywords': 'deep reinforcement learning; internet of things; target tracking; best sensor selection; energy consumption', 'moddate': '2021-05-10T11:40:37+08:00', 'subject': 'The Internet of Things (IoT)-based target tracking system is required for applications such as smart farm, smart factory, and smart city where many sensor devices are jointly connected to collect the moving target positions. Each sensor device continuously runs on battery-operated power, consuming energy while perceiving target information in a particular environment. To reduce sensor device energy consumption in real-time IoT tracking applications, many traditional methods such as clustering, information-driven, and other approaches have previously been utilized to select the best sensor. However, applying machine learning methods, particularly deep reinforcement learning (Deep RL), to address the problem of sensor selection in tracking applications is quite demanding because of the limited sensor node battery lifetime. In this study, we proposed a long short-term memory deep Q-network (DQN)-based Deep RL target tracking model to overcome the problem of energy consumption in IoT target applications. The proposed method is utilized to select the energy-efficient best sensor while tracking the target. The best sensor is defined by the minimum distance function (i.e., derived as the state), which leads to lower energy consumption. The simulation results show favorable features in terms of the best sensor selection and energy consumption.', 'title': 'Energy Conservation for Internet of Things Tracking Applications Using Deep Reinforcement Learning', 'source': '../pdf/sensors-21-03261-v2.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='using one or more sensors. However, it is impractical to utilize a single sensor for col-\\nlecting the target position information owing to an extended area and will take increased\\ncomputation with low tracking accuracy [ 11]. Therefore, it is pertinent to use multiple\\nsensors, particularly in tracking applications. Energy consumption in sensor applications is\\nSensors 2021, 21, 3261. https://doi.org/10.3390/s21093261 https://www.mdpi.com/journal/sensors')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split docs\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbe08868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Researchers propose using a Deep Q-Network (DQN) with a Long Short-Term Memory (LSTM) neural network to optimize energy consumption in IoT target tracking applications by selecting the most energy-efficient sensor. The LSTM-DQN-epsilon-greedy model outperforms other methods, achieving high accuracy and cumulative rewards while minimizing energy consumption.\n"
     ]
    }
   ],
   "source": [
    "# summarization\n",
    "summary_chain = load_summarize_chain(llm, chain_type='map_reduce')\n",
    "summary = summary_chain.run(documents)\n",
    "print(summary) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7ea73",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f27e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670d94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fec3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6630dbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66bba30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220843b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe186b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e565c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c793f743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
